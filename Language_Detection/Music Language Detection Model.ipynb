{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fcebc0",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae72afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,recall_score\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebf11c",
   "metadata": {},
   "source": [
    "# 2. Read in Language Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34becf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts in Train Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Spanish        2412\n",
       "Portuguese     2405\n",
       "English        2371\n",
       "Kinyarwanda    1332\n",
       "Italian        1156\n",
       "French          968\n",
       "German          700\n",
       "Other           509\n",
       "Finnish         114\n",
       "Swedish          97\n",
       "Romanian         74\n",
       "Name: language label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset = pd.read_csv('Train_Test_Data/train.csv')[['Lyric','language label']]\n",
    "test_dataset = pd.read_csv('Train_Test_Data/test.csv')[['Lyric','language label']]\n",
    "print('Label Counts in Train Set')\n",
    "display(sample_dataset['language label'].value_counts())\n",
    "train_set = sample_dataset\n",
    "val_set = test_dataset.iloc[:1517]\n",
    "test_set = test_dataset.iloc[1517:]\n",
    "test_set.index = np.arange(0,len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dda47",
   "metadata": {},
   "source": [
    "#### Resampled Version of Train Set for Non Class Weight Method of Dealing With Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750e833e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Varf&amp;oumlr ska det vara så seri&amp;oumlst f&amp;oumlr...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intro:\\n(What a group of kids we sent out into...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Vem är Gud? (Vad är Gud?) \"\\n\"Det är en svår ...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vi sover på dagen,\\nvi saknar tidsuppfattning,...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Honey, honey, underbara, aha, honey honey\\nHon...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26527</th>\n",
       "      <td>Trece timpul si inteleg ca trece\\nDragostea da...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26528</th>\n",
       "      <td>Astazi pe la 5 ma vad cu ea\\nNu stiu ce m-aste...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26529</th>\n",
       "      <td>can you give me ,can you give me\\n\\nAstazi pe ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26530</th>\n",
       "      <td>I:\\nLasa-ma sa-ti spun :\\n'viata mea fara tine...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26531</th>\n",
       "      <td>Asculta si ia aminte !\\nAsculta ! Asculta ! As...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26532 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lyric language label\n",
       "0      Varf&oumlr ska det vara så seri&oumlst f&oumlr...        Swedish\n",
       "1      Intro:\\n(What a group of kids we sent out into...        Swedish\n",
       "2      \"Vem är Gud? (Vad är Gud?) \"\\n\"Det är en svår ...        Swedish\n",
       "3      vi sover på dagen,\\nvi saknar tidsuppfattning,...        Swedish\n",
       "4      Honey, honey, underbara, aha, honey honey\\nHon...        Swedish\n",
       "...                                                  ...            ...\n",
       "26527  Trece timpul si inteleg ca trece\\nDragostea da...       Romanian\n",
       "26528  Astazi pe la 5 ma vad cu ea\\nNu stiu ce m-aste...       Romanian\n",
       "26529  can you give me ,can you give me\\n\\nAstazi pe ...       Romanian\n",
       "26530  I:\\nLasa-ma sa-ti spun :\\n'viata mea fara tine...       Romanian\n",
       "26531  Asculta si ia aminte !\\nAsculta ! Asculta ! As...       Romanian\n",
       "\n",
       "[26532 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(50)\n",
    "max_class_counts = train_set['language label'].value_counts().iloc[0]\n",
    "resampled_train_set = pd.DataFrame()\n",
    "for lang in train_set['language label'].unique():\n",
    "    subset = train_set[train_set['language label'] == lang].copy()\n",
    "    if len(subset) == max_class_counts:\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset],ignore_index=True)\n",
    "    else:\n",
    "        added_subset = subset.iloc[random.choices(np.arange(0,len(subset)),k=max_class_counts - len(subset))]\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset,added_subset],ignore_index=True)\n",
    "        \n",
    "display(resampled_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a87df1",
   "metadata": {},
   "source": [
    "# 3. Create Term Density Representation of train and val/test lyrics where terms are from non-other class lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdd175",
   "metadata": {},
   "source": [
    "#### Preprocess Text, Create Vectorizer fit on non-other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdfaef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function preprocess_text at 0x7f8aa0aff0d0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocess_text)\n",
    "vectorizer.fit(train_set['Lyric'][train_set['language label'] != 'Other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5375aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer,open('word_vectorizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faf874",
   "metadata": {},
   "source": [
    "#### Lyrics to Term Density, Featurization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add64c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrics_to_term_density(text_df,vectorizer):\n",
    "    lyrics = vectorizer.transform(text_df['Lyric'])\n",
    "    lyrics = pd.DataFrame(lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "    label = text_df['language label'].copy()\n",
    "    label.index = np.arange(0,len(lyrics))\n",
    "    lyrics.dropna(inplace=True)\n",
    "    label = label.loc[lyrics.index]\n",
    "    token_count = np.array(text_df['Lyric'].apply(lambda x:len(preprocess_text(x).split())))\n",
    "    token_count = token_count.repeat(lyrics.shape[1])\n",
    "    token_count = token_count.reshape(lyrics.shape)\n",
    "    lyrics = (lyrics/token_count).astype('float32')\n",
    "    lyrics = scipy.sparse.csr_matrix(lyrics)\n",
    "    return lyrics,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81b1d9",
   "metadata": {},
   "source": [
    "#### Featurize Lyrics, Train Set, Resampled Train Set, Val Set, Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35500061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize Train Lyrics\n",
    "train_lyrics = vectorizer.transform(train_set['Lyric'])\n",
    "train_lyrics = pd.DataFrame(train_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "train_lyrics_token_count = train_lyrics.sum(axis=1)\n",
    "train_lyrics = train_lyrics/np.array(train_lyrics_token_count.repeat(len(train_lyrics.columns))).reshape(train_lyrics.shape)\n",
    "\n",
    "#Oversampled Vectorize Train Lyrics\n",
    "resampled_train_lyrics = vectorizer.transform(resampled_train_set['Lyric'])\n",
    "resampled_train_lyrics = pd.DataFrame(resampled_train_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "resampled_train_lyrics_token_count = resampled_train_lyrics.sum(axis=1)\n",
    "resampled_train_lyrics = resampled_train_lyrics/np.array(resampled_train_lyrics_token_count.repeat(len(resampled_train_lyrics.columns))).reshape(resampled_train_lyrics.shape)\n",
    "\n",
    "#Vectorize Val Lyrics\n",
    "val_lyrics = vectorizer.transform(val_set['Lyric'])\n",
    "val_lyrics = pd.DataFrame(val_lyrics.todense(),columns = vectorizer.get_feature_names(),index=val_set.index)\n",
    "val_lyrics_token_count = val_lyrics.sum(axis=1)\n",
    "val_lyrics = val_lyrics/np.array(val_lyrics_token_count.repeat(len(val_lyrics.columns))).reshape(val_lyrics.shape)\n",
    "\n",
    "#Vectorize Test Lyrics\n",
    "test_lyrics = vectorizer.transform(test_set['Lyric'])\n",
    "test_lyrics = pd.DataFrame(test_lyrics.todense(),columns = vectorizer.get_feature_names(),index=test_set.index)\n",
    "test_lyrics_token_count = test_lyrics.sum(axis=1)\n",
    "test_lyrics = test_lyrics/np.array(test_lyrics_token_count.repeat(len(test_lyrics.columns))).reshape(test_lyrics.shape)\n",
    "\n",
    "resampled_train_labels = resampled_train_set['language label']\n",
    "train_labels = train_set['language label']\n",
    "val_labels = val_set['language label']\n",
    "test_labels = test_set['language label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac0740",
   "metadata": {},
   "source": [
    "#### Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218a3d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>000000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>00000002</th>\n",
       "      <th>0001</th>\n",
       "      <th>00011</th>\n",
       "      <th>00014</th>\n",
       "      <th>00020</th>\n",
       "      <th>...</th>\n",
       "      <th>ținem</th>\n",
       "      <th>еl</th>\n",
       "      <th>еm</th>\n",
       "      <th>еnnui</th>\n",
       "      <th>еt</th>\n",
       "      <th>еtt</th>\n",
       "      <th>йquateur</th>\n",
       "      <th>йternellement</th>\n",
       "      <th>оles</th>\n",
       "      <th>時間</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1517 rows × 121319 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  0000  000000  00000000  00000002  0001  00011  00014  00020  \\\n",
       "0     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "2     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "3     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "4     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "...   ...  ...   ...     ...       ...       ...   ...    ...    ...    ...   \n",
       "1512  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1513  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1514  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1515  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1516  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "\n",
       "      ...  ținem   еl   еm  еnnui   еt  еtt  йquateur  йternellement  оles  \\\n",
       "0     ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "1     ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "2     ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "3     ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "4     ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "...   ...    ...  ...  ...    ...  ...  ...       ...            ...   ...   \n",
       "1512  ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "1513  ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "1514  ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "1515  ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "1516  ...    0.0  0.0  0.0    0.0  0.0  0.0       0.0            0.0   0.0   \n",
       "\n",
       "       時間  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "...   ...  \n",
       "1512  0.0  \n",
       "1513  0.0  \n",
       "1514  0.0  \n",
       "1515  0.0  \n",
       "1516  0.0  \n",
       "\n",
       "[1517 rows x 121319 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lyrics.astype('float32')\n",
    "resampled_train_lyrics.astype('float32')\n",
    "val_lyrics.astype('float32')\n",
    "test_lyrics.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53906c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lyrics.fillna(0,inplace=True)\n",
    "resampled_train_lyrics.fillna(0,inplace=True)\n",
    "val_lyrics.fillna(0,inplace=True)\n",
    "test_lyrics.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e1cff",
   "metadata": {},
   "source": [
    "# 4. ID Class Imbalance and ID Weights for Each Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee26dd",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232efb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spanish        2412\n",
       "Portuguese     2405\n",
       "English        2371\n",
       "Kinyarwanda    1332\n",
       "Italian        1156\n",
       "French          968\n",
       "German          700\n",
       "Other           509\n",
       "Finnish         114\n",
       "Swedish          97\n",
       "Romanian         74\n",
       "Name: language label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = train_set['language label'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc6475",
   "metadata": {},
   "source": [
    "#### Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "908e9094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spanish         1.000000\n",
       "Portuguese      1.002911\n",
       "English         1.017292\n",
       "Kinyarwanda     1.810811\n",
       "Italian         2.086505\n",
       "French          2.491736\n",
       "German          3.445714\n",
       "Other           4.738703\n",
       "Finnish        21.157895\n",
       "Swedish        24.865979\n",
       "Romanian       32.594595\n",
       "Name: language label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = class_counts.iloc[0]/class_counts\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097f902",
   "metadata": {},
   "source": [
    "#### Labels for Resampled Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c287650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swedish        2412\n",
       "French         2412\n",
       "Kinyarwanda    2412\n",
       "Spanish        2412\n",
       "German         2412\n",
       "Portuguese     2412\n",
       "Italian        2412\n",
       "Finnish        2412\n",
       "English        2412\n",
       "Other          2412\n",
       "Romanian       2412\n",
       "Name: language label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts1 = resampled_train_set['language label'].value_counts()\n",
    "class_counts1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceadd8d",
   "metadata": {},
   "source": [
    "#### Weights for Resampled Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b031d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swedish        1.0\n",
       "French         1.0\n",
       "Kinyarwanda    1.0\n",
       "Spanish        1.0\n",
       "German         1.0\n",
       "Portuguese     1.0\n",
       "Italian        1.0\n",
       "Finnish        1.0\n",
       "English        1.0\n",
       "Other          1.0\n",
       "Romanian       1.0\n",
       "Name: language label, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights1 = class_counts1.iloc[0]/class_counts1\n",
    "class_weights1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9894c2",
   "metadata": {},
   "source": [
    "#### Mapping Language to Numerical Label, Mapping Numerical Label to Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67077ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {}\n",
    "weight_mapping = {}\n",
    "count = 0\n",
    "for index in class_counts.index:\n",
    "    label_mapping[index] = count\n",
    "    weight_mapping[count] = class_weights.loc[index]\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a3567ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping1 = {}\n",
    "weight_mapping1 = {}\n",
    "count1 = 0\n",
    "for index in class_counts1.index:\n",
    "    label_mapping1[index] = count\n",
    "    weight_mapping1[count1] = class_weights1.loc[index]\n",
    "    count1 = count1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe3d06",
   "metadata": {},
   "source": [
    "# 5. Feed Forward Network For Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2201498",
   "metadata": {},
   "source": [
    "#### Custom Metric for Evaluating Performance - Average Class Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6a3ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_recall(y_true,y_pred):\n",
    "    #true labels\n",
    "    true = y_true.numpy()\n",
    "    #predicted prob of each class for each sample\n",
    "    pred = y_pred.numpy()\n",
    "    #prob to class based off max predicted prob\n",
    "    pred = np.array([x.argmax() for x in pred])\n",
    "    #confusion matrix\n",
    "    confuse = confusion_matrix(true,pred)\n",
    "    confuse_sum = confuse.sum(axis=1)\n",
    "    score = 0\n",
    "    for num in range(len(confuse_sum)):\n",
    "        if confuse_sum[num]!=0:\n",
    "            score = score + confuse[num][num]/confuse_sum[num]\n",
    "    \n",
    "    return score/len(confuse_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3c67f",
   "metadata": {},
   "source": [
    "#### Initialize FF Neural Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e44aea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feed_forward_network(\n",
    "                     shape=(1000,),\n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001,\n",
    "                     metrics = ['accuracy']):\n",
    "    \"\"\"\n",
    "    Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    input_layer = tf.keras.layers.Input(shape=shape,dtype='float32',name='input')    \n",
    "    #input into hidden layers\n",
    "    x = input_layer #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        #dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = hidden\n",
    "        \n",
    "    classification = tf.keras.layers.Dense(output_layer_size, activation='softmax', name='classification')(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=[classification])\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                 metrics=metrics,\n",
    "                 run_eagerly=True)\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fb065",
   "metadata": {},
   "source": [
    "#### Initialize and Train/Evaluate FF Neural Network to Detect Primary Music Language of Song Given Term Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c074aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    xtrain, xval, xtest, ytrain, yval, ytest, # Train/Val/Test Data\n",
    "    class_weights, batch_size, epochs, # Attributes for Fit Method of Model\n",
    "    patience, mode, #attributes for early stoppage\n",
    "    savepath,#attributes for model checkpoints\n",
    "    #Attributes for Model Architecture\n",
    "    hidden_dim=[100,100,100],\n",
    "    dropout_rate=0,\n",
    "    hidden_layer_activation = 'relu',\n",
    "    output_layer_size = 4,\n",
    "    output_activation = 'softmax',\n",
    "    learning_rate=0.001,\n",
    "    metrics = ['accuracy'],\n",
    "    opt_metric = 'class_recall',\n",
    "    opt_func = class_recall):\n",
    "    \n",
    "    #Sparse to Dense Matrices\n",
    "    xtr_dense = xtrain.copy()\n",
    "    xva_dense = xval.copy()\n",
    "    xte_dense = xtest.copy()\n",
    "    \n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    #Initialize Architecture\n",
    "    model = create_feed_forward_network(shape=(xtr_dense.shape[1],),hidden_dim=hidden_dim,\n",
    "                                        dropout_rate=dropout_rate,hidden_layer_activation=hidden_layer_activation,\n",
    "                                        output_layer_size=output_layer_size,\n",
    "                                        output_activation=output_activation,\n",
    "                                        learning_rate=learning_rate,metrics=metrics)\n",
    "    \n",
    "    #Early Stoppage and Model Checkpoints Objects\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_' + opt_metric,verbose=1,patience=patience,mode=mode)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(savepath,monitor='val_' + opt_metric,save_best_only=True,mode=mode)\n",
    "    \n",
    "    #Fit Model on Training Data, iteratively evaluate on val data\n",
    "    model.fit(xtr_dense,ytrain,\n",
    "              validation_data=(xva_dense, yval),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              shuffle=True,\n",
    "              class_weight = class_weights,\n",
    "              callbacks = [stoppage,checkpoint],\n",
    "              use_multiprocessing=True,workers=multiprocessing.cpu_count() - 8)\n",
    "    \n",
    "    #Final Evaluation of Optimal Model on Test Data\n",
    "    final_model = load_model(savepath,custom_objects={opt_metric:opt_func})\n",
    "    preds = final_model.predict(xte_dense)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf51d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 121319)]          0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 100)               12132000  \n",
      "                                                                 \n",
      " classification (Dense)      (None, 11)                1111      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,133,111\n",
      "Trainable params: 12,133,111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1518/1518 [==============================] - ETA: 0s - loss: 0.7675 - accuracy: 0.9561 - class_recall: 0.9267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - 124s 82ms/step - loss: 0.7675 - accuracy: 0.9561 - class_recall: 0.9267 - val_loss: 0.1083 - val_accuracy: 0.9776 - val_class_recall: 0.9619\n",
      "Epoch 2/30\n",
      "1518/1518 [==============================] - 122s 81ms/step - loss: 0.0632 - accuracy: 0.9926 - class_recall: 0.9872 - val_loss: 0.0908 - val_accuracy: 0.9796 - val_class_recall: 0.9697\n",
      "Epoch 3/30\n",
      "1518/1518 [==============================] - 122s 81ms/step - loss: 0.0083 - accuracy: 0.9991 - class_recall: 0.9987 - val_loss: 0.1079 - val_accuracy: 0.9802 - val_class_recall: 0.9693\n",
      "Epoch 4/30\n",
      "1518/1518 [==============================] - 122s 81ms/step - loss: 0.0028 - accuracy: 0.9998 - class_recall: 0.9994 - val_loss: 0.1213 - val_accuracy: 0.9776 - val_class_recall: 0.9673\n",
      "Epoch 5/30\n",
      "1518/1518 [==============================] - 122s 81ms/step - loss: 9.7444e-04 - accuracy: 0.9999 - class_recall: 0.9998 - val_loss: 0.1366 - val_accuracy: 0.9796 - val_class_recall: 0.9702\n",
      "Epoch 6/30\n",
      "1518/1518 [==============================] - 122s 81ms/step - loss: 2.2132e-04 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.9796 - val_class_recall: 0.9702\n",
      "Epoch 7/30\n",
      "1518/1518 [==============================] - 122s 81ms/step - loss: 8.5803e-05 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9796 - val_class_recall: 0.9702\n",
      "Epoch 8/30\n",
      "1518/1518 [==============================] - 124s 82ms/step - loss: 3.3782e-05 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9789 - val_class_recall: 0.9692\n",
      "Epoch 9/30\n",
      "1518/1518 [==============================] - 123s 81ms/step - loss: 1.6250e-05 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1712 - val_accuracy: 0.9796 - val_class_recall: 0.9702\n",
      "Epoch 10/30\n",
      "1518/1518 [==============================] - 123s 81ms/step - loss: 7.8290e-06 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1906 - val_accuracy: 0.9782 - val_class_recall: 0.9687\n",
      "Epoch 10: early stopping\n",
      " 6/48 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = eval_model(xtrain = np.array(train_lyrics), xval = np.array(val_lyrics), xtest = np.array(test_lyrics), \n",
    "           ytrain = train_labels.map(label_mapping),\n",
    "           yval = val_labels.map(label_mapping),\n",
    "           ytest = test_labels.map(label_mapping),\n",
    "           class_weights = weight_mapping, batch_size=8, epochs = 30,\n",
    "           patience=5,mode='max',savepath='language_detection_ff_tf.h5',\n",
    "           hidden_dim=[100],dropout_rate=0.3,hidden_layer_activation='relu',\n",
    "           output_layer_size=11,output_activation='softmax',\n",
    "           learning_rate = 0.005,metrics=['accuracy',class_recall],opt_metric='class_recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb83949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_label_to_map = {}\n",
    "for key,value in label_mapping.items():\n",
    "    num_label_to_map[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "291697a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(confusion_matrix(test_labels.map(label_mapping),np.array([x.argmax() for x in preds])))\n",
    "test_results.index = [num_label_to_map[x] for x in test_results.index]\n",
    "test_results.columns = test_results.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1a16364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <th>Italian</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Other</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Romanian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finnish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Spanish  Portuguese  English  Kinyarwanda  Italian  French  \\\n",
       "Spanish          285           0        1            0        0       0   \n",
       "Portuguese         0         319        0            0        0       0   \n",
       "English            2           0      302            0        0       0   \n",
       "Kinyarwanda        2           0        1          162        0       0   \n",
       "Italian            0           0        0            0      147       0   \n",
       "French             0           0        1            0        0     115   \n",
       "German             0           0        0            0        0       0   \n",
       "Other              2           1       10            1        3       0   \n",
       "Finnish            0           0        0            0        0       0   \n",
       "Swedish            0           0        0            0        0       0   \n",
       "Romanian           0           0        0            0        0       0   \n",
       "\n",
       "             German  Other  Finnish  Swedish  Romanian  \n",
       "Spanish           0      0        0        0         0  \n",
       "Portuguese        0      0        0        0         0  \n",
       "English           0      4        0        0         0  \n",
       "Kinyarwanda       0      0        0        0         0  \n",
       "Italian           0      0        0        0         0  \n",
       "French            0      0        0        0         0  \n",
       "German           80      0        0        0         0  \n",
       "Other             0     42        1        0         0  \n",
       "Finnish           0      0       15        0         0  \n",
       "Swedish           0      0        0        8         0  \n",
       "Romanian          0      0        0        0        13  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41118e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_class_recall = round(test_results/np.array(test_results.sum(axis=1).repeat(11)).reshape(11,11),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f1369b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <th>Italian</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Other</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Romanian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>0.997</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finnish</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Spanish  Portuguese  English  Kinyarwanda  Italian  French  \\\n",
       "Spanish        0.997       0.000    0.003        0.000     0.00   0.000   \n",
       "Portuguese     0.000       1.000    0.000        0.000     0.00   0.000   \n",
       "English        0.006       0.000    0.981        0.000     0.00   0.000   \n",
       "Kinyarwanda    0.012       0.000    0.006        0.982     0.00   0.000   \n",
       "Italian        0.000       0.000    0.000        0.000     1.00   0.000   \n",
       "French         0.000       0.000    0.009        0.000     0.00   0.991   \n",
       "German         0.000       0.000    0.000        0.000     0.00   0.000   \n",
       "Other          0.033       0.017    0.167        0.017     0.05   0.000   \n",
       "Finnish        0.000       0.000    0.000        0.000     0.00   0.000   \n",
       "Swedish        0.000       0.000    0.000        0.000     0.00   0.000   \n",
       "Romanian       0.000       0.000    0.000        0.000     0.00   0.000   \n",
       "\n",
       "             German  Other  Finnish  Swedish  Romanian  \n",
       "Spanish         0.0  0.000    0.000      0.0       0.0  \n",
       "Portuguese      0.0  0.000    0.000      0.0       0.0  \n",
       "English         0.0  0.013    0.000      0.0       0.0  \n",
       "Kinyarwanda     0.0  0.000    0.000      0.0       0.0  \n",
       "Italian         0.0  0.000    0.000      0.0       0.0  \n",
       "French          0.0  0.000    0.000      0.0       0.0  \n",
       "German          1.0  0.000    0.000      0.0       0.0  \n",
       "Other           0.0  0.700    0.017      0.0       0.0  \n",
       "Finnish         0.0  0.000    1.000      0.0       0.0  \n",
       "Swedish         0.0  0.000    0.000      1.0       0.0  \n",
       "Romanian        0.0  0.000    0.000      0.0       1.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_class_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0995e76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Recall: 0.9682727272727273\n",
      "Accuracy: 0.980883322346737\n"
     ]
    }
   ],
   "source": [
    "print('Class Recall: ' + str(np.array(test_results_class_recall).diagonal().sum()/11))\n",
    "print('Accuracy: ' + str(np.array(test_results).diagonal().sum()/np.array(test_results).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e85c8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_results,open('ff_model_test_results.pkl','wb'))\n",
    "pickle.dump(test_results_class_recall,open('ff_model_test_class_recall.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4623c1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98586059",
   "metadata": {},
   "source": [
    "# 6. DAN & WAN for Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3e0f3",
   "metadata": {},
   "source": [
    "#### Create Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68975db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(50)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "embedding_matrix = np.random.randn((len(vocab)+1)*300).reshape((len(vocab) + 1,300)) #Instantiate Embedding Matrix\n",
    "embedding_matrix[-1] = 0\n",
    "vocab_mapping = {}\n",
    "count = 0\n",
    "for word in vocab:\n",
    "    vocab_mapping[word] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3060c0",
   "metadata": {},
   "source": [
    "#### Map Words to Token For Each Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28ad6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(text_data,mapping,max_size):\n",
    "    return_data = []\n",
    "    for text in text_data:\n",
    "        new_text = text.lower()\n",
    "        new_text = text.replace('\\n',' ')\n",
    "        new_text = text.replace('  ',' ')\n",
    "        new_text = new_text.split()\n",
    "        mapped_text = []\n",
    "        for token in new_text:\n",
    "            try:\n",
    "                mapped_text.append(mapping[token])\n",
    "            except:\n",
    "                mapped_text.append(len(mapping))\n",
    "        \n",
    "        if len(mapped_text) > max_size:\n",
    "            mapped_text = mapped_text[:max_size]\n",
    "        else:\n",
    "            while len(mapped_text) < max_size:\n",
    "                mapped_text.append(len(mapping))\n",
    "                \n",
    "        return_data.append(mapped_text)\n",
    "    \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39f4e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = text_to_index(train_set['Lyric'],vocab_mapping,1000)\n",
    "val_tokens = text_to_index(val_set['Lyric'],vocab_mapping,1000)\n",
    "test_tokens = text_to_index(test_set['Lyric'],vocab_mapping,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d0813",
   "metadata": {},
   "source": [
    "#### DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aba741f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix, \n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    dan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    dan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    dan_embeddings = dan_embedding_layer(dan_input_layer)\n",
    "    #Embeddings are averaged, forming single vector represenation of size embedding matrix\n",
    "    dan_avg_input_embeddings = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1), name='averaging')(dan_embeddings)\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = dan_avg_input_embeddings #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dense(hidden_dim, activation='relu', name='hidden_1')(dan_avg_input_embeddings)\n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dropout(dropout)(dan_hidden_out_1)\n",
    "    dan_classification = tf.keras.layers.Dense(output_layer_size, activation=output_activation, name='dan_classification')(x)\n",
    "    dan_model = tf.keras.models.Model(inputs=dan_input_layer, outputs=[dan_classification])\n",
    "    dan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics=['accuracy',class_recall])\n",
    "    \n",
    "    print(dan_model.summary())\n",
    "\n",
    "    return dan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4ae3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_dan_model():\n",
    "    #Early Stoppage and Model Checkpoints Objects\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('language_detection_dan.h5',monitor='val_class_recall',save_best_only=True,mode=max)\n",
    "\n",
    "    model = create_dan_model(output_layer_size=11,learning_rate=0.005,hidden_dim=[100],\n",
    "                            retrain_embeddings=True)\n",
    "    #Fit Model on Training Data, iteratively evaluate on val data\n",
    "    model.fit(np.array(train_tokens),np.array(train_labels.map(label_mapping)),\n",
    "              validation_data=(np.array(val_tokens), np.array(val_labels.map(label_mapping))),\n",
    "              batch_size=8,\n",
    "              epochs=10,\n",
    "              shuffle=True,\n",
    "              class_weight = weight_mapping,\n",
    "              callbacks = [stoppage,checkpoint],\n",
    "              use_multiprocessing=True,workers=multiprocessing.cpu_count() - 8)\n",
    "\n",
    "    #Final Evaluation of Optimal Model on Test Data\n",
    "    final_model = load_model('language_detection_dan.h5',custom_objects={'class_recall':class_recall})\n",
    "    preds = final_model.predict(np.array(test_tokens))\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed057b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:ModelCheckpoint mode <built-in function max> is unknown, fallback to auto mode.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1000)]            0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 1000, 300)        36396000  \n",
      "                                                                 \n",
      " averaging (Lambda)          (None, 300)               0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dan_classification (Dense)  (None, 11)                1111      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,427,211\n",
      "Trainable params: 36,427,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - ETA: 0s - loss: 1.6238 - accuracy: 0.8652 - class_recall: 0.7851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - 722s 476ms/step - loss: 1.6238 - accuracy: 0.8652 - class_recall: 0.7851 - val_loss: 0.2694 - val_accuracy: 0.9176 - val_class_recall: 0.8476\n",
      "Epoch 2/10\n",
      "1518/1518 [==============================] - 722s 476ms/step - loss: 0.7411 - accuracy: 0.9316 - class_recall: 0.8823 - val_loss: 0.2183 - val_accuracy: 0.9446 - val_class_recall: 0.8962\n",
      "Epoch 3/10\n",
      "1518/1518 [==============================] - 724s 477ms/step - loss: 0.5364 - accuracy: 0.9474 - class_recall: 0.9063 - val_loss: 0.2133 - val_accuracy: 0.9361 - val_class_recall: 0.8855\n",
      "Epoch 4/10\n",
      "1518/1518 [==============================] - 724s 477ms/step - loss: 0.4140 - accuracy: 0.9572 - class_recall: 0.9228 - val_loss: 0.2081 - val_accuracy: 0.9611 - val_class_recall: 0.9272\n",
      "Epoch 5/10\n",
      "1518/1518 [==============================] - 722s 476ms/step - loss: 0.5054 - accuracy: 0.9553 - class_recall: 0.9232 - val_loss: 0.2312 - val_accuracy: 0.9604 - val_class_recall: 0.9295\n",
      "Epoch 6/10\n",
      "1518/1518 [==============================] - 725s 477ms/step - loss: 0.3606 - accuracy: 0.9619 - class_recall: 0.9332 - val_loss: 0.3352 - val_accuracy: 0.9354 - val_class_recall: 0.8927\n",
      "Epoch 7/10\n",
      "1518/1518 [==============================] - 724s 477ms/step - loss: 0.3656 - accuracy: 0.9592 - class_recall: 0.9297 - val_loss: 0.3041 - val_accuracy: 0.9512 - val_class_recall: 0.9141\n",
      "Epoch 8/10\n",
      "1518/1518 [==============================] - 720s 475ms/step - loss: 0.4133 - accuracy: 0.9635 - class_recall: 0.9338 - val_loss: 0.3266 - val_accuracy: 0.9604 - val_class_recall: 0.9325\n",
      "Epoch 9/10\n",
      "1518/1518 [==============================] - 719s 474ms/step - loss: 0.4103 - accuracy: 0.9698 - class_recall: 0.9436 - val_loss: 0.3807 - val_accuracy: 0.9664 - val_class_recall: 0.9426\n",
      "Epoch 10/10\n",
      "1518/1518 [==============================] - 719s 474ms/step - loss: 0.2430 - accuracy: 0.9731 - class_recall: 0.9511 - val_loss: 0.3865 - val_accuracy: 0.9578 - val_class_recall: 0.9244\n",
      "11/48 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = train_eval_dan_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7aa1cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(confusion_matrix(test_labels.map(label_mapping),np.array([x.argmax() for x in preds])))\n",
    "test_results.index = [num_label_to_map[x] for x in test_results.index]\n",
    "test_results.columns = test_results.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1fdf605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <th>Italian</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Other</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Romanian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>0</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finnish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Spanish  Portuguese  English  Kinyarwanda  Italian  French  \\\n",
       "Spanish          274           0        1            0        0       0   \n",
       "Portuguese         0         307        0            0        0       0   \n",
       "English            0           0      277            0        0       0   \n",
       "Kinyarwanda        2           0        1          159        0       0   \n",
       "Italian            0           0        0            0      142       1   \n",
       "French             0           0        0            0        0     114   \n",
       "German             0           0        0            0        0       0   \n",
       "Other              2           1        8            1        2       0   \n",
       "Finnish            0           0        0            0        0       0   \n",
       "Swedish            0           0        0            0        0       0   \n",
       "Romanian           0           0        0            0        0       0   \n",
       "\n",
       "             German  Other  Finnish  Swedish  Romanian  \n",
       "Spanish           0      7        4        0         0  \n",
       "Portuguese        0      0       12        0         0  \n",
       "English           0      3       28        0         0  \n",
       "Kinyarwanda       0      1        2        0         0  \n",
       "Italian           0      0        3        0         1  \n",
       "French            0      0        2        0         0  \n",
       "German           77      0        3        0         0  \n",
       "Other             0     16       26        4         0  \n",
       "Finnish           0      0       15        0         0  \n",
       "Swedish           0      0        0        8         0  \n",
       "Romanian          0      0        0        0        13  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18dce4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_class_recall = round(test_results/np.array(test_results.sum(axis=1).repeat(11)).reshape(11,11),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54263616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <th>Italian</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Other</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Romanian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>0.958</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finnish</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Spanish  Portuguese  English  Kinyarwanda  Italian  French  \\\n",
       "Spanish        0.958       0.000    0.003        0.000    0.000   0.000   \n",
       "Portuguese     0.000       0.962    0.000        0.000    0.000   0.000   \n",
       "English        0.000       0.000    0.899        0.000    0.000   0.000   \n",
       "Kinyarwanda    0.012       0.000    0.006        0.964    0.000   0.000   \n",
       "Italian        0.000       0.000    0.000        0.000    0.966   0.007   \n",
       "French         0.000       0.000    0.000        0.000    0.000   0.983   \n",
       "German         0.000       0.000    0.000        0.000    0.000   0.000   \n",
       "Other          0.033       0.017    0.133        0.017    0.033   0.000   \n",
       "Finnish        0.000       0.000    0.000        0.000    0.000   0.000   \n",
       "Swedish        0.000       0.000    0.000        0.000    0.000   0.000   \n",
       "Romanian       0.000       0.000    0.000        0.000    0.000   0.000   \n",
       "\n",
       "             German  Other  Finnish  Swedish  Romanian  \n",
       "Spanish       0.000  0.024    0.014    0.000     0.000  \n",
       "Portuguese    0.000  0.000    0.038    0.000     0.000  \n",
       "English       0.000  0.010    0.091    0.000     0.000  \n",
       "Kinyarwanda   0.000  0.006    0.012    0.000     0.000  \n",
       "Italian       0.000  0.000    0.020    0.000     0.007  \n",
       "French        0.000  0.000    0.017    0.000     0.000  \n",
       "German        0.962  0.000    0.038    0.000     0.000  \n",
       "Other         0.000  0.267    0.433    0.067     0.000  \n",
       "Finnish       0.000  0.000    1.000    0.000     0.000  \n",
       "Swedish       0.000  0.000    0.000    1.000     0.000  \n",
       "Romanian      0.000  0.000    0.000    0.000     1.000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_class_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f648a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Recall: 0.9055454545454545\n",
      "Accuracy: 0.9241924851680949\n"
     ]
    }
   ],
   "source": [
    "print('Class Recall: ' + str(np.array(test_results_class_recall).diagonal().sum()/11))\n",
    "print('Accuracy: ' + str(np.array(test_results).diagonal().sum()/np.array(test_results).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e2f3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_results,open('dan_model_test_results.pkl','wb'))\n",
    "pickle.dump(test_results_class_recall,open('dan_model_test_class_recall.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733269b9",
   "metadata": {},
   "source": [
    "#### WAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61bcb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix,\n",
    "                     num_attention = 1,\n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the WAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    num_attention = number of parallel attention computations that learn how to balance embeddings into a single\n",
    "    vector representation, final attention layer weights prior attention based representations\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    wan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    wan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    wan_embeddings = wan_embedding_layer(wan_input_layer)\n",
    "    \n",
    "    if num_attention > 1:\n",
    "        #Create attention based single vector representations of words according to alternative query vectors\n",
    "        attention_embeddings = []\n",
    "        for num in range(num_attention):\n",
    "            #Apply Query Vector to words in embeddings, returning a max_sequence_length x 1 tensor\n",
    "            l1_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query' + str(num+1))(wan_embeddings)\n",
    "            #reshape to 1 x max_sequence_length\n",
    "            l1_reshape_query = tf.keras.layers.Reshape((1,max_sequence_length))(l1_query)\n",
    "            #Softmax over query * key (words) to obtain weights\n",
    "            l1_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                                name='attention_weights' + str(num+1))(l1_reshape_query)\n",
    "            #weight embeddings according to weights\n",
    "            l1_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((wan_embeddings,l1_weights)))\n",
    "            attention_embeddings.append(l1_attention)\n",
    "\n",
    "        concat_attention = tf.keras.layers.Concatenate()(attention_embeddings)\n",
    "        concat_attention = tf.keras.layers.Reshape((num_attention,embedding_matrix.shape[1]))(concat_attention)\n",
    "    else:\n",
    "        concat_attention = wan_embeddings\n",
    "        num_attention = max_sequence_length\n",
    "    \n",
    "    #Apply Query Vector to attention based representations, returning a num_attention x 1 tensor\n",
    "    wan_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query')(concat_attention)\n",
    "    #reshape to 1 x num_attention\n",
    "    reshaped_query = tf.keras.layers.Reshape((1,num_attention))(wan_query)\n",
    "    #Softmax over query * key (words) to obtain weights\n",
    "    wan_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                        name='attention_weights')(reshaped_query)\n",
    "    #weight attention embeddings according to weights, learning how to balance attention based vector representations \n",
    "    #from prior layer\n",
    "    wan_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((concat_attention,wan_weights)))\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = wan_attention #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    wan_classification = tf.keras.layers.Dense(output_layer_size, activation=output_activation, name='wan_classification')(x)\n",
    "    wan_model = tf.keras.models.Model(inputs=wan_input_layer, outputs=[wan_classification])\n",
    "    wan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics=['accuracy',class_recall],\n",
    "                     run_eagerly=True)\n",
    "    \n",
    "    print(wan_model.summary())\n",
    "\n",
    "    return wan_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90097b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_wan_model():\n",
    "    #Early Stoppage and Model Checkpoints Objects\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('language_detection_wan.h5',monitor='val_class_recall',save_best_only=True,mode=max)\n",
    "\n",
    "    model = create_wan_model(output_layer_size=11,learning_rate=0.005,hidden_dim=[100],\n",
    "                            retrain_embeddings=True,num_attention=10)\n",
    "    #Fit Model on Training Data, iteratively evaluate on val data\n",
    "    model.fit(np.array(train_tokens),np.array(train_labels.map(label_mapping)),\n",
    "              validation_data=(np.array(val_tokens), np.array(val_labels.map(label_mapping))),\n",
    "              batch_size=8,\n",
    "              epochs=10,\n",
    "              shuffle=True,\n",
    "              class_weight = weight_mapping,\n",
    "              callbacks = [stoppage,checkpoint],\n",
    "              use_multiprocessing=True,workers=multiprocessing.cpu_count() - 8)\n",
    "\n",
    "    #Final Evaluation of Optimal Model on Test Data\n",
    "    final_model = load_model('language_detection_wan.h5',custom_objects={'class_recall':class_recall})\n",
    "    preds = final_model.predict(np.array(test_tokens))\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc248200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:ModelCheckpoint mode <built-in function max> is unknown, fallback to auto mode.\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 1000, 300)    36396000    ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " attention_query1 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query2 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query3 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query4 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query5 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query6 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query7 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query8 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query9 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query10 (Dense)      (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1000)      0           ['attention_query1[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1000)      0           ['attention_query2[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 1000)      0           ['attention_query3[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 1000)      0           ['attention_query4[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 1000)      0           ['attention_query5[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 1000)      0           ['attention_query6[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 1000)      0           ['attention_query7[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 1, 1000)      0           ['attention_query8[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 1, 1000)      0           ['attention_query9[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 1, 1000)      0           ['attention_query10[0][0]']      \n",
      "                                                                                                  \n",
      " attention_weights1 (Lambda)    (None, 1, 1000)      0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " attention_weights2 (Lambda)    (None, 1, 1000)      0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights3 (Lambda)    (None, 1, 1000)      0           ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights4 (Lambda)    (None, 1, 1000)      0           ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights5 (Lambda)    (None, 1, 1000)      0           ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights6 (Lambda)    (None, 1, 1000)      0           ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights7 (Lambda)    (None, 1, 1000)      0           ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights8 (Lambda)    (None, 1, 1000)      0           ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights9 (Lambda)    (None, 1, 1000)      0           ['reshape_8[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights10 (Lambda)   (None, 1, 1000)      0           ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights1[0][0]']     \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights2[0][0]']     \n",
      "                                                                                                  \n",
      " dot_2 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights3[0][0]']     \n",
      "                                                                                                  \n",
      " dot_3 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights4[0][0]']     \n",
      "                                                                                                  \n",
      " dot_4 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights5[0][0]']     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dot_5 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights6[0][0]']     \n",
      "                                                                                                  \n",
      " dot_6 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights7[0][0]']     \n",
      "                                                                                                  \n",
      " dot_7 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights8[0][0]']     \n",
      "                                                                                                  \n",
      " dot_8 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights9[0][0]']     \n",
      "                                                                                                  \n",
      " dot_9 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights10[0][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 300)          0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 300)          0           ['dot_2[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 300)          0           ['dot_3[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 300)          0           ['dot_4[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 300)          0           ['dot_5[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 300)          0           ['dot_6[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 300)          0           ['dot_7[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 300)          0           ['dot_8[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 300)          0           ['dot_9[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3000)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]',              \n",
      "                                                                  'flatten_8[0][0]',              \n",
      "                                                                  'flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 10, 300)      0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " attention_query (Dense)        (None, 10, 1)        300         ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 1, 10)        0           ['attention_query[0][0]']        \n",
      "                                                                                                  \n",
      " attention_weights (Lambda)     (None, 1, 10)        0           ['reshape_11[0][0]']             \n",
      "                                                                                                  \n",
      " dot_10 (Dot)                   (None, 300, 1)       0           ['reshape_10[0][0]',             \n",
      "                                                                  'attention_weights[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 300)          0           ['dot_10[0][0]']                 \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 100)          30100       ['flatten_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100)          0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " wan_classification (Dense)     (None, 11)           1111        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 36,430,511\n",
      "Trainable params: 36,430,511\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - ETA: 0s - loss: 2.4393 - accuracy: 0.8386 - class_recall: 0.7462"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - 959s 632ms/step - loss: 2.4393 - accuracy: 0.8386 - class_recall: 0.7462 - val_loss: 0.5574 - val_accuracy: 0.8543 - val_class_recall: 0.7582\n",
      "Epoch 2/10\n",
      "1518/1518 [==============================] - 964s 635ms/step - loss: 1.5914 - accuracy: 0.8552 - class_recall: 0.7676 - val_loss: 0.4636 - val_accuracy: 0.8926 - val_class_recall: 0.8216\n",
      "Epoch 3/10\n",
      "1518/1518 [==============================] - 958s 631ms/step - loss: 1.8276 - accuracy: 0.8897 - class_recall: 0.8230 - val_loss: 0.7163 - val_accuracy: 0.8899 - val_class_recall: 0.8208\n",
      "Epoch 4/10\n",
      "1518/1518 [==============================] - 957s 631ms/step - loss: 1.5393 - accuracy: 0.8921 - class_recall: 0.8332 - val_loss: 0.4381 - val_accuracy: 0.9202 - val_class_recall: 0.8716\n",
      "Epoch 5/10\n",
      "1518/1518 [==============================] - 956s 630ms/step - loss: 1.2395 - accuracy: 0.9018 - class_recall: 0.8503 - val_loss: 0.5769 - val_accuracy: 0.9150 - val_class_recall: 0.8707\n",
      "Epoch 6/10\n",
      "1518/1518 [==============================] - 951s 626ms/step - loss: 1.3243 - accuracy: 0.8969 - class_recall: 0.8427 - val_loss: 0.4518 - val_accuracy: 0.9123 - val_class_recall: 0.8621\n",
      "Epoch 7/10\n",
      "1518/1518 [==============================] - 952s 627ms/step - loss: 1.2422 - accuracy: 0.9017 - class_recall: 0.8481 - val_loss: 0.4397 - val_accuracy: 0.9301 - val_class_recall: 0.8922\n",
      "Epoch 8/10\n",
      "1518/1518 [==============================] - 952s 627ms/step - loss: 1.3196 - accuracy: 0.8750 - class_recall: 0.8164 - val_loss: 0.7155 - val_accuracy: 0.8668 - val_class_recall: 0.7912\n",
      "Epoch 9/10\n",
      "1518/1518 [==============================] - 956s 630ms/step - loss: 1.2719 - accuracy: 0.8923 - class_recall: 0.8429 - val_loss: 0.6539 - val_accuracy: 0.8431 - val_class_recall: 0.7743\n",
      "Epoch 10/10\n",
      "1518/1518 [==============================] - 952s 627ms/step - loss: 1.3315 - accuracy: 0.8721 - class_recall: 0.8195 - val_loss: 0.6248 - val_accuracy: 0.9077 - val_class_recall: 0.8582\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 10s 203ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = train_eval_wan_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2f113fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(confusion_matrix(test_labels.map(label_mapping),np.array([x.argmax() for x in preds])))\n",
    "test_results.index = [num_label_to_map[x] for x in test_results.index]\n",
    "test_results.columns = test_results.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d01a4bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <th>Italian</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Other</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Romanian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>6</td>\n",
       "      <td>285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finnish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Spanish  Portuguese  English  Kinyarwanda  Italian  French  \\\n",
       "Spanish          261           1        2            0        0       0   \n",
       "Portuguese         6         285        0            0        0       1   \n",
       "English            0           0      241            1       24       1   \n",
       "Kinyarwanda        2           0        2          154        4       0   \n",
       "Italian            0           1        1            0      132       1   \n",
       "French             1           2        6            0        0     103   \n",
       "German             0           0        1            0        0       0   \n",
       "Other              1           1       13            1        2       0   \n",
       "Finnish            0           0        0            1        0       0   \n",
       "Swedish            0           0        0            0        0       0   \n",
       "Romanian           0           0        0            0        0       0   \n",
       "\n",
       "             German  Other  Finnish  Swedish  Romanian  \n",
       "Spanish           0     13        9        0         0  \n",
       "Portuguese        0     12       15        0         0  \n",
       "English           0     17       21        0         3  \n",
       "Kinyarwanda       0      3        0        0         0  \n",
       "Italian           0      7        5        0         0  \n",
       "French            0      1        3        0         0  \n",
       "German           56     20        3        0         0  \n",
       "Other             0     14       27        0         1  \n",
       "Finnish           0      4       10        0         0  \n",
       "Swedish           0      0        0        8         0  \n",
       "Romanian          0      3        0        0        10  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5fb701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_class_recall = round(test_results/np.array(test_results.sum(axis=1).repeat(11)).reshape(11,11),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "188c1b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <th>Italian</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Other</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Romanian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>0.913</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kinyarwanda</th>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Finnish</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swedish</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romanian</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Spanish  Portuguese  English  Kinyarwanda  Italian  French  \\\n",
       "Spanish        0.913       0.003    0.007        0.000    0.000   0.000   \n",
       "Portuguese     0.019       0.893    0.000        0.000    0.000   0.003   \n",
       "English        0.000       0.000    0.782        0.003    0.078   0.003   \n",
       "Kinyarwanda    0.012       0.000    0.012        0.933    0.024   0.000   \n",
       "Italian        0.000       0.007    0.007        0.000    0.898   0.007   \n",
       "French         0.009       0.017    0.052        0.000    0.000   0.888   \n",
       "German         0.000       0.000    0.012        0.000    0.000   0.000   \n",
       "Other          0.017       0.017    0.217        0.017    0.033   0.000   \n",
       "Finnish        0.000       0.000    0.000        0.067    0.000   0.000   \n",
       "Swedish        0.000       0.000    0.000        0.000    0.000   0.000   \n",
       "Romanian       0.000       0.000    0.000        0.000    0.000   0.000   \n",
       "\n",
       "             German  Other  Finnish  Swedish  Romanian  \n",
       "Spanish         0.0  0.045    0.031      0.0     0.000  \n",
       "Portuguese      0.0  0.038    0.047      0.0     0.000  \n",
       "English         0.0  0.055    0.068      0.0     0.010  \n",
       "Kinyarwanda     0.0  0.018    0.000      0.0     0.000  \n",
       "Italian         0.0  0.048    0.034      0.0     0.000  \n",
       "French          0.0  0.009    0.026      0.0     0.000  \n",
       "German          0.7  0.250    0.038      0.0     0.000  \n",
       "Other           0.0  0.233    0.450      0.0     0.017  \n",
       "Finnish         0.0  0.267    0.667      0.0     0.000  \n",
       "Swedish         0.0  0.000    0.000      1.0     0.000  \n",
       "Romanian        0.0  0.231    0.000      0.0     0.769  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_class_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2426b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Recall: 0.7887272727272727\n",
      "Accuracy: 0.8398154251812788\n"
     ]
    }
   ],
   "source": [
    "print('Class Recall: ' + str(np.array(test_results_class_recall).diagonal().sum()/11))\n",
    "print('Accuracy: ' + str(np.array(test_results).diagonal().sum()/np.array(test_results).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2eb5845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_results,open('wan_model_test_results.pkl','wb'))\n",
    "pickle.dump(test_results_class_recall,open('wan_model_test_class_recall.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b682d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "559918cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 20:39:46.805688: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ff_model = load_model('language_detection_ff_tf.h5',custom_objects={'class_recall':class_recall})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c74369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kinyarwanda'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = val_set[val_set['language label'] == 'Kinyarwanda'].iloc[2,0]\n",
    "input_text = vectorizer.transform([input_text]).todense()\n",
    "input_text = input_text/input_text.sum()\n",
    "reverse_mapping[ff_model.predict(input_text).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bcb3138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327b3bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spanish': 0,\n",
       " 'Portuguese': 1,\n",
       " 'English': 2,\n",
       " 'Kinyarwanda': 3,\n",
       " 'Italian': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Other': 7,\n",
       " 'Finnish': 8,\n",
       " 'Swedish': 9,\n",
       " 'Romanian': 10}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4b03fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look around, what a lovely day\\nThere's feelin' we'll have a great day\\n\\nThe sky is blue as it can ever be\\nOh, how the breezy wind relaxes me\\n\\nItsumo ijou ni agaridasu tenshon wo I can't help stop it\\nOsaekirenai machikirenai special day\\n\\nOoh baby let's go\\nDriving, driving motto\\nDriving, driving (oh yeah)\\nVolume agete (let's go) your favorite music\\n\\nDriving, driving kyou wa\\nDriving, driving (oh yeah)\\nTanoshimou futari no free time\\n\\nAtemonaku hashirou kono highway wo tobashite\\nHikaru kono umi wo koe tadoritsuku no secret zone\\n\\nItsumo ijou ni hashaijau watashi wo I can't help stop it\\nYasashiku uketomete mou hajimatteru sweet time\\n\\nOoh baby let's go\\nDriving, driving motto\\nDriving, driving (oh yeah)\\nVolume agete (let's go) your favorite music\\n\\nDriving, driving kyou wa\\nDriving, driving (oh yeah)\\nDare mo shiranai (let's go) secret time\\n\\nOoh baby let's go\\nDriving, driving motto\\nDriving, driving (oh yeah)\\nTanoshimou futari no let's go special day\\n\\nDriving, driving keep going\\nDriving, driving (oh yeah)\\nDokomademo ikou let's try\\n\\nOoh baby let's go\\nDriving, driving motto\\nDriving, driving (oh yeah)\\n\\nDriving, driving kyou wa\\nDriving, driving (oh yeah)\\n\\nOoh baby let's go\\nDriving, driving keep going\\nDriving driving (oh yeah)...\\n\\nDriving, driving...\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set[val_set['language label'] == 'Kinyarwanda'].iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
