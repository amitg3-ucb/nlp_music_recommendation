{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fcebc0",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae72afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,recall_score\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebf11c",
   "metadata": {},
   "source": [
    "# 2. Read in Language Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34becf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts in Train Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Spanish        2405\n",
       "Portuguese     2372\n",
       "English        2359\n",
       "Kinyarwanda    1366\n",
       "Italian        1162\n",
       "French          999\n",
       "German          698\n",
       "Other           498\n",
       "Finnish         111\n",
       "Swedish          90\n",
       "Romanian         78\n",
       "Name: language label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset = pd.read_csv('Train_Test_Data/train.csv')[['Lyric','language label']]\n",
    "test_dataset = pd.read_csv('Train_Test_Data/test.csv')[['Lyric','language label']]\n",
    "print('Label Counts in Train Set')\n",
    "display(sample_dataset['language label'].value_counts())\n",
    "train_set = sample_dataset\n",
    "val_set = test_dataset.iloc[:1517]\n",
    "test_set = test_dataset.iloc[1517:]\n",
    "test_set.index = np.arange(0,len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dda47",
   "metadata": {},
   "source": [
    "#### Resampled Version of Train Set for Non Class Weight Method of Dealing With Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750e833e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teu olhar\\nFez cinema mim\\nEm cada sessão\\nFui...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tristeza, por favor vá embora\\nMinha alma que ...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Só procurou, sempre encontrou\\nAmores tontos, ...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cruzei uma doida - Charlie Brown Jr\\n\\nCruzei ...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Como as cores num retrato o tempo insiste em d...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26450</th>\n",
       "      <td>Hednaorden\\n\\nI ekot av fäders stolthet\\nVår h...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26451</th>\n",
       "      <td>Utmed brådkalla rännilars fors och fall\\nYrväd...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26452</th>\n",
       "      <td>Inciklad av stjärnhärars krestsande fält,\\nfrå...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26453</th>\n",
       "      <td>När jag tänker på den ständiga resan genom liv...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26454</th>\n",
       "      <td>Jag vaknar upp, tidigt på morgon å tittar ut o...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26455 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lyric language label\n",
       "0      Teu olhar\\nFez cinema mim\\nEm cada sessão\\nFui...     Portuguese\n",
       "1      Tristeza, por favor vá embora\\nMinha alma que ...     Portuguese\n",
       "2      Só procurou, sempre encontrou\\nAmores tontos, ...     Portuguese\n",
       "3      Cruzei uma doida - Charlie Brown Jr\\n\\nCruzei ...     Portuguese\n",
       "4      Como as cores num retrato o tempo insiste em d...     Portuguese\n",
       "...                                                  ...            ...\n",
       "26450  Hednaorden\\n\\nI ekot av fäders stolthet\\nVår h...        Swedish\n",
       "26451  Utmed brådkalla rännilars fors och fall\\nYrväd...        Swedish\n",
       "26452  Inciklad av stjärnhärars krestsande fält,\\nfrå...        Swedish\n",
       "26453  När jag tänker på den ständiga resan genom liv...        Swedish\n",
       "26454  Jag vaknar upp, tidigt på morgon å tittar ut o...        Swedish\n",
       "\n",
       "[26455 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(50)\n",
    "max_class_counts = train_set['language label'].value_counts().iloc[0]\n",
    "resampled_train_set = pd.DataFrame()\n",
    "for lang in train_set['language label'].unique():\n",
    "    subset = train_set[train_set['language label'] == lang].copy()\n",
    "    if len(subset) == max_class_counts:\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset],ignore_index=True)\n",
    "    else:\n",
    "        added_subset = subset.iloc[random.choices(np.arange(0,len(subset)),k=max_class_counts - len(subset))]\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset,added_subset],ignore_index=True)\n",
    "        \n",
    "display(resampled_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a87df1",
   "metadata": {},
   "source": [
    "# 3. Create Term Density Representation of train and val/test lyrics where terms are from non-other class lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdd175",
   "metadata": {},
   "source": [
    "#### Preprocess Text, Create Vectorizer fit on non-other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfaef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function preprocess_text at 0x7fdd0d549700>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocess_text)\n",
    "vectorizer.fit(train_set['Lyric'][train_set['language label'] != 'Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faf874",
   "metadata": {},
   "source": [
    "#### Lyrics to Term Density, Featurization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add64c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyrics_to_term_density(text_df,vectorizer):\n",
    "    lyrics = vectorizer.transform(text_df['Lyric'])\n",
    "    lyrics = pd.DataFrame(lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "    label = text_df['language label'].copy()\n",
    "    label.index = np.arange(0,len(lyrics))\n",
    "    lyrics.dropna(inplace=True)\n",
    "    label = label.loc[lyrics.index]\n",
    "    token_count = np.array(text_df['Lyric'].apply(lambda x:len(preprocess_text(x).split())))\n",
    "    token_count = token_count.repeat(lyrics.shape[1])\n",
    "    token_count = token_count.reshape(lyrics.shape)\n",
    "    lyrics = (lyrics/token_count).astype('float32')\n",
    "    lyrics = scipy.sparse.csr_matrix(lyrics)\n",
    "    return lyrics,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81b1d9",
   "metadata": {},
   "source": [
    "#### Featurize Lyrics, Train Set, Resampled Train Set, Val Set, Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35500061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize Train Lyrics\n",
    "train_lyrics = vectorizer.transform(train_set['Lyric'])\n",
    "train_lyrics = pd.DataFrame(train_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "train_lyrics_token_count = train_lyrics.sum(axis=1)\n",
    "train_lyrics = train_lyrics/np.array(train_lyrics_token_count.repeat(len(train_lyrics.columns))).reshape(train_lyrics.shape)\n",
    "\n",
    "#Oversampled Vectorize Train Lyrics\n",
    "resampled_train_lyrics = vectorizer.transform(resampled_train_set['Lyric'])\n",
    "resampled_train_lyrics = pd.DataFrame(resampled_train_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "resampled_train_lyrics_token_count = resampled_train_lyrics.sum(axis=1)\n",
    "resampled_train_lyrics = resampled_train_lyrics/np.array(resampled_train_lyrics_token_count.repeat(len(resampled_train_lyrics.columns))).reshape(resampled_train_lyrics.shape)\n",
    "\n",
    "#Vectorize Val Lyrics\n",
    "val_lyrics = vectorizer.transform(val_set['Lyric'])\n",
    "val_lyrics = pd.DataFrame(val_lyrics.todense(),columns = vectorizer.get_feature_names(),index=val_set.index)\n",
    "val_lyrics_token_count = val_lyrics.sum(axis=1)\n",
    "val_lyrics = val_lyrics/np.array(val_lyrics_token_count.repeat(len(val_lyrics.columns))).reshape(val_lyrics.shape)\n",
    "\n",
    "#Vectorize Test Lyrics\n",
    "test_lyrics = vectorizer.transform(test_set['Lyric'])\n",
    "test_lyrics = pd.DataFrame(test_lyrics.todense(),columns = vectorizer.get_feature_names(),index=test_set.index)\n",
    "test_lyrics_token_count = test_lyrics.sum(axis=1)\n",
    "test_lyrics = test_lyrics/np.array(test_lyrics_token_count.repeat(len(test_lyrics.columns))).reshape(test_lyrics.shape)\n",
    "\n",
    "resampled_train_labels = resampled_train_set['language label']\n",
    "train_labels = train_set['language label']\n",
    "val_labels = val_set['language label']\n",
    "test_labels = test_set['language label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac0740",
   "metadata": {},
   "source": [
    "#### Dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218a3d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>000000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>00000002</th>\n",
       "      <th>0001</th>\n",
       "      <th>00011</th>\n",
       "      <th>00012</th>\n",
       "      <th>00015</th>\n",
       "      <th>...</th>\n",
       "      <th>心の鱗を剥がそう</th>\n",
       "      <th>正解も不正解もないシーツの波</th>\n",
       "      <th>沈んでいく</th>\n",
       "      <th>泳いだ</th>\n",
       "      <th>溺れていく</th>\n",
       "      <th>激しく真実だけ抱いてほしい</th>\n",
       "      <th>覗いてくれ</th>\n",
       "      <th>話せない大切な弱さを</th>\n",
       "      <th>露に青く透き通った</th>\n",
       "      <th>飾りはいいよ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1517 rows × 122970 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  0000  000000  00000000  00000002  0001  00011  00012  00015  \\\n",
       "0     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "2     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "3     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "4     0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "...   ...  ...   ...     ...       ...       ...   ...    ...    ...    ...   \n",
       "1512  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1513  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1514  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1515  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "1516  0.0  0.0   0.0     0.0       0.0       0.0   0.0    0.0    0.0    0.0   \n",
       "\n",
       "      ...  心の鱗を剥がそう  正解も不正解もないシーツの波  沈んでいく  泳いだ  溺れていく  激しく真実だけ抱いてほしい  覗いてくれ  \\\n",
       "0     ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "1     ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "2     ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "3     ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "4     ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "...   ...       ...             ...    ...  ...    ...            ...    ...   \n",
       "1512  ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "1513  ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "1514  ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "1515  ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "1516  ...       0.0             0.0    0.0  0.0    0.0            0.0    0.0   \n",
       "\n",
       "      話せない大切な弱さを  露に青く透き通った  飾りはいいよ  \n",
       "0            0.0        0.0     0.0  \n",
       "1            0.0        0.0     0.0  \n",
       "2            0.0        0.0     0.0  \n",
       "3            0.0        0.0     0.0  \n",
       "4            0.0        0.0     0.0  \n",
       "...          ...        ...     ...  \n",
       "1512         0.0        0.0     0.0  \n",
       "1513         0.0        0.0     0.0  \n",
       "1514         0.0        0.0     0.0  \n",
       "1515         0.0        0.0     0.0  \n",
       "1516         0.0        0.0     0.0  \n",
       "\n",
       "[1517 rows x 122970 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lyrics.astype('float32')\n",
    "resampled_train_lyrics.astype('float32')\n",
    "val_lyrics.astype('float32')\n",
    "test_lyrics.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53906c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lyrics.fillna(0,inplace=True)\n",
    "resampled_train_lyrics.fillna(0,inplace=True)\n",
    "val_lyrics.fillna(0,inplace=True)\n",
    "test_lyrics.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e1cff",
   "metadata": {},
   "source": [
    "# 4. ID Class Imbalance and ID Weights for Each Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee26dd",
   "metadata": {},
   "source": [
    "#### Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232efb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spanish        2405\n",
       "Portuguese     2372\n",
       "English        2359\n",
       "Kinyarwanda    1366\n",
       "Italian        1162\n",
       "French          999\n",
       "German          698\n",
       "Other           498\n",
       "Finnish         111\n",
       "Swedish          90\n",
       "Romanian         78\n",
       "Name: language label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = train_set['language label'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc6475",
   "metadata": {},
   "source": [
    "#### Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908e9094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spanish         1.000000\n",
       "Portuguese      1.013912\n",
       "English         1.019500\n",
       "Kinyarwanda     1.760615\n",
       "Italian         2.069707\n",
       "French          2.407407\n",
       "German          3.445559\n",
       "Other           4.829317\n",
       "Finnish        21.666667\n",
       "Swedish        26.722222\n",
       "Romanian       30.833333\n",
       "Name: language label, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = class_counts.iloc[0]/class_counts\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097f902",
   "metadata": {},
   "source": [
    "#### Labels for Resampled Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c287650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Portuguese     2405\n",
       "English        2405\n",
       "Italian        2405\n",
       "German         2405\n",
       "Kinyarwanda    2405\n",
       "Spanish        2405\n",
       "Finnish        2405\n",
       "French         2405\n",
       "Other          2405\n",
       "Romanian       2405\n",
       "Swedish        2405\n",
       "Name: language label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts1 = resampled_train_set['language label'].value_counts()\n",
    "class_counts1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceadd8d",
   "metadata": {},
   "source": [
    "#### Weights for Resampled Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b031d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Portuguese     1.0\n",
       "English        1.0\n",
       "Italian        1.0\n",
       "German         1.0\n",
       "Kinyarwanda    1.0\n",
       "Spanish        1.0\n",
       "Finnish        1.0\n",
       "French         1.0\n",
       "Other          1.0\n",
       "Romanian       1.0\n",
       "Swedish        1.0\n",
       "Name: language label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights1 = class_counts1.iloc[0]/class_counts1\n",
    "class_weights1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9894c2",
   "metadata": {},
   "source": [
    "#### Mapping Language to Numerical Label, Mapping Numerical Label to Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67077ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {}\n",
    "weight_mapping = {}\n",
    "count = 0\n",
    "for index in class_counts.index:\n",
    "    label_mapping[index] = count\n",
    "    weight_mapping[count] = class_weights.loc[index]\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3567ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping1 = {}\n",
    "weight_mapping1 = {}\n",
    "count1 = 0\n",
    "for index in class_counts1.index:\n",
    "    label_mapping1[index] = count\n",
    "    weight_mapping1[count1] = class_weights1.loc[index]\n",
    "    count1 = count1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe3d06",
   "metadata": {},
   "source": [
    "# 5. Feed Forward Network For Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2201498",
   "metadata": {},
   "source": [
    "#### Custom Metric for Evaluating Performance - Average Class Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a3ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_recall(y_true,y_pred):\n",
    "    #true labels\n",
    "    true = y_true.numpy()\n",
    "    #predicted prob of each class for each sample\n",
    "    pred = y_pred.numpy()\n",
    "    #prob to class based off max predicted prob\n",
    "    pred = np.array([x.argmax() for x in pred])\n",
    "    #confusion matrix\n",
    "    confuse = confusion_matrix(true,pred)\n",
    "    confuse_sum = confuse.sum(axis=1)\n",
    "    score = 0\n",
    "    for num in range(len(confuse_sum)):\n",
    "        if confuse_sum[num]!=0:\n",
    "            score = score + confuse[num][num]/confuse_sum[num]\n",
    "    \n",
    "    return score/len(confuse_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3c67f",
   "metadata": {},
   "source": [
    "#### Initialize FF Neural Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e44aea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feed_forward_network(\n",
    "                     shape=(1000,),\n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001,\n",
    "                     metrics = ['accuracy']):\n",
    "    \"\"\"\n",
    "    Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    input_layer = tf.keras.layers.Input(shape=shape,dtype='float32',name='input')    \n",
    "    #input into hidden layers\n",
    "    x = input_layer #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        #dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = hidden\n",
    "        \n",
    "    classification = tf.keras.layers.Dense(output_layer_size, activation='softmax', name='classification')(x)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs=[classification])\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "                 metrics=metrics,\n",
    "                 run_eagerly=True)\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fb065",
   "metadata": {},
   "source": [
    "#### Initialize and Train/Evaluate FF Neural Network to Detect Primary Music Language of Song Given Term Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c074aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    xtrain, xval, xtest, ytrain, yval, ytest, # Train/Val/Test Data\n",
    "    class_weights, batch_size, epochs, # Attributes for Fit Method of Model\n",
    "    patience, mode, #attributes for early stoppage\n",
    "    savepath,#attributes for model checkpoints\n",
    "    #Attributes for Model Architecture\n",
    "    hidden_dim=[100,100,100],\n",
    "    dropout_rate=0,\n",
    "    hidden_layer_activation = 'relu',\n",
    "    output_layer_size = 4,\n",
    "    output_activation = 'softmax',\n",
    "    learning_rate=0.001,\n",
    "    metrics = ['accuracy'],\n",
    "    opt_metric = 'class_recall',\n",
    "    opt_func = class_recall):\n",
    "    \n",
    "    #Sparse to Dense Matrices\n",
    "    xtr_dense = xtrain.copy()\n",
    "    xva_dense = xval.copy()\n",
    "    xte_dense = xtest.copy()\n",
    "    \n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    #Initialize Architecture\n",
    "    model = create_feed_forward_network(shape=(xtr_dense.shape[1],),hidden_dim=hidden_dim,\n",
    "                                        dropout_rate=dropout_rate,hidden_layer_activation=hidden_layer_activation,\n",
    "                                        output_layer_size=output_layer_size,\n",
    "                                        output_activation=output_activation,\n",
    "                                        learning_rate=learning_rate,metrics=metrics)\n",
    "    \n",
    "    #Early Stoppage and Model Checkpoints Objects\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_' + opt_metric,verbose=1,patience=patience,mode=mode)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(savepath,monitor='val_' + opt_metric,save_best_only=True,mode=mode)\n",
    "    \n",
    "    #Fit Model on Training Data, iteratively evaluate on val data\n",
    "    model.fit(xtr_dense,ytrain,\n",
    "              validation_data=(xva_dense, yval),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              shuffle=True,\n",
    "              class_weight = class_weights,\n",
    "              callbacks = [stoppage,checkpoint],\n",
    "              use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)\n",
    "    \n",
    "    #Final Evaluation of Optimal Model on Test Data\n",
    "    final_model = load_model(savepath,custom_objects={opt_metric:opt_func})\n",
    "    preds = final_model.predict(xte_dense)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf51d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 122970)]          0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 100)               12297100  \n",
      "                                                                 \n",
      " classification (Dense)      (None, 11)                1111      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,298,211\n",
      "Trainable params: 12,298,211\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-12 01:10:03.403877: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - ETA: 0s - loss: 0.7539 - accuracy: 0.9521 - class_recall: 0.9249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1518/1518 [==============================] - 126s 81ms/step - loss: 0.7539 - accuracy: 0.9521 - class_recall: 0.9249 - val_loss: 0.0877 - val_accuracy: 0.9769 - val_class_recall: 0.9667\n",
      "Epoch 2/30\n",
      "1518/1518 [==============================] - 121s 80ms/step - loss: 0.0715 - accuracy: 0.9914 - class_recall: 0.9846 - val_loss: 0.1114 - val_accuracy: 0.9736 - val_class_recall: 0.9551\n",
      "Epoch 3/30\n",
      "1518/1518 [==============================] - 122s 80ms/step - loss: 0.0102 - accuracy: 0.9991 - class_recall: 0.9983 - val_loss: 0.1091 - val_accuracy: 0.9782 - val_class_recall: 0.9694\n",
      "Epoch 4/30\n",
      "1518/1518 [==============================] - 121s 80ms/step - loss: 7.9046e-04 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1079 - val_accuracy: 0.9763 - val_class_recall: 0.9659\n",
      "Epoch 5/30\n",
      "1518/1518 [==============================] - 124s 81ms/step - loss: 2.7005e-04 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1247 - val_accuracy: 0.9782 - val_class_recall: 0.9694\n",
      "Epoch 6/30\n",
      "1518/1518 [==============================] - 123s 81ms/step - loss: 1.1712e-04 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9782 - val_class_recall: 0.9694\n",
      "Epoch 7/30\n",
      "1518/1518 [==============================] - 124s 81ms/step - loss: 5.6171e-05 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1410 - val_accuracy: 0.9789 - val_class_recall: 0.9709\n",
      "Epoch 8/30\n",
      "1518/1518 [==============================] - 122s 80ms/step - loss: 2.6483e-05 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1438 - val_accuracy: 0.9789 - val_class_recall: 0.9709\n",
      "Epoch 9/30\n",
      "1518/1518 [==============================] - 121s 80ms/step - loss: 1.2289e-05 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9789 - val_class_recall: 0.9709\n",
      "Epoch 10/30\n",
      "1518/1518 [==============================] - 122s 80ms/step - loss: 6.0318e-06 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1511 - val_accuracy: 0.9789 - val_class_recall: 0.9708\n",
      "Epoch 11/30\n",
      "1518/1518 [==============================] - 121s 80ms/step - loss: 2.7819e-06 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1710 - val_accuracy: 0.9789 - val_class_recall: 0.9709\n",
      "Epoch 12/30\n",
      "1518/1518 [==============================] - 123s 81ms/step - loss: 1.3869e-06 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1796 - val_accuracy: 0.9796 - val_class_recall: 0.9718\n",
      "Epoch 13/30\n",
      "1518/1518 [==============================] - 124s 81ms/step - loss: 6.8086e-07 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1870 - val_accuracy: 0.9796 - val_class_recall: 0.9718\n",
      "Epoch 14/30\n",
      "1518/1518 [==============================] - 123s 81ms/step - loss: 3.3198e-07 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9802 - val_class_recall: 0.9736\n",
      "Epoch 15/30\n",
      "1518/1518 [==============================] - 122s 80ms/step - loss: 1.7277e-07 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9802 - val_class_recall: 0.9736\n",
      "Epoch 16/30\n",
      "1518/1518 [==============================] - 122s 80ms/step - loss: 8.8371e-08 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.2127 - val_accuracy: 0.9796 - val_class_recall: 0.9727\n",
      "Epoch 17/30\n",
      "1518/1518 [==============================] - 121s 80ms/step - loss: 5.4096e-08 - accuracy: 1.0000 - class_recall: 1.0000 - val_loss: 0.2140 - val_accuracy: 0.9796 - val_class_recall: 0.9718\n",
      "Epoch 18/30\n",
      "1518/1518 [==============================] - 122s 80ms/step - loss: 0.0093 - accuracy: 0.9986 - class_recall: 0.9979 - val_loss: 0.2314 - val_accuracy: 0.9756 - val_class_recall: 0.9644\n",
      "Epoch 19/30\n",
      "1518/1518 [==============================] - 121s 80ms/step - loss: 2.0537e-04 - accuracy: 0.9999 - class_recall: 0.9999 - val_loss: 0.2618 - val_accuracy: 0.9750 - val_class_recall: 0.9646\n",
      "Epoch 19: early stopping\n",
      " 3/48 [>.............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = eval_model(xtrain = np.array(train_lyrics), xval = np.array(val_lyrics), xtest = np.array(test_lyrics), \n",
    "           ytrain = train_labels.map(label_mapping),\n",
    "           yval = val_labels.map(label_mapping),\n",
    "           ytest = test_labels.map(label_mapping),\n",
    "           class_weights = weight_mapping, batch_size=8, epochs = 30,\n",
    "           patience=5,mode='max',savepath='language_detection_ff_tf.h5',\n",
    "           hidden_dim=[100],dropout_rate=0.3,hidden_layer_activation='relu',\n",
    "           output_layer_size=11,output_activation='softmax',\n",
    "           learning_rate = 0.01,metrics=['accuracy',class_recall],opt_metric='class_recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "291697a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[294,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, 314,   0,   0,   1,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,   0, 322,   1,   0,   0,   0,   3,   0,   0,   0],\n",
       "       [  1,   0,   1, 161,   0,   0,   0,   1,   0,   0,   0],\n",
       "       [  1,   0,   2,   0, 135,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   1,   0,   0, 110,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   2,   0,   0,   0,  67,   0,   0,   0,   0],\n",
       "       [  1,   1,   6,   2,   1,   0,   0,  50,   0,   0,   1],\n",
       "       [  0,   0,   1,   0,   0,   0,   0,   0,  16,   0,   0],\n",
       "       [  0,   0,   1,   0,   0,   0,   0,   0,   0,  10,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   9]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels.map(label_mapping),np.array([x.argmax() for x in preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a16364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
