{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d278e917",
   "metadata": {},
   "source": [
    "# 1. Problem Statement:\n",
    "\n",
    "Given labeled data (feature, target), what is an optimal supervised machine learning model that generalizes best out of sample.\n",
    "\n",
    "Using grid search, a near optimal solution to the statement above can be quickly identified. By specifying our data, a model type, hyperparameter options, and an error metric, grid search uses cross validation to estimate the model's out of sample performance under different hyperparameter conditions enabling the selection of the combination that performs best i.e. minimizes our error metric of choice out of sample. By repeating this procedure for various models and associated hyperparameter options, we can identify a singular model type and hyperparameter settings that are estimated to perform best on new data.\n",
    "\n",
    "**Variants:**\n",
    "- Grid search methods can be varied across a variety of approaches: traditional, telescopic, random search, etc.\n",
    "- Threshold analysis can also be incorporated. In addition to returning a hard class assignment in classification settings, many models can also return a probabilistic output that corresponds to the P(Y = class k | x) for each class. In binary classification settings (Y = +/- 1), a threshold of 0.5 is typically assumed where if P(Y = class 1 | x) >= 0.5, predict 1 else predict -1. By storing out of sample probabilistic predictions throughout the cross validation process, we are also able to inspect whether modifying a model's probability threshold allows us to optimize performance even further with respect to our error metric of choice.\n",
    "\n",
    "**Key assumptions:**  \n",
    "\n",
    "This framework is primarily optimized for evaluating out of sample performance of classical machine learning models rather than deep learning models due to training time, lack of sensitivity to starting points for parametric models, and neural architecture training stoppage largely being dependent on validation set performance which is measured after each training epoch. While it can be used for deep learning, expect longer execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69891dd9",
   "metadata": {},
   "source": [
    "# 2. Develop grid search pipeline for IDing optimal hyperparameter settings given labeled data and model type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba28319",
   "metadata": {},
   "source": [
    "### Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166a810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#ML Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
    "\n",
    "#ML Model Evaluation\n",
    "from sklearn.metrics import (accuracy_score,f1_score,precision_score,\n",
    "                             recall_score,roc_auc_score,\n",
    "                             mean_squared_error,mean_absolute_error,r2_score)\n",
    "\n",
    "#Grid Search Methods\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\n",
    "\n",
    "#Clone Model\n",
    "from sklearn.base import clone\n",
    "\n",
    "#Parallel processing\n",
    "import multiprocessing\n",
    "\n",
    "#Dummy Dataset Generation\n",
    "from sklearn.datasets import make_classification,make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa75c7",
   "metadata": {},
   "source": [
    "### Develop Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878f098",
   "metadata": {},
   "source": [
    "#### General Scoring Function to evaluate model performance given true, pred values -- alter as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f0548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function(ytrue,ypred,metric):\n",
    "    if metric == 'f1':\n",
    "        score = f1_score(ytrue,ypred)\n",
    "    if metric == 'accuracy':\n",
    "        score = accuracy_score(ytrue,ypred)\n",
    "    if metric == 'precision':\n",
    "        score = precision_score(ytrue,ypred)\n",
    "    if metric == 'recall':\n",
    "        score = recall_score(ytrue,ypred)\n",
    "    if metric == 'neg_mean_absolute_error':\n",
    "        score = mean_absolute_error(ytrue,ypred)\n",
    "    if metric == 'neg_mean_squared_error':\n",
    "        score = mean_squared_error(ytrue,ypred)\n",
    "    if metric == 'r2':\n",
    "        score = r2_score(ytrue,ypred)\n",
    "    if metric == 'roc_auc':\n",
    "        score = roc_auc_score(ytrue,ypred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0752423",
   "metadata": {},
   "source": [
    "#### Optimal model development pipeline -- given labeled data and model type, identify model hyperparameters that optimize out of sample parameters therefore maximizing estimated generalization capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73379b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_model_dev(X,y,model_type,param_options,metric,metric_direction,\n",
    "                      search_method = 'trad',num_folds = 5,threshold_analysis_bool=False,random_state=50):\n",
    "    '''\n",
    "    X: features, assumes pandas df/series\n",
    "    y: labels, assumes pandas df/series\n",
    "    model_type: type of ML model\n",
    "    param_options: hyperparameter options for the given model to be evaluated through parameter search process\n",
    "    metric: metric we are trying to optimize out of sample, accepts one of ['accuracy','precision','recall','f1','roc_auc','neg_mean_squared_error','neg_mean_absolute_error','r2']\n",
    "    metric_direction: 'max' if higher metric = better, 'min' if lower metric = better\n",
    "    search_method: 'trad' = traditional grid search, 'rand' = random search\n",
    "    num_folds: number of folds throughout the cross validation process internal to grid search\n",
    "    threshold_analysis_bool: FOR BINARY CLASSIFICATION ONLY -- True to conduct out of sample threshold analysis, False otherwise\n",
    "    random_state: random state for reproducable results\n",
    "    '''\n",
    "    #Dictionary to Store Optimal Model Results\n",
    "    return_dict = {}\n",
    "    \n",
    "    #Create copies to avoid broadcasting\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    \n",
    "    #Instantiate search method/search space\n",
    "    if search_method == 'trad':\n",
    "        grid_search = GridSearchCV(estimator=model_type,param_grid=param_options,scoring=metric,\n",
    "                                   n_jobs=multiprocessing.cpu_count()-1,cv=num_folds)\n",
    "    elif search_method == 'rand':\n",
    "        #set number of combinations to evaluate\n",
    "        possible_combos = np.prod([len(x) for x in param_options.values()])\n",
    "        if possible_combos > 10 and possible_combos < 30:\n",
    "            possible_combos = 10\n",
    "        elif possible_combos >= 30:\n",
    "            possible_combos = int((1/3)*possible_combos)\n",
    "            \n",
    "        grid_search = RandomizedSearchCV(estimator=model_type,param_distributions=param_options,scoring=metric,\n",
    "                                        n_jobs=multiprocessing.cpu_count()-1,cv=num_folds,\n",
    "                                        n_iter=possible_combos)\n",
    "    \n",
    "    \n",
    "    #Fit search method\n",
    "    grid_search.fit(X_copy,y_copy)\n",
    "    \n",
    "    #store results of best performing model out of sample --> best hyperparameter settings to optimize model performance\n",
    "    return_dict['best_model'] = grid_search.best_estimator_\n",
    "    return_dict['best_params'] = grid_search.best_params_\n",
    "    return_dict['metric'] = metric\n",
    "    return_dict['best_score'] = grid_search.best_score_\n",
    "    \n",
    "    #conduct threshold analysis in binary class setting to find probabilistic threshold that optimizes performance\n",
    "    #of currently identified best performing model out of sample\n",
    "    if threshold_analysis_bool and y_copy.nunique() == 2:\n",
    "        oos_preds = y.copy() #store preds out of sample in CV process\n",
    "        kfold = KFold(n_splits=num_folds)#instantiate kfold object\n",
    "        model = clone(grid_search.best_estimator_)#clone model to capture best model's hyperparameters w/o training\n",
    "        \n",
    "        for train,test in kfold.split(X_copy):\n",
    "            #split data on k-1 folds for train, 1 test fold\n",
    "            xtrain,xtest,ytrain,ytest = X_copy.iloc[train],X_copy.iloc[test],y_copy.iloc[train],y_copy.iloc[test]\n",
    "            model.fit(xtrain,ytrain) #fit model\n",
    "            oos_preds.iloc[test] = list(model.predict_proba(xtest)[:,1]) #make preds, store out of sample pred P(Y = 1 | X)\n",
    "        \n",
    "        #conduct threshold analysis\n",
    "        best_score = grid_search.best_score_\n",
    "        best_thresh = 0.5\n",
    "        for num in np.linspace(0.01,0.999,500):\n",
    "            mapped_preds = oos_preds.apply(lambda x:1 if x>num else 0) #map probs to preds according to threshold\n",
    "            #evaluate performance\n",
    "            score = scoring_function(y,mapped_preds,metric)\n",
    "            if (metric_direction == 'max' and score>best_score) or (metric_direction == 'min' and score < best_score):\n",
    "                best_score = score\n",
    "                best_thresh = num\n",
    "        \n",
    "        return_dict['best_thresh'] = best_thresh\n",
    "        return_dict['best_score_adjusted'] = best_score\n",
    "            \n",
    "    return return_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763a091",
   "metadata": {},
   "source": [
    "### Test Optimal Model Development Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9b238",
   "metadata": {},
   "source": [
    "#### Create Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb228720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Classification\n",
    "binary_classification_data = make_classification(n_samples=2000,n_features=5,n_informative=5,n_redundant=0,n_classes=2)\n",
    "\n",
    "#Multiclass classification\n",
    "multiclass_classification_data = make_classification(n_samples=4000,n_features=5,n_informative=5,n_redundant=0,n_classes=4)\n",
    "\n",
    "#Regression\n",
    "regression_data = make_regression(n_samples = 1000, n_features = 5,n_informative=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baf3bd",
   "metadata": {},
   "source": [
    "#### Binary Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a9dfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.501\n",
       "1    0.499\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.DataFrame(binary_classification_data[0])\n",
    "label = pd.Series(binary_classification_data[1])\n",
    "label.value_counts()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99c0f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_model': KNeighborsClassifier(n_neighbors=7), 'best_params': {'p': 2, 'n_neighbors': 7}, 'metric': 'accuracy', 'best_score': 0.929375, 'best_thresh': 0.4301763527054108, 'best_score_adjusted': 0.93}\n",
      "\n",
      "{'best_model': RandomForestClassifier(criterion='entropy', n_estimators=300, random_state=50), 'best_params': {'n_estimators': 300, 'min_samples_split': 2, 'criterion': 'entropy'}, 'metric': 'accuracy', 'best_score': 0.9174999999999999, 'best_thresh': 0.531256513026052, 'best_score_adjusted': 0.918125}\n",
      "\n",
      "{'best_model': GradientBoostingClassifier(max_depth=6, max_features='sqrt', n_estimators=300), 'best_params': {'n_estimators': 300, 'max_depth': 6}, 'metric': 'accuracy', 'best_score': 0.9200000000000002, 'best_thresh': 0.24387174348697394, 'best_score_adjusted': 0.92125}\n",
      "\n",
      "{'best_model': GaussianNB(var_smoothing=1), 'best_params': {'var_smoothing': 1}, 'metric': 'accuracy', 'best_score': 0.8231249999999999, 'best_thresh': 0.5035090180360721, 'best_score_adjusted': 0.826875}\n",
      "\n",
      "{'best_model': LogisticRegression(C=1, solver='saga'), 'best_params': {'penalty': 'l2', 'C': 1}, 'metric': 'accuracy', 'best_score': 0.85125, 'best_thresh': 0.525310621242485, 'best_score_adjusted': 0.855625}\n",
      "\n",
      "Final Evaluation\n",
      "KNeighborsClassifier(n_neighbors=7)\n",
      "0.9125\n"
     ]
    }
   ],
   "source": [
    "models = [KNeighborsClassifier(),RandomForestClassifier(random_state=50),\n",
    "         GradientBoostingClassifier(max_features='sqrt'),GaussianNB(),LogisticRegression(solver='saga')]\n",
    "params = [{'n_neighbors':[3,5,7,9,11,13,15,17,19],'p':[1,2]},\n",
    "         {'n_estimators':[100,200,300,400],'criterion':['gini','entropy'],'min_samples_split':[2,4,6]},\n",
    "         {'n_estimators':[100,200,300,400],'max_depth':[2,3,4,6,8]},\n",
    "         {'var_smoothing':[0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1]},\n",
    "         {'C':[0.00001,0.0001,0.001,0.01,0.1,1,10],'penalty':['l1','l2']}\n",
    "         ]\n",
    "\n",
    "best_results = ''\n",
    "best_model = ''\n",
    "best_score = 0\n",
    "metric = 'accuracy'\n",
    "metric_direction = 'max'\n",
    "for model,param in zip(models,params):\n",
    "    results = optimal_model_dev(features[:1600],label[:1600],model,param,\n",
    "                      metric,metric_direction,'rand',5,True,50)\n",
    "    if results['best_score_adjusted'] > best_score:\n",
    "        best_results = results\n",
    "        best_score = results['best_score_adjusted']\n",
    "        best_model = results['best_model']\n",
    "    print(str(results) + '\\n')\n",
    "\n",
    "print('Final Evaluation')\n",
    "print(best_model)\n",
    "print(scoring_function(label[1600:],\n",
    "                 [1 if x >= best_results['best_thresh'] else 0 for x in best_model.predict_proba(features[1600:])[:,1]],\n",
    "                 metric=metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8557a7",
   "metadata": {},
   "source": [
    "#### Multiclass Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f3a660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.25100\n",
       "0    0.25000\n",
       "3    0.24975\n",
       "1    0.24925\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.DataFrame(multiclass_classification_data[0])\n",
    "label = pd.Series(multiclass_classification_data[1])\n",
    "label.value_counts()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb6ad4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_model': KNeighborsClassifier(n_neighbors=9), 'best_params': {'p': 2, 'n_neighbors': 9}, 'metric': 'accuracy', 'best_score': 0.8306250000000001}\n",
      "\n",
      "{'best_model': RandomForestClassifier(criterion='entropy', n_estimators=300, random_state=50), 'best_params': {'n_estimators': 300, 'min_samples_split': 2, 'criterion': 'entropy'}, 'metric': 'accuracy', 'best_score': 0.81125}\n",
      "\n",
      "{'best_model': GradientBoostingClassifier(max_depth=8, max_features='sqrt', n_estimators=200), 'best_params': {'n_estimators': 200, 'max_depth': 8}, 'metric': 'accuracy', 'best_score': 0.8146875}\n",
      "\n",
      "{'best_model': GaussianNB(var_smoothing=0.1), 'best_params': {'var_smoothing': 0.1}, 'metric': 'accuracy', 'best_score': 0.645625}\n",
      "\n",
      "{'best_model': LogisticRegression(C=0.1, penalty='l1', solver='saga'), 'best_params': {'penalty': 'l1', 'C': 0.1}, 'metric': 'accuracy', 'best_score': 0.6353125000000001}\n",
      "\n",
      "Final Evaluation\n",
      "KNeighborsClassifier(n_neighbors=9)\n",
      "0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amit.Gattadahallic3.ai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/Amit.Gattadahallic3.ai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/Amit.Gattadahallic3.ai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/Amit.Gattadahallic3.ai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/Amit.Gattadahallic3.ai/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "models = [KNeighborsClassifier(),RandomForestClassifier(random_state=50),\n",
    "         GradientBoostingClassifier(max_features='sqrt'),GaussianNB(),LogisticRegression(solver='saga')]\n",
    "params = [{'n_neighbors':[3,5,7,9,11,13,15,17,19],'p':[1,2]},\n",
    "         {'n_estimators':[100,200,300,400],'criterion':['gini','entropy'],'min_samples_split':[2,4,6]},\n",
    "         {'n_estimators':[100,200,300,400],'max_depth':[2,3,4,6,8]},\n",
    "         {'var_smoothing':[0.0000001,0.000001,0.00001,0.0001,0.001,0.01,0.1,1]},\n",
    "         {'C':[0.00001,0.0001,0.001,0.01,0.1,1,10],'penalty':['l1','l2']}\n",
    "         ]\n",
    "\n",
    "best_results = ''\n",
    "best_model = ''\n",
    "best_score = 0\n",
    "metric = 'accuracy'\n",
    "metric_direction = 'max'\n",
    "for model,param in zip(models,params):\n",
    "    results = optimal_model_dev(features[:3200],label[:3200],model,param,\n",
    "                      metric,metric_direction,'rand',5,True,50)\n",
    "    if results['best_score'] > best_score:\n",
    "        best_results = results\n",
    "        best_score = results['best_score']\n",
    "        best_model = results['best_model']\n",
    "    print(str(results) + '\\n')\n",
    "\n",
    "print('Final Evaluation')\n",
    "print(best_model)\n",
    "print(scoring_function(label[3200:],best_model.predict(features[3200:]),\n",
    "                 metric=metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1084f2",
   "metadata": {},
   "source": [
    "#### Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b892e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(regression_data[0])\n",
    "label = pd.Series(regression_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e43fab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_model': KNeighborsRegressor(n_neighbors=3), 'best_params': {'p': 2, 'n_neighbors': 3}, 'metric': 'r2', 'best_score': 0.9109167407182271}\n",
      "\n",
      "{'best_model': RandomForestRegressor(n_estimators=200, random_state=50), 'best_params': {'n_estimators': 200, 'min_samples_split': 2}, 'metric': 'r2', 'best_score': 0.9195511031482153}\n",
      "\n",
      "{'best_model': GradientBoostingRegressor(max_depth=2, max_features='sqrt', n_estimators=400), 'best_params': {'n_estimators': 400, 'max_depth': 2}, 'metric': 'r2', 'best_score': 0.9849280706856236}\n",
      "\n",
      "Final Evaluation\n",
      "GradientBoostingRegressor(max_depth=2, max_features='sqrt', n_estimators=400)\n",
      "0.9853906588458496\n"
     ]
    }
   ],
   "source": [
    "models = [KNeighborsRegressor(),RandomForestRegressor(random_state=50),\n",
    "         GradientBoostingRegressor(max_features='sqrt')]\n",
    "params = [{'n_neighbors':[3,5,7,9,11,13,15,17,19],'p':[1,2]},\n",
    "         {'n_estimators':[100,200,300,400],'min_samples_split':[2,4,6]},\n",
    "         {'n_estimators':[100,200,300,400],'max_depth':[2,3,4,6,8]}\n",
    "         ]\n",
    "\n",
    "best_results = ''\n",
    "best_model = ''\n",
    "best_score = 0\n",
    "metric = 'r2'\n",
    "metric_direction = 'max'\n",
    "for model,param in zip(models,params):\n",
    "    results = optimal_model_dev(features[:800],label[:800],model,param,\n",
    "                      metric,metric_direction,'rand',5,True,50)\n",
    "    if results['best_score'] > best_score:\n",
    "        best_results = results\n",
    "        best_score = results['best_score']\n",
    "        best_model = results['best_model']\n",
    "    print(str(results) + '\\n')\n",
    "\n",
    "print('Final Evaluation')\n",
    "print(best_model)\n",
    "print(scoring_function(label[800:],best_model.predict(features[800:]),\n",
    "                 metric=metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837e6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
