{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c884b687",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6620b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding\n",
    "import keras.backend as K\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3e8350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf 2.9.1\n",
      "sklearn 0.24.2\n",
      "xgboost 1.6.1\n",
      "nltk 3.7\n",
      "pd 1.3.4\n",
      "np 1.20.3\n",
      "shap 0.41.0\n",
      "mpl 3.4.3\n",
      "scipy 1.7.1\n",
      "gensim 4.2.0\n"
     ]
    }
   ],
   "source": [
    "print('tf ' + tf.__version__)\n",
    "print('sklearn ' + sklearn.__version__)\n",
    "print('xgboost ' + xgboost.__version__)\n",
    "print('nltk ' + nltk.__version__)\n",
    "print('pd ' + pd.__version__)\n",
    "print('np ' + np.__version__)\n",
    "print('shap ' + shap.__version__)\n",
    "print('mpl ' + matplotlib.__version__)\n",
    "print('scipy ' + scipy.__version__)\n",
    "print('gensim ' + gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376cb027",
   "metadata": {},
   "source": [
    "# 2. Read in Dataset + Create Train/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdecafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts in Train Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "English       2145\n",
       "Portuguese    2144\n",
       "Spanish       2141\n",
       "Other         2097\n",
       "Italian       1140\n",
       "French         985\n",
       "German         697\n",
       "Name: language label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset = pd.read_csv('Language_Detection/Train_Test_Data/train.csv')[['Lyric','language label']]\n",
    "test_dataset = pd.read_csv('Language_Detection/Train_Test_Data/test.csv')[['Lyric','language label']]\n",
    "print('Label Counts in Train Set')\n",
    "display(sample_dataset['language label'].value_counts())\n",
    "train_set = sample_dataset\n",
    "val_set = test_dataset.iloc[:1418]\n",
    "test_set = test_dataset.iloc[1418:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a66d3a",
   "metadata": {},
   "source": [
    "# 3. Resample (Oversample on Minority Classes) Training Set to Deal with Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9265cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nagaretsuita sono basho de\\nHito wa nani omou ...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Music non-stop\\nMusic non-stop\\nMusic non-stop...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyttäret tulen tekevät\\nTuvan taakse taaton sa...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tekrar geldik buraya oooo..\\nHep bereber olmay...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Itsuka kimi ga hitomi ni tomosu ai no hikari g...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15010</th>\n",
       "      <td>Les amoureux de l'an deux mille\\nCherchent Ã c...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15011</th>\n",
       "      <td>J'aime ta couleur café\\nTes cheveux café\\nTa g...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15012</th>\n",
       "      <td>Je ne suis pas une dame, je ne suis pas une da...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15013</th>\n",
       "      <td>Viens seigneur remplir cet endroit\\nAvec ta gl...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15014</th>\n",
       "      <td>Quelqu'un m'entendra-t-il\\nSi je ne dis plus u...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15015 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lyric language label\n",
       "0      Nagaretsuita sono basho de\\nHito wa nani omou ...          Other\n",
       "1      Music non-stop\\nMusic non-stop\\nMusic non-stop...          Other\n",
       "2      Tyttäret tulen tekevät\\nTuvan taakse taaton sa...          Other\n",
       "3      Tekrar geldik buraya oooo..\\nHep bereber olmay...          Other\n",
       "4      Itsuka kimi ga hitomi ni tomosu ai no hikari g...          Other\n",
       "...                                                  ...            ...\n",
       "15010  Les amoureux de l'an deux mille\\nCherchent Ã c...         French\n",
       "15011  J'aime ta couleur café\\nTes cheveux café\\nTa g...         French\n",
       "15012  Je ne suis pas une dame, je ne suis pas une da...         French\n",
       "15013  Viens seigneur remplir cet endroit\\nAvec ta gl...         French\n",
       "15014  Quelqu'un m'entendra-t-il\\nSi je ne dis plus u...         French\n",
       "\n",
       "[15015 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(50)\n",
    "max_class_counts = train_set['language label'].value_counts().iloc[0]\n",
    "resampled_train_set = pd.DataFrame()\n",
    "for lang in train_set['language label'].unique():\n",
    "    subset = train_set[train_set['language label'] == lang].copy()\n",
    "    if len(subset) == max_class_counts:\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset],ignore_index=True)\n",
    "    else:\n",
    "        added_subset = subset.iloc[random.choices(np.arange(0,len(subset)),k=max_class_counts - len(subset))]\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset,added_subset],ignore_index=True)\n",
    "        \n",
    "display(resampled_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a1c1b",
   "metadata": {},
   "source": [
    "# 4. Term Density Transformation of Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8471c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other         2145\n",
       "Spanish       2145\n",
       "German        2145\n",
       "Portuguese    2145\n",
       "Italian       2145\n",
       "English       2145\n",
       "French        2145\n",
       "Name: language label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_train_set['language label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2e10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c6ec1",
   "metadata": {},
   "source": [
    "#### Vectorize According to Terms in Non-Other Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253f8677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(preprocessor=<function preprocess_text at 0x7ff5263a1ca0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(resampled_train_set[resampled_train_set['language label'] != 'Other']['Lyric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f70df0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize Train Lyrics\n",
    "train_lyrics = vectorizer.transform(resampled_train_set['Lyric'])\n",
    "train_lyrics = pd.DataFrame(train_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "train_lyrics_token_count = train_lyrics.sum(axis=1)\n",
    "train_lyrics = train_lyrics/np.array(train_lyrics_token_count.repeat(len(train_lyrics.columns))).reshape(train_lyrics.shape)\n",
    "\n",
    "#Vectorize Val Lyrics\n",
    "val_lyrics = vectorizer.transform(val_set['Lyric'])\n",
    "val_lyrics = pd.DataFrame(val_lyrics.todense(),columns = vectorizer.get_feature_names(),index=val_set.index)\n",
    "val_lyrics_token_count = val_lyrics.sum(axis=1)\n",
    "val_lyrics = val_lyrics/np.array(val_lyrics_token_count.repeat(len(val_lyrics.columns))).reshape(val_lyrics.shape)\n",
    "\n",
    "#Vectorize Test Lyrics\n",
    "test_lyrics = vectorizer.transform(test_set['Lyric'])\n",
    "test_lyrics = pd.DataFrame(test_lyrics.todense(),columns = vectorizer.get_feature_names(),index=test_set.index)\n",
    "test_lyrics_token_count = test_lyrics.sum(axis=1)\n",
    "test_lyrics = test_lyrics/np.array(test_lyrics_token_count.repeat(len(test_lyrics.columns))).reshape(test_lyrics.shape)\n",
    "\n",
    "train_labels = resampled_train_set['language label']\n",
    "val_labels = val_set['language label']\n",
    "test_labels = test_set['language label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd634a",
   "metadata": {},
   "source": [
    "#### Convert to float 32 and drop observations that failed to featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5905370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lyrics = train_lyrics.astype('float32')\n",
    "val_lyrics = val_lyrics.astype('float32')\n",
    "test_lyrics = test_lyrics.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f97154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lyrics.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a4c41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.loc[train_lyrics.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b1a9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lyrics.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1803865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = val_labels.loc[val_lyrics.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "105e0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lyrics.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e4d2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels.loc[test_lyrics.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b153d60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "1416\n",
      "1417\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lyrics))\n",
    "print(len(val_lyrics))\n",
    "print(len(test_lyrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16613fa",
   "metadata": {},
   "source": [
    "#### Mapping to map text labels to numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e55bcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "count = 0\n",
    "for label in train_labels.unique():\n",
    "    mapping[label] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf04f9c",
   "metadata": {},
   "source": [
    "# 5. Quick Evaluation of Classical ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a907256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_model_id(xtrain,xval,xtest,ytrain,yval,ytest,estimator,param_grid,metric='accuracy'):\n",
    "    \n",
    "    #Concatenate training and validation data\n",
    "    train_val_feats = pd.concat([xtrain,xval],ignore_index=True)\n",
    "    train_val_labels = pd.concat([ytrain,yval],ignore_index=True)\n",
    "    #Instantiate Grid Search with model and param grid to ID which hyperparameter combo enables the model to generalize\n",
    "    #best on the validation set\n",
    "    grid = GridSearchCV(estimator = estimator, param_grid= param_grid,\n",
    "                        scoring=metric,cv=[(np.arange(0,len(xtrain)),np.arange(len(xtrain),len(train_val_feats)))])\n",
    "    \n",
    "    display(train_val_feats)\n",
    "    display(train_val_labels.map(mapping))\n",
    "    grid.fit(train_val_feats,train_val_labels.map(mapping))\n",
    "    \n",
    "    #Store Best Performing Model Output\n",
    "    best_estimator = grid.best_estimator_\n",
    "    best_val_score = grid.best_score_\n",
    "    \n",
    "    #Predictions on test set with optimal model\n",
    "    test_preds = best_estimator.predict(xtest)\n",
    "    #performance on test set\n",
    "    oos_score = accuracy_score(ytest.map(mapping),test_preds)\n",
    "    label_options = list(ytest.unique())\n",
    "    \n",
    "    #Confustion matrix of true for predicted values on the test set\n",
    "    confuse = pd.DataFrame(confusion_matrix(ytest.map(mapping),test_preds),index = label_options,columns = label_options)\n",
    "    \n",
    "    #return optimal model results\n",
    "    return {'best_estimator':best_estimator,\n",
    "           'best_val_score':best_val_score,\n",
    "           'best_test_score':oos_score,\n",
    "           'metric':metric,\n",
    "           'test_set_confusion_matrix':confuse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e1abd",
   "metadata": {},
   "source": [
    "#### KNN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a01269bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00015</th>\n",
       "      <th>01</th>\n",
       "      <th>012</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>...</th>\n",
       "      <th>ʿalaykum</th>\n",
       "      <th>еl</th>\n",
       "      <th>еm</th>\n",
       "      <th>еn</th>\n",
       "      <th>еnnui</th>\n",
       "      <th>еs</th>\n",
       "      <th>еven</th>\n",
       "      <th>йquateur</th>\n",
       "      <th>йternellement</th>\n",
       "      <th>оles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16411</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16412</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16413</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16415</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16416 rows × 90178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  0000  00015   01  012   02   03   04   05  ...  ʿalaykum  \\\n",
       "0      0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "1      0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "2      0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "3      0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "4      0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "...    ...  ...   ...    ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "16411  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "16412  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "16413  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "16414  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "16415  0.0  0.0   0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0   \n",
       "\n",
       "        еl   еm   еn  еnnui   еs  еven  йquateur  йternellement  оles  \n",
       "0      0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "1      0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "2      0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "3      0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "4      0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "...    ...  ...  ...    ...  ...   ...       ...            ...   ...  \n",
       "16411  0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "16412  0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "16413  0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "16414  0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "16415  0.0  0.0  0.0    0.0  0.0   0.0       0.0            0.0   0.0  \n",
       "\n",
       "[16416 rows x 90178 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "16411    4\n",
       "16412    1\n",
       "16413    0\n",
       "16414    3\n",
       "16415    3\n",
       "Name: language label, Length: 16416, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'best_estimator': KNeighborsClassifier(n_neighbors=1),\n",
       " 'best_val_score': 0.9293785310734464,\n",
       " 'best_test_score': 0.9343683839096684,\n",
       " 'metric': 'accuracy',\n",
       " 'test_set_confusion_matrix':             French  English  Italian  Portuguese  German  Other  Spanish\n",
       " French         248        6        5           2       6     21        3\n",
       " English          0      252        0           3       0      0        0\n",
       " Italian          0        0       62           0       0      2        2\n",
       " Portuguese       0        1        0         280       0      0        0\n",
       " German           3        2        4           1     135      1        0\n",
       " Other            1        1        9           0       8    237        2\n",
       " Spanish          1        2        0           0       1      6      110}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>French</th>\n",
       "      <th>English</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>German</th>\n",
       "      <th>Other</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>248</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>237</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            French  English  Italian  Portuguese  German  Other  Spanish\n",
       "French         248        6        5           2       6     21        3\n",
       "English          0      252        0           3       0      0        0\n",
       "Italian          0        0       62           0       0      2        2\n",
       "Portuguese       0        1        0         280       0      0        0\n",
       "German           3        2        4           1     135      1        0\n",
       "Other            1        1        9           0       8    237        2\n",
       "Spanish          1        2        0           0       1      6      110"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = optimal_model_id(train_lyrics,val_lyrics,test_lyrics,train_labels,val_labels,test_labels,\n",
    "                KNeighborsClassifier(),{'n_neighbors':[1,3,5,7,9]},'accuracy')\n",
    "display(test)\n",
    "display(test['test_set_confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30dd09a",
   "metadata": {},
   "source": [
    "#### XGBoost Classifier Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = optimal_model_id(train_lyrics,val_lyrics,test_lyrics,train_labels,val_labels,test_labels,\n",
    "                XGBClassifier(),{'max_depth':[2,3,4],'max_features':['auto'],'n_estimators':[10]},'accuracy')\n",
    "display(test)\n",
    "display(test['test_set_confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a6e43",
   "metadata": {},
   "source": [
    "# 6. Basic Feedforward NN w/ Keras Sequential API and Term Density Representation of Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38d94d",
   "metadata": {},
   "source": [
    "#### Input goes sequentially from one hidden layer to the next \"left to right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09acd851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.1178 - accuracy: 0.9668 - val_loss: 0.1142 - val_accuracy: 0.9746\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0219 - accuracy: 0.9931 - val_loss: 0.1444 - val_accuracy: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4d1ec2040>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define Model Architecture Sequentially\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(7,activation='softmax')\n",
    "])\n",
    "\n",
    "#Compile the model, specifying loss function, optimizer, and performance metric\n",
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer = keras.optimizers.Adam(learning_rate=0.01),\n",
    "             metrics=['accuracy'],\n",
    "             )\n",
    "\n",
    "#Fit model and validate on val set between epochs, set multiprocessing\n",
    "model.fit(x = np.array(train_lyrics),y = train_labels.map(mapping),batch_size=8,epochs=2,\n",
    "         validation_data=(np.array(val_lyrics),val_labels.map(mapping)),\n",
    "         use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b12742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(np.array(test_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fbd0556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788285109386027"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels.map(mapping),[x.argmax() for x in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9cce6a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 100)               9017900   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,028,707\n",
      "Trainable params: 9,028,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7684b",
   "metadata": {},
   "source": [
    "# 7. Word Embedding Based Models That Build Vector Representation of Input, Captures General Meaning Before Pass into Feed Forward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce16e54",
   "metadata": {},
   "source": [
    "#### Build Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "393c4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
    "\n",
    "#construct embedding matrix w/ prebuilt embedding\n",
    "vocab_dict = model.key_to_index.copy()\n",
    "embedding_matrix = np.zeros((43982,300))\n",
    "for word,index in model.key_to_index.items():\n",
    "    embedding_matrix[index] = model[word]\n",
    "\n",
    "#Construct custom embedding matrix for this task\n",
    "vocab_dict_custom = {}\n",
    "count = 0\n",
    "for word in vectorizer.get_feature_names():\n",
    "    vocab_dict_custom[word] = count\n",
    "    count = count + 1\n",
    "embedding_matrix_custom = np.random.random((len(vectorizer.get_feature_names()) + 1,300))\n",
    "embedding_matrix_custom[-1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db348b6",
   "metadata": {},
   "source": [
    "#### Map tokens in train, val, test set to row in embedding matrices for both word2vec and custom embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ebc0cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(text_data,mapping,max_size):\n",
    "    return_data = []\n",
    "    for text in text_data:\n",
    "        new_text = text.lower()\n",
    "        new_text = text.replace('\\n',' ')\n",
    "        new_text = text.replace('  ',' ')\n",
    "        new_text = new_text.split()\n",
    "        mapped_text = []\n",
    "        for token in new_text:\n",
    "            try:\n",
    "                mapped_text.append(mapping[token])\n",
    "            except:\n",
    "                mapped_text.append(len(mapping))\n",
    "        \n",
    "        if len(mapped_text) > max_size:\n",
    "            mapped_text = mapped_text[:max_size]\n",
    "        else:\n",
    "            while len(mapped_text) < max_size:\n",
    "                mapped_text.append(len(mapping))\n",
    "                \n",
    "        return_data.append(mapped_text)\n",
    "    \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3745a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_prebuilt = text_to_index(resampled_train_set['Lyric'].loc[train_lyrics.index],vocab_dict,1000)\n",
    "train_tokens_custom = text_to_index(resampled_train_set['Lyric'].loc[train_lyrics.index],vocab_dict_custom,1000)\n",
    "\n",
    "val_tokens_prebuilt = text_to_index(val_set['Lyric'].loc[val_lyrics.index],vocab_dict,1000)\n",
    "val_tokens_custom = text_to_index(val_set['Lyric'].loc[val_lyrics.index],vocab_dict_custom,1000)\n",
    "\n",
    "test_tokens_prebuilt = text_to_index(test_set['Lyric'].loc[test_lyrics.index],vocab_dict,1000)\n",
    "test_tokens_custom = text_to_index(test_set['Lyric'].loc[test_lyrics.index],vocab_dict_custom,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71758bd0",
   "metadata": {},
   "source": [
    "### Deep Averaging Network (DAN) w/ Functional Keras API and Custom Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7a137fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix_custom, \n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    dan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    dan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    dan_embeddings = dan_embedding_layer(dan_input_layer)\n",
    "    #Embeddings are averaged, forming single vector represenation of size embedding matrix\n",
    "    dan_avg_input_embeddings = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1), name='averaging')(dan_embeddings)\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = dan_avg_input_embeddings #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dense(hidden_dim, activation='relu', name='hidden_1')(dan_avg_input_embeddings)\n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dropout(dropout)(dan_hidden_out_1)\n",
    "    dan_classification = tf.keras.layers.Dense(output_layer_size, activation=output_activation, name='dan_classification')(x)\n",
    "    dan_model = tf.keras.models.Model(inputs=dan_input_layer, outputs=[dan_classification])\n",
    "    dan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics='accuracy')\n",
    "    \n",
    "    print(dan_model.summary())\n",
    "\n",
    "    return dan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1dbb09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1000)]            0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 1000, 300)        27053700  \n",
      "                                                                 \n",
      " averaging (Lambda)          (None, 300)               0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " hidden_3 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dan_classification (Dense)  (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,104,707\n",
      "Trainable params: 27,104,707\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 346s 184ms/step - loss: 0.8406 - accuracy: 0.6890 - val_loss: 0.2120 - val_accuracy: 0.9421\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 347s 185ms/step - loss: 0.1838 - accuracy: 0.9505 - val_loss: 0.1864 - val_accuracy: 0.9527\n"
     ]
    }
   ],
   "source": [
    "dan_model_sorted = create_dan_model(retrain_embeddings=True,embedding_matrix=embedding_matrix_custom,\n",
    "                                   output_layer_size=7)\n",
    "dan_sorted_history = dan_model_sorted.fit(np.array(train_tokens_custom),\n",
    "                        np.array(train_labels.map(mapping)),\n",
    "                        validation_data=(np.array(val_tokens_custom), np.array(val_labels.map(mapping))),\n",
    "                        batch_size=8,\n",
    "                        epochs=2,\n",
    "                        shuffle=True,\n",
    "                        use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a526c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9618913196894848"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels.map(mapping),[x.argmax() for x in dan_model_sorted.predict(test_tokens_custom)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef0c4bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([90179, 300])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dan_model_sorted.weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8ff30",
   "metadata": {},
   "source": [
    "### Weighted Attention Network (WAN) with Custom Embeddings, allows for computation of multiple attention based representations of input before a final attention layer learns how to balance attention vectors from prior layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5029548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix_custom,\n",
    "                     num_attention = 1,\n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the WAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    num_attention = number of parallel attention computations that learn how to balance embeddings into a single\n",
    "    vector representation, final attention layer weights prior attention based representations\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    wan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    wan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    wan_embeddings = wan_embedding_layer(wan_input_layer)\n",
    "    \n",
    "    #Create attention based single vector representations of words according to alternative query vectors\n",
    "    attention_embeddings = []\n",
    "    for num in range(num_attention):\n",
    "        #Apply Query Vector to words in embeddings, returning a max_sequence_length x 1 tensor\n",
    "        l1_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query' + str(num+1))(wan_embeddings)\n",
    "        #reshape to 1 x max_sequence_length\n",
    "        l1_reshape_query = tf.keras.layers.Reshape((1,max_sequence_length))(l1_query)\n",
    "        #Softmax over query * key (words) to obtain weights\n",
    "        l1_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                            name='attention_weights' + str(num+1))(l1_reshape_query)\n",
    "        #weight embeddings according to weights\n",
    "        l1_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((wan_embeddings,l1_weights)))\n",
    "        attention_embeddings.append(l1_attention)\n",
    "    \n",
    "    concat_attention = tf.keras.layers.Concatenate()(attention_embeddings)\n",
    "    concat_attention = tf.keras.layers.Reshape((num_attention,embedding_matrix.shape[1]))(concat_attention)\n",
    "    \n",
    "    #Apply Query Vector to attention based representations, returning a num_attention x 1 tensor\n",
    "    wan_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query')(concat_attention)\n",
    "    #reshape to 1 x num_attention\n",
    "    reshaped_query = tf.keras.layers.Reshape((1,num_attention))(wan_query)\n",
    "    #Softmax over query * key (words) to obtain weights\n",
    "    wan_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                        name='attention_weights')(reshaped_query)\n",
    "    #weight attention embeddings according to weights, learning how to balance attention based vector representations \n",
    "    #from prior layer\n",
    "    wan_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((concat_attention,wan_weights)))\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = wan_attention #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    #wan_hidden_out_1 = tf.keras.layers.Dense(hidden_dim, activation='relu', name='hidden_1')(wan_avg_input_embeddings)\n",
    "    #wan_hidden_out_1 = tf.keras.layers.Dropout(dropout)(wan_hidden_out_1)\n",
    "    wan_classification = tf.keras.layers.Dense(output_layer_size, activation=output_activation, name='wan_classification')(x)\n",
    "    wan_model = tf.keras.models.Model(inputs=wan_input_layer, outputs=[wan_classification])\n",
    "    wan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics='accuracy')\n",
    "    \n",
    "    print(wan_model.summary())\n",
    "\n",
    "    return wan_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6eb57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 1000, 300)    27053700    ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " attention_query1 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query2 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query3 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query4 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query5 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 1, 1000)      0           ['attention_query1[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 1, 1000)      0           ['attention_query2[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 1, 1000)      0           ['attention_query3[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 1, 1000)      0           ['attention_query4[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 1, 1000)      0           ['attention_query5[0][0]']       \n",
      "                                                                                                  \n",
      " attention_weights1 (Lambda)    (None, 1, 1000)      0           ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights2 (Lambda)    (None, 1, 1000)      0           ['reshape_8[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights3 (Lambda)    (None, 1, 1000)      0           ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights4 (Lambda)    (None, 1, 1000)      0           ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights5 (Lambda)    (None, 1, 1000)      0           ['reshape_11[0][0]']             \n",
      "                                                                                                  \n",
      " dot_6 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights1[0][0]']     \n",
      "                                                                                                  \n",
      " dot_7 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights2[0][0]']     \n",
      "                                                                                                  \n",
      " dot_8 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights3[0][0]']     \n",
      "                                                                                                  \n",
      " dot_9 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights4[0][0]']     \n",
      "                                                                                                  \n",
      " dot_10 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights5[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 300)          0           ['dot_6[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 300)          0           ['dot_7[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 300)          0           ['dot_8[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 300)          0           ['dot_9[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 300)          0           ['dot_10[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1500)         0           ['flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]',              \n",
      "                                                                  'flatten_8[0][0]',              \n",
      "                                                                  'flatten_9[0][0]',              \n",
      "                                                                  'flatten_10[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_12 (Reshape)           (None, 5, 300)       0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " attention_query (Dense)        (None, 5, 1)         300         ['reshape_12[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_13 (Reshape)           (None, 1, 5)         0           ['attention_query[0][0]']        \n",
      "                                                                                                  \n",
      " attention_weights (Lambda)     (None, 1, 5)         0           ['reshape_13[0][0]']             \n",
      "                                                                                                  \n",
      " dot_11 (Dot)                   (None, 300, 1)       0           ['reshape_12[0][0]',             \n",
      "                                                                  'attention_weights[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 300)          0           ['dot_11[0][0]']                 \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 100)          30100       ['flatten_11[0][0]']             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_1 (Dropout)            (None, 100)          0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 100)          10100       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_3 (Dense)               (None, 100)          10100       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100)          0           ['hidden_3[0][0]']               \n",
      "                                                                                                  \n",
      " wan_classification (Dense)     (None, 7)            707         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,106,507\n",
      "Trainable params: 27,106,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 383s 204ms/step - loss: 0.7510 - accuracy: 0.7305 - val_loss: 0.3038 - val_accuracy: 0.9266\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 384s 205ms/step - loss: 0.2302 - accuracy: 0.9486 - val_loss: 0.2760 - val_accuracy: 0.9371\n"
     ]
    }
   ],
   "source": [
    "wan_model_sorted = create_wan_model(retrain_embeddings=True,embedding_matrix=embedding_matrix_custom,\n",
    "                                   num_attention=5,output_layer_size=7)\n",
    "wan_sorted_history = wan_model_sorted.fit(np.array(train_tokens_custom),\n",
    "                        np.array(train_labels.map(mapping)),\n",
    "                        validation_data=(np.array(val_tokens_custom), np.array(val_labels.map(mapping))),\n",
    "                        batch_size=8,\n",
    "                        epochs=2,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e0b870a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9428369795342273"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels.map(mapping),[x.argmax() for x in wan_model_sorted.predict(test_tokens_custom)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2786f9",
   "metadata": {},
   "source": [
    "# 8. BERT Based Models to Develop Contextual Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
