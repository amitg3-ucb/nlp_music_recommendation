{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c884b687",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6620b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding\n",
    "import keras.backend as K\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376cb027",
   "metadata": {},
   "source": [
    "# 2. Read in Dataset + Create Train/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ebdecafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Remapping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "English        2693\n",
       "Portuguese     2073\n",
       "Spanish         148\n",
       "Italian          25\n",
       "Kinyarwanda      22\n",
       "Other            14\n",
       "German           14\n",
       "French           11\n",
       "Name: language label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Remapping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "English       2693\n",
       "Portuguese    2073\n",
       "Spanish        148\n",
       "Other           86\n",
       "Name: language label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset = pd.read_csv('Language_Detection/Train_Test_Data/train.csv')[['Lyric','language label']].iloc[:5000]\n",
    "print('Before Remapping')\n",
    "display(sample_dataset['language label'].value_counts())\n",
    "print('After Remapping')\n",
    "sample_dataset['language label'] = sample_dataset['language label'].apply(lambda x: x if x in ['English','Portuguese','Spanish'] else 'Other')\n",
    "display(sample_dataset['language label'].value_counts())\n",
    "train_set = sample_dataset.iloc[:4000]\n",
    "val_set = sample_dataset.iloc[4000:4500]\n",
    "test_set = sample_dataset.iloc[4500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a66d3a",
   "metadata": {},
   "source": [
    "# 3. Resample (Oversample on Minority Classes) Training Set to Deal with Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d9265cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uma menina me ensinou\\nQuase tudo que eu sei\\n...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foram lá fora buscar\\nComo atração singular\\nD...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sei que vou morrer,\\nNão sei o dia,\\nLevarei s...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facção central\\nO poder que eu não quero\\n\\n\\n...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ele não me esquece / Sabe o meu nome!\\nEle é o...</td>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651</th>\n",
       "      <td>La lluvia de tu corazón, desaparecerá,\\nla llu...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8652</th>\n",
       "      <td>Hoy, lo pude ver\\nmurió este amor, lo vi en tu...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>Comenzó la fiesta\\nTe enciende, te enciende\\nT...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8654</th>\n",
       "      <td>Pon tu lengua bajo la mía,\\nEl silencio dice m...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>Jenny ayer cumpliò los dieciséis\\nY con su cab...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8656 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Lyric language label\n",
       "0     Uma menina me ensinou\\nQuase tudo que eu sei\\n...     Portuguese\n",
       "1     Foram lá fora buscar\\nComo atração singular\\nD...     Portuguese\n",
       "2     Sei que vou morrer,\\nNão sei o dia,\\nLevarei s...     Portuguese\n",
       "3     Facção central\\nO poder que eu não quero\\n\\n\\n...     Portuguese\n",
       "4     Ele não me esquece / Sabe o meu nome!\\nEle é o...     Portuguese\n",
       "...                                                 ...            ...\n",
       "8651  La lluvia de tu corazón, desaparecerá,\\nla llu...        Spanish\n",
       "8652  Hoy, lo pude ver\\nmurió este amor, lo vi en tu...        Spanish\n",
       "8653  Comenzó la fiesta\\nTe enciende, te enciende\\nT...        Spanish\n",
       "8654  Pon tu lengua bajo la mía,\\nEl silencio dice m...        Spanish\n",
       "8655  Jenny ayer cumpliò los dieciséis\\nY con su cab...        Spanish\n",
       "\n",
       "[8656 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(50)\n",
    "max_class_counts = train_set['language label'].value_counts().iloc[0]\n",
    "resampled_train_set = pd.DataFrame()\n",
    "for lang in train_set['language label'].unique():\n",
    "    subset = train_set[train_set['language label'] == lang].copy()\n",
    "    if len(subset) == max_class_counts:\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset],ignore_index=True)\n",
    "    else:\n",
    "        added_subset = subset.iloc[random.choices(np.arange(0,len(subset)),k=max_class_counts - len(subset))]\n",
    "        resampled_train_set = pd.concat([resampled_train_set,subset,added_subset],ignore_index=True)\n",
    "        \n",
    "display(resampled_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a1c1b",
   "metadata": {},
   "source": [
    "# 4. Term Density Transformation of Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fe8471c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Portuguese    2164\n",
       "English       2164\n",
       "Other         2164\n",
       "Spanish       2164\n",
       "Name: language label, dtype: int64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_train_set['language label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4d2e10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f70df0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize Train Lyrics\n",
    "train_lyrics = vectorizer.fit_transform(resampled_train_set['Lyric'])\n",
    "train_lyrics = pd.DataFrame(train_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "train_lyrics_token_count = train_lyrics.sum(axis=1)\n",
    "train_lyrics = train_lyrics/np.array(train_lyrics_token_count.repeat(len(train_lyrics.columns))).reshape(train_lyrics.shape)\n",
    "\n",
    "#Vectorize Val Lyrics\n",
    "val_lyrics = vectorizer.transform(val_set['Lyric'])\n",
    "val_lyrics = pd.DataFrame(val_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "val_lyrics_token_count = val_lyrics.sum(axis=1)\n",
    "val_lyrics = val_lyrics/np.array(val_lyrics_token_count.repeat(len(val_lyrics.columns))).reshape(val_lyrics.shape)\n",
    "\n",
    "#Vectorize Test Lyrics\n",
    "test_lyrics = vectorizer.transform(test_set['Lyric'])\n",
    "test_lyrics = pd.DataFrame(test_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "test_lyrics_token_count = test_lyrics.sum(axis=1)\n",
    "test_lyrics = test_lyrics/np.array(test_lyrics_token_count.repeat(len(test_lyrics.columns))).reshape(test_lyrics.shape)\n",
    "\n",
    "train_labels = resampled_train_set['language label']\n",
    "val_labels = val_set['language label']\n",
    "test_labels = test_set['language label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "79dd7164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English       272\n",
       "Portuguese    211\n",
       "Spanish        13\n",
       "Other           4\n",
       "Name: language label, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a16d1",
   "metadata": {},
   "source": [
    "#### Mapping to map text labels to numeric labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "63f81cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "count = 0\n",
    "for label in train_labels.unique():\n",
    "    mapping[label] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf04f9c",
   "metadata": {},
   "source": [
    "# 5. Quick Evaluation of Classical ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "a907256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_model_id(xtrain,xval,xtest,ytrain,yval,ytest,estimator,param_grid,metric='accuracy'):\n",
    "    \n",
    "    #Concatenate training and validation data\n",
    "    train_val_feats = pd.concat([xtrain,xval],ignore_index=True)\n",
    "    train_val_labels = pd.concat([ytrain,yval],ignore_index=True)\n",
    "    #Instantiate Grid Search with model and param grid to ID which hyperparameter combo enables the model to generalize\n",
    "    #best on the validation set\n",
    "    grid = GridSearchCV(estimator = estimator, param_grid= param_grid,\n",
    "                        scoring=metric,cv=[(np.arange(0,len(xtrain)),np.arange(len(xtrain),len(train_val_feats)))])\n",
    "\n",
    "    grid.fit(train_val_feats,train_val_labels.map(mapping))\n",
    "    \n",
    "    #Store Best Performing Model Output\n",
    "    best_estimator = grid.best_estimator_\n",
    "    best_val_score = grid.best_score_\n",
    "    \n",
    "    #Predictions on test set with optimal model\n",
    "    test_preds = best_estimator.predict(xtest)\n",
    "    #performance on test set\n",
    "    oos_score = accuracy_score(ytest.map(mapping),test_preds)\n",
    "    label_options = list(ytest.unique())\n",
    "    \n",
    "    #Confustion matrix of true for predicted values on the test set\n",
    "    confuse = pd.DataFrame(confusion_matrix(ytest.map(mapping),test_preds),index = label_options,columns = label_options)\n",
    "    \n",
    "    #return optimal model results\n",
    "    return {'best_estimator':best_estimator,\n",
    "           'best_val_score':best_val_score,\n",
    "           'best_test_score':oos_score,\n",
    "           'metric':metric,\n",
    "           'test_set_confusion_matrix':confuse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a138a",
   "metadata": {},
   "source": [
    "#### KNN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a01269bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_estimator': KNeighborsClassifier(),\n",
       " 'best_val_score': 0.988,\n",
       " 'best_test_score': 0.996,\n",
       " 'metric': 'accuracy',\n",
       " 'test_set_confusion_matrix':             Portuguese  English  Spanish  Other\n",
       " Portuguese         211        0        0      0\n",
       " English              1      271        0      0\n",
       " Spanish              0        0        4      0\n",
       " Other                0        1        0     12}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>1</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Portuguese  English  Spanish  Other\n",
       "Portuguese         211        0        0      0\n",
       "English              1      271        0      0\n",
       "Spanish              0        0        4      0\n",
       "Other                0        1        0     12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = optimal_model_id(train_lyrics,val_lyrics,test_lyrics,train_labels,val_labels,test_labels,\n",
    "                KNeighborsClassifier(),{'n_neighbors':[1,3,5,7,9]},'accuracy')\n",
    "display(test)\n",
    "display(test['test_set_confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d87268",
   "metadata": {},
   "source": [
    "#### XGBoost Classifier Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1cf3a383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:20:47] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_features\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:21:27] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_features\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:22:23] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_features\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:23:29] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"max_features\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_estimator': XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "               colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "               early_stopping_rounds=None, enable_categorical=False,\n",
       "               eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "               importance_type=None, interaction_constraints='',\n",
       "               learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "               max_delta_step=0, max_depth=4, max_features='auto', max_leaves=0,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "               n_estimators=10, n_jobs=0, num_parallel_tree=1,\n",
       "               objective='multi:softprob', predictor='auto', random_state=0, ...),\n",
       " 'best_val_score': 0.988,\n",
       " 'best_test_score': 0.974,\n",
       " 'metric': 'accuracy',\n",
       " 'test_set_confusion_matrix':             Portuguese  English  Spanish  Other\n",
       " Portuguese         206        0        5      0\n",
       " English              0      266        5      1\n",
       " Spanish              0        0        4      0\n",
       " Other                0        1        1     11}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>English</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Portuguese</th>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0</td>\n",
       "      <td>266</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Portuguese  English  Spanish  Other\n",
       "Portuguese         206        0        5      0\n",
       "English              0      266        5      1\n",
       "Spanish              0        0        4      0\n",
       "Other                0        1        1     11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = optimal_model_id(train_lyrics,val_lyrics,test_lyrics,train_labels,val_labels,test_labels,\n",
    "                XGBClassifier(),{'max_depth':[2,3,4],'max_features':['auto'],'n_estimators':[10]},'accuracy')\n",
    "display(test)\n",
    "display(test['test_set_confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a6e43",
   "metadata": {},
   "source": [
    "# 6. Basic Feedforward NN w/ Keras Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eee563",
   "metadata": {},
   "source": [
    "#### Input goes sequentially from one hidden layer to the next \"left to right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "09acd851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1082/1082 [==============================] - 12s 10ms/step - loss: 0.0357 - accuracy: 0.9881 - val_loss: 0.0749 - val_accuracy: 0.9900\n",
      "Epoch 2/2\n",
      "1082/1082 [==============================] - 11s 10ms/step - loss: 6.4978e-05 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fcc621df0>"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define Model Architecture Sequentially\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(4,activation='softmax')\n",
    "])\n",
    "\n",
    "#Compile the model, specifying loss function, optimizer, and performance metric\n",
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer = keras.optimizers.Adam(learning_rate=0.01),\n",
    "             metrics=['accuracy'],\n",
    "             )\n",
    "\n",
    "#Fit model and validate on val set between epochs, set multiprocessing\n",
    "model.fit(x = np.array(train_lyrics),y = train_labels.map(mapping),batch_size=8,epochs=2,\n",
    "         validation_data=(np.array(val_lyrics),val_labels.map(mapping)),\n",
    "         use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "2b12742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(np.array(test_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "4a330a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels.map(mapping),[x.argmax() for x in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "8f89006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_27 (Dense)            (None, 100)               3958200   \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,968,704\n",
      "Trainable params: 3,968,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12478a",
   "metadata": {},
   "source": [
    "# 7. Word Embedding Based Models That Build Vector Representation of Input, Captures General Meaning Before Pass into Feed Forward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924713aa",
   "metadata": {},
   "source": [
    "#### Build Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "54f6b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
    "\n",
    "#construct embedding matrix w/ prebuilt embedding\n",
    "vocab_dict = model.key_to_index.copy()\n",
    "embedding_matrix = np.zeros((43982,300))\n",
    "for word,index in model.key_to_index.items():\n",
    "    embedding_matrix[index] = model[word]\n",
    "\n",
    "#Construct custom embedding matrix for this task\n",
    "vocab_dict_custom = {}\n",
    "count = 0\n",
    "for word in vectorizer.get_feature_names():\n",
    "    vocab_dict_custom[word] = count\n",
    "    count = count + 1\n",
    "embedding_matrix_custom = np.random.random((len(vectorizer.get_feature_names()) + 1,300))\n",
    "embedding_matrix_custom[-1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbeeb59",
   "metadata": {},
   "source": [
    "#### Map tokens in train, val, test set to row in embedding matrices for both word2vec and custom embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "5fcdee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(text_data,mapping,max_size):\n",
    "    return_data = []\n",
    "    for text in text_data:\n",
    "        new_text = text.lower()\n",
    "        new_text = text.replace('\\n',' ')\n",
    "        new_text = text.replace('  ',' ')\n",
    "        new_text = new_text.split()\n",
    "        mapped_text = []\n",
    "        for token in new_text:\n",
    "            try:\n",
    "                mapped_text.append(mapping[token])\n",
    "            except:\n",
    "                mapped_text.append(len(mapping))\n",
    "        \n",
    "        if len(mapped_text) > max_size:\n",
    "            mapped_text = mapped_text[:max_size]\n",
    "        else:\n",
    "            while len(mapped_text) < max_size:\n",
    "                mapped_text.append(len(mapping))\n",
    "                \n",
    "        return_data.append(mapped_text)\n",
    "    \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "49366d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_prebuilt = text_to_index(resampled_train_set['Lyric'],vocab_dict,1000)\n",
    "train_tokens_custom = text_to_index(resampled_train_set['Lyric'],vocab_dict_custom,1000)\n",
    "\n",
    "val_tokens_prebuilt = text_to_index(val_set['Lyric'],vocab_dict,1000)\n",
    "val_tokens_custom = text_to_index(val_set['Lyric'],vocab_dict_custom,1000)\n",
    "\n",
    "test_tokens_prebuilt = text_to_index(test_set['Lyric'],vocab_dict,1000)\n",
    "test_tokens_custom = text_to_index(test_set['Lyric'],vocab_dict_custom,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30288088",
   "metadata": {},
   "source": [
    "### Deep Averaging Network (DAN) w/ Functional Keras API and Custom Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "13aac139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix_custom, \n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    dan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    dan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    dan_embeddings = dan_embedding_layer(dan_input_layer)\n",
    "    #Embeddings are averaged, forming single vector represenation of size embedding matrix\n",
    "    dan_avg_input_embeddings = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1), name='averaging')(dan_embeddings)\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = dan_avg_input_embeddings #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dense(hidden_dim, activation='relu', name='hidden_1')(dan_avg_input_embeddings)\n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dropout(dropout)(dan_hidden_out_1)\n",
    "    dan_classification = tf.keras.layers.Dense(output_layer_size, activation='softmax', name='dan_classification')(x)\n",
    "    dan_model = tf.keras.models.Model(inputs=dan_input_layer, outputs=[dan_classification])\n",
    "    dan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics='accuracy')\n",
    "    \n",
    "    print(dan_model.summary())\n",
    "\n",
    "    return dan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "e94297c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1000)]            0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 1000, 300)        11874600  \n",
      "                                                                 \n",
      " averaging (Lambda)          (None, 300)               0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " hidden_3 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dan_classification (Dense)  (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,925,304\n",
      "Trainable params: 11,925,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1082/1082 [==============================] - 92s 85ms/step - loss: 0.5143 - accuracy: 0.7750 - val_loss: 0.0655 - val_accuracy: 0.9780\n",
      "Epoch 2/2\n",
      "1082/1082 [==============================] - 90s 83ms/step - loss: 0.0479 - accuracy: 0.9828 - val_loss: 0.0683 - val_accuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "dan_model_sorted = create_dan_model(retrain_embeddings=True,embedding_matrix=embedding_matrix_custom)\n",
    "dan_sorted_history = dan_model_sorted.fit(np.array(train_tokens_custom),\n",
    "                        np.array(train_labels.map(mapping)),\n",
    "                        validation_data=(np.array(val_tokens_custom), np.array(val_labels.map(mapping))),\n",
    "                        batch_size=8,\n",
    "                        epochs=2,\n",
    "                        shuffle=True,\n",
    "                        use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "1d0d8885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels.map(mapping),[x.argmax() for x in dan_model_sorted.predict(test_tokens_custom)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "c6c74864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([39582, 300])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dan_model_sorted.weights[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6dcc0c",
   "metadata": {},
   "source": [
    "### Weighted Attention Network (WAN) with Custom Embeddings, allows for computation of multiple attention based representations of input before a final attention layer learns how to balance attention vectors from prior layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "394003b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix_custom,\n",
    "                     num_attention = 1,\n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the WAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    num_attention = number of parallel attention computations that learn how to balance embeddings into a single\n",
    "    vector representation, final attention layer weights prior attention based representations\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    wan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    wan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    wan_embeddings = wan_embedding_layer(wan_input_layer)\n",
    "    \n",
    "    #Create attention based single vector representations of words according to alternative query vectors\n",
    "    attention_embeddings = []\n",
    "    for num in range(num_attention):\n",
    "        #Apply Query Vector to words in embeddings, returning a max_sequence_length x 1 tensor\n",
    "        l1_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query' + str(num+1))(wan_embeddings)\n",
    "        #reshape to 1 x max_sequence_length\n",
    "        l1_reshape_query = tf.keras.layers.Reshape((1,max_sequence_length))(l1_query)\n",
    "        #Softmax over query * key (words) to obtain weights\n",
    "        l1_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                            name='attention_weights' + str(num+1))(l1_reshape_query)\n",
    "        #weight embeddings according to weights\n",
    "        l1_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((wan_embeddings,l1_weights)))\n",
    "        attention_embeddings.append(l1_attention)\n",
    "    \n",
    "    concat_attention = tf.keras.layers.Concatenate()(attention_embeddings)\n",
    "    concat_attention = tf.keras.layers.Reshape((num_attention,embedding_matrix.shape[1]))(concat_attention)\n",
    "    \n",
    "    #Apply Query Vector to attention based representations, returning a num_attention x 1 tensor\n",
    "    wan_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query')(concat_attention)\n",
    "    #reshape to 1 x num_attention\n",
    "    reshaped_query = tf.keras.layers.Reshape((1,num_attention))(wan_query)\n",
    "    #Softmax over query * key (words) to obtain weights\n",
    "    wan_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                        name='attention_weights')(reshaped_query)\n",
    "    #weight attention embeddings according to weights, learning how to balance attention based vector representations \n",
    "    #from prior layer\n",
    "    wan_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((concat_attention,wan_weights)))\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = wan_attention #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    #wan_hidden_out_1 = tf.keras.layers.Dense(hidden_dim, activation='relu', name='hidden_1')(wan_avg_input_embeddings)\n",
    "    #wan_hidden_out_1 = tf.keras.layers.Dropout(dropout)(wan_hidden_out_1)\n",
    "    wan_classification = tf.keras.layers.Dense(output_layer_size, activation='softmax', name='wan_classification')(x)\n",
    "    wan_model = tf.keras.models.Model(inputs=wan_input_layer, outputs=[wan_classification])\n",
    "    wan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics='accuracy')\n",
    "    \n",
    "    print(wan_model.summary())\n",
    "\n",
    "    return wan_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "c1dad084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_64\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 1000, 300)    11874600    ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " attention_query1 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query2 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query3 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query4 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query5 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_191 (Reshape)          (None, 1, 1000)      0           ['attention_query1[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_192 (Reshape)          (None, 1, 1000)      0           ['attention_query2[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_193 (Reshape)          (None, 1, 1000)      0           ['attention_query3[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_194 (Reshape)          (None, 1, 1000)      0           ['attention_query4[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_195 (Reshape)          (None, 1, 1000)      0           ['attention_query5[0][0]']       \n",
      "                                                                                                  \n",
      " attention_weights1 (Lambda)    (None, 1, 1000)      0           ['reshape_191[0][0]']            \n",
      "                                                                                                  \n",
      " attention_weights2 (Lambda)    (None, 1, 1000)      0           ['reshape_192[0][0]']            \n",
      "                                                                                                  \n",
      " attention_weights3 (Lambda)    (None, 1, 1000)      0           ['reshape_193[0][0]']            \n",
      "                                                                                                  \n",
      " attention_weights4 (Lambda)    (None, 1, 1000)      0           ['reshape_194[0][0]']            \n",
      "                                                                                                  \n",
      " attention_weights5 (Lambda)    (None, 1, 1000)      0           ['reshape_195[0][0]']            \n",
      "                                                                                                  \n",
      " dot_169 (Dot)                  (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights1[0][0]']     \n",
      "                                                                                                  \n",
      " dot_170 (Dot)                  (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights2[0][0]']     \n",
      "                                                                                                  \n",
      " dot_171 (Dot)                  (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights3[0][0]']     \n",
      "                                                                                                  \n",
      " dot_172 (Dot)                  (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights4[0][0]']     \n",
      "                                                                                                  \n",
      " dot_173 (Dot)                  (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights5[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_140 (Flatten)          (None, 300)          0           ['dot_169[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_141 (Flatten)          (None, 300)          0           ['dot_170[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_142 (Flatten)          (None, 300)          0           ['dot_171[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_143 (Flatten)          (None, 300)          0           ['dot_172[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_144 (Flatten)          (None, 300)          0           ['dot_173[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 1500)         0           ['flatten_140[0][0]',            \n",
      "                                                                  'flatten_141[0][0]',            \n",
      "                                                                  'flatten_142[0][0]',            \n",
      "                                                                  'flatten_143[0][0]',            \n",
      "                                                                  'flatten_144[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_196 (Reshape)          (None, 5, 300)       0           ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " attention_query (Dense)        (None, 5, 1)         300         ['reshape_196[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_197 (Reshape)          (None, 1, 5)         0           ['attention_query[0][0]']        \n",
      "                                                                                                  \n",
      " attention_weights (Lambda)     (None, 1, 5)         0           ['reshape_197[0][0]']            \n",
      "                                                                                                  \n",
      " dot_174 (Dot)                  (None, 300, 1)       0           ['reshape_196[0][0]',            \n",
      "                                                                  'attention_weights[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_145 (Flatten)          (None, 300)          0           ['dot_174[0][0]']                \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 100)          30100       ['flatten_145[0][0]']            \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_1 (Dropout)            (None, 100)          0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 100)          10100       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_3 (Dense)               (None, 100)          10100       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100)          0           ['hidden_3[0][0]']               \n",
      "                                                                                                  \n",
      " wan_classification (Dense)     (None, 4)            404         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,927,104\n",
      "Trainable params: 11,927,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "1082/1082 [==============================] - 114s 104ms/step - loss: 0.4250 - accuracy: 0.8377 - val_loss: 0.1352 - val_accuracy: 0.9800\n",
      "Epoch 2/2\n",
      "1082/1082 [==============================] - 113s 104ms/step - loss: 0.0633 - accuracy: 0.9827 - val_loss: 0.1380 - val_accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "wan_model_sorted = create_wan_model(retrain_embeddings=True,embedding_matrix=embedding_matrix_custom,\n",
    "                                   num_attention=5)\n",
    "wan_sorted_history = wan_model_sorted.fit(np.array(train_tokens_custom),\n",
    "                        np.array(train_labels.map(mapping)),\n",
    "                        validation_data=(np.array(val_tokens_custom), np.array(val_labels.map(mapping))),\n",
    "                        batch_size=8,\n",
    "                        epochs=2,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "fdfec04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels.map(mapping),[x.argmax() for x in wan_model_sorted.predict(test_tokens_custom)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05286c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
