{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import unicodedata\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "#nltk.download('word2vec_sample')\n",
    "from nltk.data import find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train and test data\n",
    "df_test = pkl.load(open('Train_Test_Data/genre_sub_genre_test.pkl', 'rb'))\n",
    "df_train = pkl.load(open('Train_Test_Data/genre_sub_genre_train.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n",
      "2875\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', text) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_versions(text):\n",
    "    text = text.split()\n",
    "    if '-'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "foo = ['a', 1]\n",
    "print('a' in foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Artist Name'] = df_train['Artist Name'].map(lambda x: strip_accents(x))\n",
    "df_train['Track Name'] = df_train['Track Name'].map(lambda x: strip_accents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>Sub-Genre: southern hip hop</th>\n",
       "      <th>Sub-Genre: nu metal</th>\n",
       "      <th>Sub-Genre: israeli mediterranean</th>\n",
       "      <th>Sub-Genre: thrash metal</th>\n",
       "      <th>Sub-Genre: pop rock</th>\n",
       "      <th>Sub-Genre: chicago blues</th>\n",
       "      <th>Sub-Genre: indie pop</th>\n",
       "      <th>Sub-Genre: classic rock</th>\n",
       "      <th>Sub-Genre: hardcore hip hop</th>\n",
       "      <th>normalized_audio_feature_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16303</th>\n",
       "      <td>Escape the Fate</td>\n",
       "      <td>Unbreakable</td>\n",
       "      <td>47</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.740</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.00546</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.08667260380184501, 0.5543217870346929, 0.1220273049840551, -0.674109859950688, -0.41543317804459345, -0.440099973618899, 0.27353173919337254, -0.04503025561781331, -0.63357693290532]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>COUCOU CHLOE</td>\n",
       "      <td>NOBODY</td>\n",
       "      <td>46</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.787</td>\n",
       "      <td>11</td>\n",
       "      <td>-7.044</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.09410</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.769134455639869, 0.3955264896487572, 0.02601600085691811, -0.3318981833522548, 0.5337352175503375, -0.4389348974290065, 0.19657635700507578, -0.45322352624784257, -1.4860406055792426]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>beabadoobee</td>\n",
       "      <td>Worth It</td>\n",
       "      <td>51</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.751</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.16450891723207395, 0.22229525613682674, 0.5907140659204748, -0.6469305932172383, -0.6667172298266449, -0.4406102259648373, -0.48747148466867257, -0.7234102089486819, -0.4817594483694913]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7945</th>\n",
       "      <td>Andrew Broder</td>\n",
       "      <td>Bloodrush</td>\n",
       "      <td>39</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.613</td>\n",
       "      <td>11</td>\n",
       "      <td>-10.160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.07980</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8650357381041337, -0.4417578056589061, -0.9580998664462383, -0.3871060689045742, 2.007173521181458, -0.4419794030931051, -0.8551471995683124, 0.19473671280702892, -0.3577680344419983]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15504</th>\n",
       "      <td>Greta Van Fleet</td>\n",
       "      <td>Age Of Man</td>\n",
       "      <td>56</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.601</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.486</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.03770</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.5300304949145832, -0.49950155016288295, 0.5180739345084961, -0.5496411725236405, -0.6267402215885913, 2.597691620027714, -1.056086253059976, 0.42505256733097685, 1.7496968265126795]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12579</th>\n",
       "      <td>ABSOLUTE.</td>\n",
       "      <td>Sage comme une image - Good as Gold</td>\n",
       "      <td>44</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.873</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.622</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0.00402</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.457789201918954, 0.8093566585939238, -0.4723584659609191, -0.6796692554188936, 2.7838696812350716, -0.440878108446455, 0.7609158263859181, 0.021398541370193903, 0.06298290919938167]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16468</th>\n",
       "      <td>Pepper</td>\n",
       "      <td>Warning (feat. Stick Figure)</td>\n",
       "      <td>51</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.527</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.01920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.511675880447574, -0.8555879746040728, -0.02041051791508581, -0.621063961524893, 0.3738271845981228, 0.05918619898462035, 0.11107037679585718, -1.6375937603588562, 0.011027938203398235]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>Kelvyn Boy</td>\n",
       "      <td>Tele</td>\n",
       "      <td>49</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.095</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.36200</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.793084090541478, 0.18379942646750885, 0.009908841282957543, 0.7023810152118274, -0.055640103902110626, -0.4366089971521044, 0.9062759927415893, -0.8241204656775679, -0.4788146847200311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15805</th>\n",
       "      <td>Sheryl Crow</td>\n",
       "      <td>Home</td>\n",
       "      <td>38</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0</td>\n",
       "      <td>-13.920</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.24500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.3803452767795279, -1.4234014622265112, -2.145608101702935, 0.250680133420123, -0.6564374277082883, 0.6247158823995809, -0.8252201064950858, 1.4164032781183464, 0.7708677674537697]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>Hedningarna</td>\n",
       "      <td>Raven</td>\n",
       "      <td>46</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.564</td>\n",
       "      <td>2</td>\n",
       "      <td>-11.030</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2543200481131073, -0.6775447623834782, -1.2328690591785056, -0.646544524087502, -0.48624959263771705, 3.1419607890285777, -0.38058900940714935, 0.7742356572607976, 0.7973317227176414]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16500 rows Ã— 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Artist Name                           Track Name  Popularity  \\\n",
       "16303  Escape the Fate                          Unbreakable          47   \n",
       "8721      COUCOU CHLOE                               NOBODY          46   \n",
       "11930      beabadoobee                             Worth It          51   \n",
       "7945     Andrew Broder                            Bloodrush          39   \n",
       "15504  Greta Van Fleet                           Age Of Man          56   \n",
       "...                ...                                  ...         ...   \n",
       "12579        ABSOLUTE.  Sage comme une image - Good as Gold          44   \n",
       "16468           Pepper         Warning (feat. Stick Figure)          51   \n",
       "5125        Kelvyn Boy                                 Tele          49   \n",
       "15805      Sheryl Crow                                 Home          38   \n",
       "2952       Hedningarna                                Raven          46   \n",
       "\n",
       "       danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "16303         0.563   0.820    1    -6.740     0       0.0489       0.00546   \n",
       "8721          0.844   0.787   11    -7.044     1       0.1320       0.09410   \n",
       "11930         0.576   0.751    2    -5.256     1       0.0269       0.01250   \n",
       "7945          0.693   0.613   11   -10.160     0       0.2610       0.07980   \n",
       "15504         0.460   0.601    6    -5.486     0       0.0304       0.03770   \n",
       "...             ...     ...  ...       ...   ...          ...           ...   \n",
       "12579         0.792   0.873    6    -8.622     0       0.3290       0.00402   \n",
       "16468         0.801   0.527    1    -7.191     0       0.1180       0.01920   \n",
       "5125          0.848   0.743    1    -7.095     0       0.0804       0.36200   \n",
       "15805         0.485   0.409    0   -13.920     1       0.0278       0.24500   \n",
       "2952          0.591   0.564    2   -11.030     1       0.0427       0.01260   \n",
       "\n",
       "       ...  Sub-Genre: southern hip hop  Sub-Genre: nu metal  \\\n",
       "16303  ...                            0                    0   \n",
       "8721   ...                            0                    0   \n",
       "11930  ...                            0                    0   \n",
       "7945   ...                            0                    0   \n",
       "15504  ...                            0                    0   \n",
       "...    ...                          ...                  ...   \n",
       "12579  ...                            0                    0   \n",
       "16468  ...                            0                    0   \n",
       "5125   ...                            0                    0   \n",
       "15805  ...                            0                    0   \n",
       "2952   ...                            0                    0   \n",
       "\n",
       "       Sub-Genre: israeli mediterranean  Sub-Genre: thrash metal  \\\n",
       "16303                                 0                        0   \n",
       "8721                                  0                        0   \n",
       "11930                                 0                        0   \n",
       "7945                                  0                        0   \n",
       "15504                                 0                        0   \n",
       "...                                 ...                      ...   \n",
       "12579                                 0                        0   \n",
       "16468                                 0                        0   \n",
       "5125                                  0                        0   \n",
       "15805                                 0                        0   \n",
       "2952                                  0                        0   \n",
       "\n",
       "      Sub-Genre: pop rock Sub-Genre: chicago blues Sub-Genre: indie pop  \\\n",
       "16303                   0                        0                    0   \n",
       "8721                    0                        0                    0   \n",
       "11930                   0                        0                    1   \n",
       "7945                    0                        0                    0   \n",
       "15504                   0                        0                    0   \n",
       "...                   ...                      ...                  ...   \n",
       "12579                   0                        0                    0   \n",
       "16468                   0                        0                    0   \n",
       "5125                    0                        0                    0   \n",
       "15805                   1                        0                    0   \n",
       "2952                    0                        0                    0   \n",
       "\n",
       "      Sub-Genre: classic rock  Sub-Genre: hardcore hip hop  \\\n",
       "16303                       0                            0   \n",
       "8721                        0                            0   \n",
       "11930                       0                            0   \n",
       "7945                        0                            0   \n",
       "15504                       0                            0   \n",
       "...                       ...                          ...   \n",
       "12579                       0                            0   \n",
       "16468                       0                            0   \n",
       "5125                        0                            0   \n",
       "15805                       0                            0   \n",
       "2952                        0                            0   \n",
       "\n",
       "                                                                                                                                                                      normalized_audio_feature_array  \n",
       "16303      [0.08667260380184501, 0.5543217870346929, 0.1220273049840551, -0.674109859950688, -0.41543317804459345, -0.440099973618899, 0.27353173919337254, -0.04503025561781331, -0.63357693290532]  \n",
       "8721      [1.769134455639869, 0.3955264896487572, 0.02601600085691811, -0.3318981833522548, 0.5337352175503375, -0.4389348974290065, 0.19657635700507578, -0.45322352624784257, -1.4860406055792426]  \n",
       "11930  [0.16450891723207395, 0.22229525613682674, 0.5907140659204748, -0.6469305932172383, -0.6667172298266449, -0.4406102259648373, -0.48747148466867257, -0.7234102089486819, -0.4817594483694913]  \n",
       "7945      [0.8650357381041337, -0.4417578056589061, -0.9580998664462383, -0.3871060689045742, 2.007173521181458, -0.4419794030931051, -0.8551471995683124, 0.19473671280702892, -0.3577680344419983]  \n",
       "15504      [-0.5300304949145832, -0.49950155016288295, 0.5180739345084961, -0.5496411725236405, -0.6267402215885913, 2.597691620027714, -1.056086253059976, 0.42505256733097685, 1.7496968265126795]  \n",
       "...                                                                                                                                                                                              ...  \n",
       "12579       [1.457789201918954, 0.8093566585939238, -0.4723584659609191, -0.6796692554188936, 2.7838696812350716, -0.440878108446455, 0.7609158263859181, 0.021398541370193903, 0.06298290919938167]  \n",
       "16468    [1.511675880447574, -0.8555879746040728, -0.02041051791508581, -0.621063961524893, 0.3738271845981228, 0.05918619898462035, 0.11107037679585718, -1.6375937603588562, 0.011027938203398235]  \n",
       "5125    [1.793084090541478, 0.18379942646750885, 0.009908841282957543, 0.7023810152118274, -0.055640103902110626, -0.4366089971521044, 0.9062759927415893, -0.8241204656775679, -0.4788146847200311]  \n",
       "15805        [-0.3803452767795279, -1.4234014622265112, -2.145608101702935, 0.250680133420123, -0.6564374277082883, 0.6247158823995809, -0.8252201064950858, 1.4164032781183464, 0.7708677674537697]  \n",
       "2952      [0.2543200481131073, -0.6775447623834782, -1.2328690591785056, -0.646544524087502, -0.48624959263771705, 3.1419607890285777, -0.38058900940714935, 0.7742356572607976, 0.7973317227176414]  \n",
       "\n",
       "[16500 rows x 73 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a column that is all of the normalized audio features we want to use\n",
    "df_train_audio_normalized = df_train[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].copy()\n",
    "df_train_audio_normalized = (df_train_audio_normalized-df_train_audio_normalized.mean())/df_train_audio_normalized.std()\n",
    "\n",
    "df_test_audio_normalized = df_test[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].copy()\n",
    "df_test_audio_normalized = (df_test_audio_normalized-df_test_audio_normalized.mean())/df_test_audio_normalized.std()\n",
    "\n",
    "#print(df_train_audio_normalized)\n",
    "\n",
    "#df_train['normalized_audio_feature_array'] = df_train_audio_normalized.to_numpy()\n",
    "#df_test['normalized_audio_feature_array'] = df_test_audio_normalized.to_numpy()\n",
    "\n",
    "df_train['normalized_audio_feature_array'] = df_train_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist()\n",
    "df_train['normalized_audio_feature_array'] = df_train['normalized_audio_feature_array'].apply(lambda x: np.array(x))\n",
    "\n",
    "\n",
    "df_test['normalized_audio_feature_array'] = df_test_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist()\n",
    "df_test['normalized_audio_feature_array'] = df_test['normalized_audio_feature_array'].apply(lambda x: np.array(x))\n",
    "\n",
    "#display(df_train['normalized_audio_feature_array'])\n",
    "#df_test['normalized_audio_feature_array'] = df_test_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].to_numpy()\n",
    "\n",
    "#df_train['normalized_audio_feature_array'] = np.array(df_train_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values)\n",
    "#df_test['normalized_audio_feature_array'] = np.array(df_test_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values)\n",
    "\n",
    "#df_train['normalized_audio_feature_array'] = df_train_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist()\n",
    "#df_test['normalized_audio_feature_array'] = df_test_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist()\n",
    "\n",
    "#df_train['normalized_audio_feature_array'] = df_train['normalized_audio_feature_array'].apply(lambda x: np.array(x, dtype=np.float64))\n",
    "#df_test['normalized_audio_feature_array'] = df_test['normalized_audio_feature_array'].apply(lambda x: np.array(x, dtype=np.float64))\n",
    "\n",
    "#df_train['normalized_audio_feature_array'] = np.array(df_train['normalized_audio_feature_array'])\n",
    "#df_test['normalized_audio_feature_array'] = np.array(df_test['normalized_audio_feature_array'])\n",
    "\n",
    "#df_train['normalized_audio_feature_array'] = np.array(df_train_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist())\n",
    "#df_test['normalized_audio_feature_array'] = np.array(df_test_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.normalized_audio_feature_array.apply(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['normalized_audio_feature_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word2vec model\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43981\n"
     ]
    }
   ],
   "source": [
    "# how big does our embedding matrix need to be\n",
    "print(len(model.key_to_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct embedding matrix w/ prebuilt embedding\n",
    "vocab_dict = model.key_to_index.copy()\n",
    "embedding_matrix = np.zeros((43982,300))\n",
    "for word,index in model.key_to_index.items():\n",
    "    embedding_matrix[index] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bringing in lyric tokenizer function\n",
    "def text_to_index(text_data,mapping,max_size):\n",
    "    return_data = []\n",
    "    for text in text_data:\n",
    "        new_text = text.lower()\n",
    "        new_text = text.replace('\\n',' ')\n",
    "        new_text = text.replace('  ',' ')\n",
    "        new_text = new_text.split()\n",
    "        mapped_text = []\n",
    "        for token in new_text:\n",
    "            try:\n",
    "                mapped_text.append(mapping[token])\n",
    "            except:\n",
    "                mapped_text.append(len(mapping))\n",
    "        \n",
    "        if len(mapped_text) > max_size:\n",
    "            mapped_text = mapped_text[:max_size]\n",
    "        else:\n",
    "            while len(mapped_text) < max_size:\n",
    "                mapped_text.append(len(mapping))\n",
    "                \n",
    "        return_data.append(mapped_text)\n",
    "    \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize train/test lyrics - \"get X\"\n",
    "train_tokens_prebuilt = text_to_index(df_train['Lyrics'],vocab_dict,1000)\n",
    "test_tokens_prebuilt = text_to_index(df_test['Lyrics'],vocab_dict,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels \"Y\"\n",
    "train_labels = df_train['Major Genre']\n",
    "test_labels = df_test['Major Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapper so we can use numeric labels in our networks\n",
    "mapping = {}\n",
    "count = 0\n",
    "for label in train_labels.unique():\n",
    "    mapping[label] = count\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to keep the functions consistent across notebooks, so defining this so i can use the DAN and WAN models as-is\n",
    "embedding_matrix_custom = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix_custom, \n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the DAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    dan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    dan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    dan_embeddings = dan_embedding_layer(dan_input_layer)\n",
    "    #Embeddings are averaged, forming single vector represenation of size embedding matrix\n",
    "    dan_avg_input_embeddings = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1), name='averaging')(dan_embeddings)\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = dan_avg_input_embeddings #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dense(hidden_dim, activation='relu', name='hidden_1')(dan_avg_input_embeddings)\n",
    "    #dan_hidden_out_1 = tf.keras.layers.Dropout(dropout)(dan_hidden_out_1)\n",
    "    dan_classification = tf.keras.layers.Dense(output_layer_size, activation='softmax', name='dan_classification')(x)\n",
    "    dan_model = tf.keras.models.Model(inputs=dan_input_layer, outputs=[dan_classification])\n",
    "    dan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics='accuracy')\n",
    "    \n",
    "    print(dan_model.summary())\n",
    "\n",
    "    return dan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_model_sorted = create_dan_model(embedding_matrix = embedding_matrix, output_layer_size = 7)\n",
    "dan_sorted_history = dan_model_sorted.fit(np.array(train_tokens_prebuilt),\n",
    "                        np.array(train_labels.map(mapping)),\n",
    "                        validation_data=(np.array(test_tokens_prebuilt), np.array(test_labels.map(mapping))),\n",
    "                        batch_size=8,\n",
    "                        epochs=10,\n",
    "                        shuffle=True,\n",
    "                        use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40% validation accuracy after 10 epochs - so not very good!  but the model is learning a little, at least"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if a WAN will perform any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wan_model(retrain_embeddings=False, \n",
    "                     max_sequence_length=1000,\n",
    "                     embedding_matrix=embedding_matrix_custom,\n",
    "                     num_attention = 1,\n",
    "                     hidden_dim=[100,100,100],\n",
    "                     dropout_rate=0.3,\n",
    "                     hidden_layer_activation = 'relu',\n",
    "                     output_layer_size = 4,\n",
    "                     output_activation = 'softmax',\n",
    "                     learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Construct the WAN model including the compilation and return it. Parametrize it using the arguments.\n",
    "    retrain_embeddings: bool, indicates whether embeddings are retrainable\n",
    "    max_sequence_length: Number of token IDs to expect in a given input\n",
    "    embedding_matrix: initialize embedding layer with embedding matrix, specifying weights\n",
    "    num_attention = number of parallel attention computations that learn how to balance embeddings into a single\n",
    "    vector representation, final attention layer weights prior attention based representations\n",
    "    hidden_dim = number of neurons in hidden layers\n",
    "    dropout = dropout rate\n",
    "    output_layer_size = # of neurons in output layer corresponding to # of classes, each neuron predicts P(class K | x)\n",
    "    output_activation = activation function for output layer\n",
    "    learning_rate = learning rate for gradient descent for finding model params to optimize loss\n",
    "    \"\"\"\n",
    "    \n",
    "    #Specify Embedding Layer, including shape, intialize with weights, expected input length, and whether it is trainable\n",
    "    wan_embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_sequence_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                   name = 'embedding_layer')\n",
    "    \n",
    "    \n",
    "    #Input Layer, sequence of max_sequence_length tokens\n",
    "    wan_input_layer = tf.keras.layers.Input(shape=(max_sequence_length,), dtype='int64',name='input')\n",
    "    #Inputs go into embedding layer, form max_sequence_length x embedding dim matrix\n",
    "    wan_embeddings = wan_embedding_layer(wan_input_layer)\n",
    "    \n",
    "    #Create attention based single vector representations of words according to alternative query vectors\n",
    "    attention_embeddings = []\n",
    "    for num in range(num_attention):\n",
    "        #Apply Query Vector to words in embeddings, returning a max_sequence_length x 1 tensor\n",
    "        l1_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query' + str(num+1))(wan_embeddings)\n",
    "        #reshape to 1 x max_sequence_length\n",
    "        l1_reshape_query = tf.keras.layers.Reshape((1,max_sequence_length))(l1_query)\n",
    "        #Softmax over query * key (words) to obtain weights\n",
    "        l1_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                            name='attention_weights' + str(num+1))(l1_reshape_query)\n",
    "        #weight embeddings according to weights\n",
    "        l1_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((wan_embeddings,l1_weights)))\n",
    "        attention_embeddings.append(l1_attention)\n",
    "    \n",
    "    concat_attention = tf.keras.layers.Concatenate()(attention_embeddings)\n",
    "    concat_attention = tf.keras.layers.Reshape((num_attention,embedding_matrix.shape[1]))(concat_attention)\n",
    "    \n",
    "    #Apply Query Vector to attention based representations, returning a num_attention x 1 tensor\n",
    "    wan_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query')(concat_attention)\n",
    "    #reshape to 1 x num_attention\n",
    "    reshaped_query = tf.keras.layers.Reshape((1,num_attention))(wan_query)\n",
    "    #Softmax over query * key (words) to obtain weights\n",
    "    wan_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                        name='attention_weights')(reshaped_query)\n",
    "    #weight attention embeddings according to weights, learning how to balance attention based vector representations \n",
    "    #from prior layer\n",
    "    wan_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((concat_attention,wan_weights)))\n",
    "    \n",
    "    #input into hidden layers\n",
    "    x = wan_attention #hidden layer initial input\n",
    "    count = 1\n",
    "    for layer in hidden_dim:\n",
    "        hidden = tf.keras.layers.Dense(layer,activation = hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        dropout = tf.keras.layers.Dropout(dropout_rate,name='dropout_' + str(count))(hidden)\n",
    "        count = count + 1\n",
    "        x = dropout\n",
    "        \n",
    "    #wan_hidden_out_1 = tf.keras.layers.Dense(hidden_dim, activation='relu', name='hidden_1')(wan_avg_input_embeddings)\n",
    "    #wan_hidden_out_1 = tf.keras.layers.Dropout(dropout)(wan_hidden_out_1)\n",
    "    wan_classification = tf.keras.layers.Dense(output_layer_size, activation='softmax', name='wan_classification')(x)\n",
    "    wan_model = tf.keras.models.Model(inputs=wan_input_layer, outputs=[wan_classification])\n",
    "    wan_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
    "                                                beta_1=0.9,\n",
    "                                                beta_2=0.999,\n",
    "                                                epsilon=1e-07,\n",
    "                                                amsgrad=False,\n",
    "                                                name='Adam'),\n",
    "                 metrics='accuracy')\n",
    "    \n",
    "    print(wan_model.summary())\n",
    "\n",
    "    return wan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wan_model_sorted = create_wan_model(embedding_matrix=embedding_matrix, output_layer_size = 7,\n",
    "                                   num_attention=1)\n",
    "wan_sorted_history = wan_model_sorted.fit(np.array(train_tokens_prebuilt),\n",
    "                        np.array(train_labels.map(mapping)),\n",
    "                        validation_data=(np.array(test_tokens_prebuilt), np.array(test_labels.map(mapping))),\n",
    "                        batch_size=8,\n",
    "                        epochs=10,\n",
    "                        shuffle=True,\n",
    "                        use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41%...so not much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of those models used prebuilt embeddings, what happens if we use custom ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vectorizer with preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT RUN BELOW CELL - it will crash the kernel.  need to figure out a workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize Train Lyrics\n",
    "#train_lyrics = vectorizer.fit_transform(df_train['Lyrics'])\n",
    "#train_lyrics = pd.DataFrame(train_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "#train_lyrics_token_count = train_lyrics.sum(axis=1)\n",
    "#train_lyrics = train_lyrics/np.array(train_lyrics_token_count.repeat(len(train_lyrics.columns))).reshape(train_lyrics.shape)\n",
    "\n",
    "#Vectorize Test Lyrics\n",
    "#test_lyrics = vectorizer.transform(df_test['Lyrics'])\n",
    "#test_lyrics = pd.DataFrame(test_lyrics.todense(),columns = vectorizer.get_feature_names())\n",
    "#test_lyrics_token_count = test_lyrics.sum(axis=1)\n",
    "#test_lyrics = test_lyrics/np.array(test_lyrics_token_count.repeat(len(test_lyrics.columns))).reshape(test_lyrics.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we can get vectorizer working, use this to make custom embedding matrix\n",
    "\n",
    "#vocab_dict_custom = {}\n",
    "#count = 0\n",
    "#for word in vectorizer.get_feature_names():\n",
    "#    vocab_dict_custom[word] = count\n",
    "#    count = count + 1\n",
    "#embedding_matrix_custom = np.random.random((len(vectorizer.get_feature_names()) + 1,300))\n",
    "#embedding_matrix_custom[-1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets look at the audio features we have too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 648/2063 [========>.....................] - ETA: 7s - loss: 1.5232 - accuracy: 0.4242"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-925bdcae066c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m              )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model.fit(x = df_train_audio_normalized,y = train_labels.map(mapping),batch_size=8,epochs=10,\n\u001b[0m\u001b[1;32m     24\u001b[0m          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_test_audio_normalized\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m          use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m           \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_load_initial_step_from_ckpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1402\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[1;32m   1403\u001b[0m                 \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m       \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m       can_run_full_execution = (\n\u001b[1;32m   1250\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    639\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4062\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4064\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   4065\u001b[0m         _ctx, \"Identity\", name, input)\n\u001b[1;32m   4066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# audio features should help too: lets see what results we get from\n",
    "# a standard feed-forward network\n",
    "# note: audio features have been normalized\n",
    "\n",
    "train_normalized_audio_features = np.array(df_train['normalized_audio_feature_array'])\n",
    "test_normalized_audio_features = np.array(df_test['normalized_audio_feature_array'])\n",
    "\n",
    "train_normalized_audio_features_1 = df_train['normalized_audio_feature_array']\n",
    "test_normalized_audio_features_1 = df_test['normalized_audio_feature_array']\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(7,activation='softmax')\n",
    "])\n",
    "\n",
    "#Compile the model, specifying loss function, optimizer, and performance metric\n",
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer = keras.optimizers.Adam(learning_rate=0.01),\n",
    "             metrics=['accuracy'],\n",
    "             )\n",
    "\n",
    "model.fit(x = df_train_audio_normalized,y = train_labels.map(mapping),batch_size=8,epochs=10,\n",
    "         validation_data = (df_test_audio_normalized ,test_labels.map(mapping)),\n",
    "         use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.0866726 ,  0.55432179,  0.1220273 , -0.67410986, -0.41543318,\n",
      "        -0.44009997,  0.27353174, -0.04503026, -0.63357693])\n",
      " array([ 1.76913446,  0.39552649,  0.026016  , -0.33189818,  0.53373522,\n",
      "        -0.4389349 ,  0.19657636, -0.45322353, -1.48604061])\n",
      " array([ 0.16450892,  0.22229526,  0.59071407, -0.64693059, -0.66671723,\n",
      "        -0.44061023, -0.48747148, -0.72341021, -0.48175945])\n",
      " ...\n",
      " array([ 1.79308409,  0.18379943,  0.00990884,  0.70238102, -0.0556401 ,\n",
      "        -0.436609  ,  0.90627599, -0.82412047, -0.47881468])\n",
      " array([-0.38034528, -1.42340146, -2.1456081 ,  0.25068013, -0.65643743,\n",
      "         0.62471588, -0.82522011,  1.41640328,  0.77086777])\n",
      " array([ 0.25432005, -0.67754476, -1.23286906, -0.64654452, -0.48624959,\n",
      "         3.14196079, -0.38058901,  0.77423566,  0.79733172])            ]\n"
     ]
    }
   ],
   "source": [
    "print(train_normalized_audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets combine audio + lyrics and see what results look like\n",
    "#train_tokens_prebuilt = np.array(train_tokens_prebuilt)\n",
    "#test_tokens_prebuilt = np.array(test_tokens_prebuilt)\n",
    "df_train_audio_normalized['Lyrics'] = train_tokens_prebuilt\n",
    "df_test_audio_normalized['Lyrics'] = test_tokens_prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       danceability    energy  loudness  acousticness  speechiness  \\\n",
      "16303      0.086673  0.554322  0.122027     -0.674110    -0.415433   \n",
      "8721       1.769134  0.395526  0.026016     -0.331898     0.533735   \n",
      "11930      0.164509  0.222295  0.590714     -0.646931    -0.666717   \n",
      "7945       0.865036 -0.441758 -0.958100     -0.387106     2.007174   \n",
      "15504     -0.530030 -0.499502  0.518074     -0.549641    -0.626740   \n",
      "...             ...       ...       ...           ...          ...   \n",
      "12579      1.457789  0.809357 -0.472358     -0.679669     2.783870   \n",
      "16468      1.511676 -0.855588 -0.020411     -0.621064     0.373827   \n",
      "5125       1.793084  0.183799  0.009909      0.702381    -0.055640   \n",
      "15805     -0.380345 -1.423401 -2.145608      0.250680    -0.656437   \n",
      "2952       0.254320 -0.677545 -1.232869     -0.646545    -0.486250   \n",
      "\n",
      "       instrumentalness   valence     tempo  duration_ms  \\\n",
      "16303         -0.440100  0.273532 -0.045030    -0.633577   \n",
      "8721          -0.438935  0.196576 -0.453224    -1.486041   \n",
      "11930         -0.440610 -0.487471 -0.723410    -0.481759   \n",
      "7945          -0.441979 -0.855147  0.194737    -0.357768   \n",
      "15504          2.597692 -1.056086  0.425053     1.749697   \n",
      "...                 ...       ...       ...          ...   \n",
      "12579         -0.440878  0.760916  0.021399     0.062983   \n",
      "16468          0.059186  0.111070 -1.637594     0.011028   \n",
      "5125          -0.436609  0.906276 -0.824120    -0.478815   \n",
      "15805          0.624716 -0.825220  1.416403     0.770868   \n",
      "2952           3.141961 -0.380589  0.774236     0.797332   \n",
      "\n",
      "                                                  Lyrics  \n",
      "16303  [43981, 43981, 41201, 11973, 16909, 31639, 234...  \n",
      "8721   [43981, 43981, 43981, 5024, 3073, 12139, 7841,...  \n",
      "11930  [29907, 42376, 43981, 43981, 23426, 28376, 429...  \n",
      "7945   [43981, 43981, 43981, 43981, 2976, 2976, 43981...  \n",
      "15504  [41920, 43981, 29538, 43981, 42382, 12291, 342...  \n",
      "...                                                  ...  \n",
      "12579  [4334, 20514, 14193, 1273, 16915, 4475, 43981,...  \n",
      "16468  [15402, 43981, 43981, 43981, 43981, 43981, 791...  \n",
      "5125   [43981, 43981, 43981, 43981, 11112, 43981, 383...  \n",
      "15805  [35732, 43981, 43981, 12139, 21075, 23908, 350...  \n",
      "2952   [43981, 43981, 43981, 43981, 43981, 43981, 439...  \n",
      "\n",
      "[16500 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#df_train_audio_normalized\n",
    "print(df_train_audio_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-65e3a1849a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m              )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m model.fit(x = df_train_audio_normalized,y = train_labels.map(mapping),batch_size=8,epochs=10,\n\u001b[0m\u001b[1;32m     15\u001b[0m          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_test_audio_normalized\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m          use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# first try basic FFN\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(7,activation='softmax')\n",
    "])\n",
    "\n",
    "#Compile the model, specifying loss function, optimizer, and performance metric\n",
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer = keras.optimizers.Adam(learning_rate=0.01),\n",
    "             metrics=['accuracy'],\n",
    "             )\n",
    "\n",
    "model.fit(x = df_train_audio_normalized,y = train_labels.map(mapping),batch_size=8,epochs=10,\n",
    "         validation_data = (df_test_audio_normalized ,test_labels.map(mapping)),\n",
    "         use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>Sub-Genre: southern hip hop</th>\n",
       "      <th>Sub-Genre: nu metal</th>\n",
       "      <th>Sub-Genre: israeli mediterranean</th>\n",
       "      <th>Sub-Genre: thrash metal</th>\n",
       "      <th>Sub-Genre: pop rock</th>\n",
       "      <th>Sub-Genre: chicago blues</th>\n",
       "      <th>Sub-Genre: indie pop</th>\n",
       "      <th>Sub-Genre: classic rock</th>\n",
       "      <th>Sub-Genre: hardcore hip hop</th>\n",
       "      <th>normalized_audio_feature_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16303</th>\n",
       "      <td>Escape the Fate</td>\n",
       "      <td>Unbreakable</td>\n",
       "      <td>47</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.820</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.740</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.00546</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.08667260380184501, 0.5543217870346929, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>COUCOU CHLOE</td>\n",
       "      <td>NOBODY</td>\n",
       "      <td>46</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.787</td>\n",
       "      <td>11</td>\n",
       "      <td>-7.044</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.09410</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.769134455639869, 0.3955264896487572, 0.0260...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>beabadoobee</td>\n",
       "      <td>Worth It</td>\n",
       "      <td>51</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.751</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.16450891723207395, 0.22229525613682674, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7945</th>\n",
       "      <td>Andrew Broder</td>\n",
       "      <td>Bloodrush</td>\n",
       "      <td>39</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.613</td>\n",
       "      <td>11</td>\n",
       "      <td>-10.160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.07980</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8650357381041337, -0.4417578056589061, -0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15504</th>\n",
       "      <td>Greta Van Fleet</td>\n",
       "      <td>Age Of Man</td>\n",
       "      <td>56</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.601</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.486</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.03770</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.5300304949145832, -0.49950155016288295, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12579</th>\n",
       "      <td>ABSOLUTE.</td>\n",
       "      <td>Sage comme une image - Good as Gold</td>\n",
       "      <td>44</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.873</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.622</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0.00402</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.457789201918954, 0.8093566585939238, -0.472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16468</th>\n",
       "      <td>Pepper</td>\n",
       "      <td>Warning (feat. Stick Figure)</td>\n",
       "      <td>51</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.527</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.01920</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.511675880447574, -0.8555879746040728, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>Kelvyn Boy</td>\n",
       "      <td>Tele</td>\n",
       "      <td>49</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.095</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.36200</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.793084090541478, 0.18379942646750885, 0.009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15805</th>\n",
       "      <td>Sheryl Crow</td>\n",
       "      <td>Home</td>\n",
       "      <td>38</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0</td>\n",
       "      <td>-13.920</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.24500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.3803452767795279, -1.4234014622265112, -2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>Hedningarna</td>\n",
       "      <td>RÃ¤ven</td>\n",
       "      <td>46</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.564</td>\n",
       "      <td>2</td>\n",
       "      <td>-11.030</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.01260</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2543200481131073, -0.6775447623834782, -1.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16500 rows Ã— 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Artist Name                           Track Name  Popularity  \\\n",
       "16303  Escape the Fate                          Unbreakable          47   \n",
       "8721      COUCOU CHLOE                               NOBODY          46   \n",
       "11930      beabadoobee                             Worth It          51   \n",
       "7945     Andrew Broder                            Bloodrush          39   \n",
       "15504  Greta Van Fleet                           Age Of Man          56   \n",
       "...                ...                                  ...         ...   \n",
       "12579        ABSOLUTE.  Sage comme une image - Good as Gold          44   \n",
       "16468           Pepper         Warning (feat. Stick Figure)          51   \n",
       "5125        Kelvyn Boy                                 Tele          49   \n",
       "15805      Sheryl Crow                                 Home          38   \n",
       "2952       Hedningarna                                RÃ¤ven          46   \n",
       "\n",
       "       danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "16303         0.563   0.820    1    -6.740     0       0.0489       0.00546   \n",
       "8721          0.844   0.787   11    -7.044     1       0.1320       0.09410   \n",
       "11930         0.576   0.751    2    -5.256     1       0.0269       0.01250   \n",
       "7945          0.693   0.613   11   -10.160     0       0.2610       0.07980   \n",
       "15504         0.460   0.601    6    -5.486     0       0.0304       0.03770   \n",
       "...             ...     ...  ...       ...   ...          ...           ...   \n",
       "12579         0.792   0.873    6    -8.622     0       0.3290       0.00402   \n",
       "16468         0.801   0.527    1    -7.191     0       0.1180       0.01920   \n",
       "5125          0.848   0.743    1    -7.095     0       0.0804       0.36200   \n",
       "15805         0.485   0.409    0   -13.920     1       0.0278       0.24500   \n",
       "2952          0.591   0.564    2   -11.030     1       0.0427       0.01260   \n",
       "\n",
       "       ...  Sub-Genre: southern hip hop  Sub-Genre: nu metal  \\\n",
       "16303  ...                            0                    0   \n",
       "8721   ...                            0                    0   \n",
       "11930  ...                            0                    0   \n",
       "7945   ...                            0                    0   \n",
       "15504  ...                            0                    0   \n",
       "...    ...                          ...                  ...   \n",
       "12579  ...                            0                    0   \n",
       "16468  ...                            0                    0   \n",
       "5125   ...                            0                    0   \n",
       "15805  ...                            0                    0   \n",
       "2952   ...                            0                    0   \n",
       "\n",
       "       Sub-Genre: israeli mediterranean  Sub-Genre: thrash metal  \\\n",
       "16303                                 0                        0   \n",
       "8721                                  0                        0   \n",
       "11930                                 0                        0   \n",
       "7945                                  0                        0   \n",
       "15504                                 0                        0   \n",
       "...                                 ...                      ...   \n",
       "12579                                 0                        0   \n",
       "16468                                 0                        0   \n",
       "5125                                  0                        0   \n",
       "15805                                 0                        0   \n",
       "2952                                  0                        0   \n",
       "\n",
       "      Sub-Genre: pop rock Sub-Genre: chicago blues Sub-Genre: indie pop  \\\n",
       "16303                   0                        0                    0   \n",
       "8721                    0                        0                    0   \n",
       "11930                   0                        0                    1   \n",
       "7945                    0                        0                    0   \n",
       "15504                   0                        0                    0   \n",
       "...                   ...                      ...                  ...   \n",
       "12579                   0                        0                    0   \n",
       "16468                   0                        0                    0   \n",
       "5125                    0                        0                    0   \n",
       "15805                   1                        0                    0   \n",
       "2952                    0                        0                    0   \n",
       "\n",
       "      Sub-Genre: classic rock  Sub-Genre: hardcore hip hop  \\\n",
       "16303                       0                            0   \n",
       "8721                        0                            0   \n",
       "11930                       0                            0   \n",
       "7945                        0                            0   \n",
       "15504                       0                            0   \n",
       "...                       ...                          ...   \n",
       "12579                       0                            0   \n",
       "16468                       0                            0   \n",
       "5125                        0                            0   \n",
       "15805                       0                            0   \n",
       "2952                        0                            0   \n",
       "\n",
       "                          normalized_audio_feature_array  \n",
       "16303  [0.08667260380184501, 0.5543217870346929, 0.12...  \n",
       "8721   [1.769134455639869, 0.3955264896487572, 0.0260...  \n",
       "11930  [0.16450891723207395, 0.22229525613682674, 0.5...  \n",
       "7945   [0.8650357381041337, -0.4417578056589061, -0.9...  \n",
       "15504  [-0.5300304949145832, -0.49950155016288295, 0....  \n",
       "...                                                  ...  \n",
       "12579  [1.457789201918954, 0.8093566585939238, -0.472...  \n",
       "16468  [1.511675880447574, -0.8555879746040728, -0.02...  \n",
       "5125   [1.793084090541478, 0.18379942646750885, 0.009...  \n",
       "15805  [-0.3803452767795279, -1.4234014622265112, -2....  \n",
       "2952   [0.2543200481131073, -0.6775447623834782, -1.2...  \n",
       "\n",
       "[16500 rows x 73 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_tokens_prebuilt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_features_to_add = df_train_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist()\n",
    "test_audio_features_to_add = df_test_audio_normalized[['danceability', 'energy', 'loudness', 'acousticness', 'speechiness', 'instrumentalness', 'valence', 'tempo','duration_ms']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append features to tokens\n",
    "\n",
    "for token_list, feature_list in zip(train_tokens_prebuilt, train_audio_features_to_add):\n",
    "    for feature in feature_list:\n",
    "        token_list.append(feature)\n",
    "\n",
    "\n",
    "for token_list, feature_list in zip(test_tokens_prebuilt, test_audio_features_to_add):\n",
    "    for feature in feature_list:\n",
    "        token_list.append(feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2063/2063 [==============================] - 8s 4ms/step - loss: 2410.3469 - accuracy: 0.3086 - val_loss: 2.5497 - val_accuracy: 0.3012\n",
      "Epoch 2/10\n",
      "2063/2063 [==============================] - 9s 4ms/step - loss: 1.8301 - accuracy: 0.3147 - val_loss: 2.6634 - val_accuracy: 0.3012\n",
      "Epoch 3/10\n",
      "2063/2063 [==============================] - 9s 4ms/step - loss: 1.8307 - accuracy: 0.3147 - val_loss: 2.6675 - val_accuracy: 0.3012\n",
      "Epoch 4/10\n",
      "2063/2063 [==============================] - 8s 4ms/step - loss: 1.8306 - accuracy: 0.3147 - val_loss: 2.6660 - val_accuracy: 0.3012\n",
      "Epoch 5/10\n",
      "2063/2063 [==============================] - 7s 4ms/step - loss: 1.8303 - accuracy: 0.3147 - val_loss: 2.6672 - val_accuracy: 0.3012\n",
      "Epoch 6/10\n",
      "2063/2063 [==============================] - 8s 4ms/step - loss: 1.8300 - accuracy: 0.3147 - val_loss: 2.6643 - val_accuracy: 0.3012\n",
      "Epoch 7/10\n",
      "2063/2063 [==============================] - 9s 4ms/step - loss: 1.8301 - accuracy: 0.3147 - val_loss: 2.6702 - val_accuracy: 0.3012\n",
      "Epoch 8/10\n",
      "2063/2063 [==============================] - 10s 5ms/step - loss: 1.8295 - accuracy: 0.3147 - val_loss: 2.6615 - val_accuracy: 0.3012\n",
      "Epoch 9/10\n",
      "2063/2063 [==============================] - 8s 4ms/step - loss: 1.8300 - accuracy: 0.3147 - val_loss: 2.6617 - val_accuracy: 0.3012\n",
      "Epoch 10/10\n",
      "2063/2063 [==============================] - 9s 4ms/step - loss: 1.8301 - accuracy: 0.3147 - val_loss: 2.6647 - val_accuracy: 0.3012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa19c217700>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try basic FFN\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(100,activation='relu'),\n",
    "    keras.layers.Dense(7,activation='softmax')\n",
    "])\n",
    "\n",
    "#Compile the model, specifying loss function, optimizer, and performance metric\n",
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer = keras.optimizers.Adam(learning_rate=0.01),\n",
    "             metrics=['accuracy'],\n",
    "             )\n",
    "\n",
    "model.fit(x = np.array(train_tokens_prebuilt),y = train_labels.map(mapping),batch_size=8,epochs=10,\n",
    "         validation_data = (np.array(test_tokens_prebuilt), test_labels.map(mapping)),\n",
    "         use_multiprocessing=True,workers=multiprocessing.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7537845451542181\n"
     ]
    }
   ],
   "source": [
    "unk, total = 0,0\n",
    "for token_list in train_tokens_prebuilt:\n",
    "    for token in token_list:\n",
    "        if token == 43981:\n",
    "            unk += 1\n",
    "        total += 1\n",
    "print(unk/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11726, 16915, 943, 30943, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 1.769134455639869, 0.3955264896487572, 0.02601600085691811, -0.3318981833522548, 0.5337352175503375, -0.4389348974290065, 0.19657635700507578, -0.45322352624784257, -1.4860406055792426]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens_prebuilt[1][300:])\n",
    "print(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16303                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unbreakable Lyrics[Intro]\\nGo take it all\\nYour life, your dreams, your fire\\nGo take it all\\nThis day, this time, this hour\\n\\n[Chorus]\\nYou're unbreakable (You are unbreakable)\\nYou're unbreakable (You are unbreakable)\\n\\n[Verse 1]\\nLook into your eyes, find the strength inside\\nYou will rise above the weakness (Weakness)\\nYou may fall and fail, but you will prevail\\nYou will rise up above the challenge\\nTime to get ready to go\\nJumping into the unknown\\nTime to get rÐµady to go\\n[Pre-Chorus]\\nGo take it all\\nYour life, your dreams, your firÐµ\\nGo take it all\\nThis day, this time, this hour\\n\\n[Chorus]\\nYou're unbreakable (You are unbreakable)\\nYou're unbreakable (You are unbreakable)\\n\\n[Verse 2]\\nYou've been high and low, caught the undertow\\nHad to save yourself from madness (Madness)\\nWhen you walk the wire, will you find the fire?\\nWill you rise above the ashes?\\nTime to get ready to go\\nJumping into the unknown\\nTime to get ready to go\\n\\n[Refrain]\\nTake it all, take it all, take it all\\nTake it all, take it all, take it all\\n\\n[Pre-Chorus]\\nGo take it all\\nYour life, your dreams, your fire\\nGo take it all\\nThis day, this time, this hour\\n[Chorus]\\nYou're unbreakable (You are unbreakable)\\nYou're unbreakable (You are unbreakable)1Embed\n",
       "8721                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NOBODY LyricsTake a bitch\\nThat I have in one click\\nRuin my life don't trip (yea)\\nYou can takeâ€…aâ€…pic\\nBitch\\nThatâ€…Iâ€…loveâ€…in one click\\nDon'tâ€…touch much onâ€…me\\nDouble, make a miss\\n\\nBitch\\nNowâ€…handcuffs on my wrists\\nNow take one second\\nCan we make thatâŸquick?\\nTakeâŸaâŸbitch\\nThat I'd haveâŸin one click\\nRuinâŸmy life don't trip (yea)\\nYou can take a pic\\nTake a hit\\nNow he callin' on my phone\\nTake me out oh no\\nI wanna stay fucking home\\n(Nothing felt so right\\nIt's okay stay with me until we die)\\nTake a bitch\\nMaybe one on my list\\nTurn on my phone now everybody's gone\\nTake a hit\\nAdd a new one on my list\\nI'm tired now you're gone\\nI just wanna stay alone\\nTake a bitch\\nThat I have in one click\\nRuin my life don't trip (yea)\\nYou can take a pic\\nBitch\\nThat I love in one click\\nDon't touch much on me\\nDouble, make a miss\\n\\nBitch\\nNow handcuffs on my wrists\\nNow take one second\\nCan we make that quick?\\nBitch\\nThat I'd have in one click\\nRuin my life don't trip (yea)\\nYou can take a pic\\nBitch\\nThat I have in one click\\nRuin my life don't trip (yea)\\nYou can take a pic\\nBitch\\nThat I love in one click\\nDon't touch much on me\\nDouble, make a miss\\nBitch\\nNow handcuffs on my wrists\\nNow take one second\\nCan we make that quick?\\nBitch\\nThat I'd have in one click\\nRuin my life don't trip (yea)\\nYou can take a pic\\n\\nBitch\\nThat I'd love in one click\\nDon't touch much on me\\nDouble, make a miss\\nBitch\\nThat I love in one click\\nBitch\\nThat i'd love in one click\\nBitch (18x)1Embed\n",
       "11930                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Worth It Lyrics[Verse 1]\\nYour eyes are just like his\\nBut your face is a bit different\\nYou're a bit moreâ€…fucked,â€…but I guessâ€…that's fine\\nMaybe that's what I wantâ€…this time\\n\\n[Chorus]\\nYou seem to be breaking up\\nConnection too slow\\nSo just try texting again\\nDon't pick up the phone\\nDon't think we can be friends\\n'Cause you're too pretty\\nI wanna see you again\\nI don't know what I'm saying\\n[Verse 2]\\nThis is too bad to boast\\nBut this is my way to float\\nAnd if I shut up, I'm scared I'll drown\\nBut maybe that's the best for now\\n\\n[Chorus]\\nSeem to be breaking up\\nConnection too slow\\nSo just try texting again\\nDon't pick up the phone\\nDon't think we can be friends\\n'Cause you're too pretty\\nI wanna see you again\\nI don't know what I'm saying\\n\\n[Bridge]\\nDon't think this is worth it\\nDon't think this is worth it\\nI'm not wasting time\\nBut you've been on my mind\\nDon't think this is worth it this time\\n\\n[Chorus]\\nSeem to be breaking up\\nConnection too slow\\nSo just try texting again\\nDon't pick up the phone\\nDon't think we can be friends\\n'Cause you're too pretty\\nI wanna see you again\\nI don't know what I'm saying\\nSeem to be breaking up\\nConnection too slow\\nSo just try texting again\\nDon't pick up the phone\\nDon't think we can be friends\\n'Cause you're too pretty\\nI wanna see you again\\nI don't know what I'm saying2Embed\n",
       "7945                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Bloodrush Lyrics[Intro: Denzel Curry]\\nUgh\\nUgh\\n(What, what) Ugh\\n(What, what) Ugh\\nUgh\\n\\n[Verse 1: Denzel Curry]\\nBehind every smile, it be a river of tears\\nSix minutes, you're on, at the crack of dawn\\nYou will face all your worst fears\\nDip the curtains, you hear all the cheers\\nAnd profanity, all of its fear\\nSee your vanity, all of your peers\\nThey're your enemy, you will be veered to insanity (Ah)\\nI am the mare of the night, while others dream in the day\\nLife is a joke, your life is part of the play\\nMost of us stuck in the grave\\nWell, Edgar Allen Poe be your cup of joy\\nAnd watch a lonely actor life get destroyed (Ugh)\\n[Chorus: Justin Vernon]\\nI don't wanna hate, we just wanna have fun\\nTouchin' on my body, closer to my lung\\nPosted on my bloodrush, I be goin' up\\nBodies goin' up\\nI don't wanna hate, we just wanna have fun\\nTouchin' on my body, closer to my lung\\nPosted on my bloodrush, I be goin' up\\nBodies goin' up\\n\\n[Verse 2: Dua Saleh]\\nSinners on a summer night, funny you believe that\\nCommies on a money mart, gully grow the tree back\\nChummy throw ya chin about, kiddies couldn't feed that\\nTimmy tied his teeth amongst, knuckles couldn't bleed that\\nSinners on a summer night, funny you believe that\\nCommies on a money mart, gully grow your tree back\\nChummy throw ya chin about, kiddies couldn't feel that\\nTimmy tied his teeth amongst, knuckles couldn't bleed that\\nWinners grow your cinnamon, Peter pulled his teeth back\\nFunny he deceived that, running you received that\\nSit as you a sycamore, Sally grow her feet black\\nBunny hop to two laps, bunny hop to three laps\\n\\n[Verse 3: Haleek Maul]\\nNo handlebars on the ten speed\\nBust that shit 'til it's empty\\nI don't ever listen when they talking\\nCharlie's all in the middle of the market\\nSpark my ting in the middle of the market\\nSmoke plumes ring, it's a missing, it's a Miley\\nFeel for the kid, got big dreams, won't kneel to the king\\nI can't kneel to the pressure\\nSee through, you like the roof to the Tesla\\nSwerve off the Earth to the next dimension\\nRun a dot, then come lay with the ward\\nThey know that you 'fraid, they wait 'til you buy\\nSafe in vibrance from brightness, as always\\n[Outro: Justin Vernon]\\nSilver tongue to cinnamon, singers on the run\\nVocal cord attraction, brazen by the gun\\nSeen you 'round the city I, seen you cross my lungs\\nSeen you in the gritty, I seen you through the tons\\n\\n[Outro: Dua Saleh]\\nSkinny stick your stomach out, petty pick abort\\nPetty prick your finger with some branches and a fork\\nTerminate your troubles and drink them out a spork\\nTitty tickled troubles, heifers on a porchEmbed\n",
       "15504                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Age of Man Lyrics[Intro]\\nIn an age of darkness light appears\\nAnd it wards away the ancient fears\\nMarch to the anthem of the heart\\nTo a brand new day, a brand new start\\n\\n[Verse 1]\\nTo wonderlands of ice and snow\\nIn the desert heat where nothing grows\\nA tree of life in rain and sun\\nTo reach the sky it's just begun\\n[Chorus]\\nAnd as we came into the clear\\nTo find ourselves where we are here\\nWho is the wiser to help us steer?\\nAnd will we know when the end is near?\\n\\n[Verse 2]\\nA beauty lives in every soul\\nThe more you love the more you know\\nThey pass the torch and it still burns\\nOnce children then it's now our turn\\n\\n[Chorus]\\nAnd as we came into the clear\\nTo find ourselves where we are here\\nWho is the wiser to help us steer?\\nAnd will we know when the end is near?\\nAnd as we came into the clear\\nTo find ourselves where we are here\\nWho is the wiser to help us steer?\\nAnd will we know when the end is near?10Embed\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "12579    Reflections on the Revolution in France LyricsIT MAY NOT BE UNNECESSARY to inform the reader that the following Reflections had their origin in a correspondence between the Author and a very young gentleman at Paris, who did him the honor of desiring his opinion upon the important transactions which then, and ever since, have so much occupied the attention of all men. An answer was written sometime in the month of October 1789, but it was kept back upon prudential considerations. That letter is alluded to in the beginning of the following sheets. It has been since forwarded to the person to whom it was addressed. The reasons for the delay in sending it were assigned in a short letter to the same gentleman. This produced on his part a new and pressing application for the Author's sentiments.\\n\\nThe Author began a second and more full discussion on the subject. This he had some thoughts of publishing early in the last spring; but, the matter gaining upon him, he found that what he had undertaken not only far exceeded the measure of a letter, but that its importance required rather a more detailed consideration than at that time he had any leisure to bestow upon it. However, having thrown down his first thoughts in the form of a letter, and, indeed, when he sat down to write, having intended it for a private letter, he found it difficult to change the form of address when his sentiments had grown into a greater extent and had received another direction. A different plan, he is sensible, might be more favourable to a commodious division and distribution of his matter.\\n\\nDear Sir,\\n\\nYou are pleased to call again, and with some earnestness, for my thoughts on the late proceedings in France. I will not give you reason to imagine that I think my sentiments of such value as to wish myself to be solicited about them. They are of too little consequence to be very anxiously either communicated or withheld. It was from attention to you, and to you only, that I hesitated at the time when you first desired to receive them. In the first letter I had the honor to write to you, and which at length I send, I wrote neither for, nor from, any description of men, nor shall I in this. My errors, if any, are my own. My reputation alone is to answer for them.\\n\\nYou see, Sir, by the long letter I have transmitted to you, that though I do most heartily wish that France may be animated by a spirit of rational liberty, and that I think you bound, in all honest policy, to provide a permanent body in which that spirit may reside, and an effectual organ by which it may act, it is my misfortune to entertain great doubts concerning several material points in your late transactions.\\n\\nYOU IMAGINED, WHEN YOU WROTE LAST, that I might possibly be reckoned among the approvers of certain proceedings in France, from the solemn public seal of sanction they have received from two clubs of gentlemen in London, called the Constitutional Society and the Revolution Society.\\nI certainly have the honor to belong to more clubs than one, in which the constitution of this kingdom and the principles of the glorious Revolution are held in high reverence, and I reckon myself among the most forward in my zeal for maintaining that constitution and those principles in their utmost purity and vigor. It is because I do so, that I think it necessary for me that there should be no mistake. Those who cultivate the memory of our Revolution and those who are attached to the constitution of this kingdom will take good care how they are involved with persons who, under the pretext of zeal toward the Revolution and constitution, too frequently wander from their true principles and are ready on every occasion to depart from the firm but cautious and deliberate spirit which produced the one, and which presides in the other. Before I proceed to answer the more material particulars in your letter, I shall beg leave to give you such information as I have been able to obtain of the two clubs which have thought proper, as bodies, to interfere in the concerns of France, first assuring you that I am not, and that I have never been, a member of either of those societies.\\n\\nThe first, calling itself the Constitutional Society, or Society for Constitutional Information, or by some such title, is, I believe, of seven or eight years standing. The institution of this society appears to be of a charitable and so far of a laudable nature; it was intended for the circulation, at the expense of the members, of many books which few others would be at the expense of buying, and which might lie on the hands of the booksellers, to the great loss of an useful body of men. Whether the books, so charitably circulated, were ever as charitably read is more than I know. Possibly several of them have been exported to France and, like goods not in request here, may with you have found a market. I have heard much talk of the lights to be drawn from books that are sent from hence. What improvemen...\n",
       "16468    The Uncommercial Traveller (Chap. 15) LyricsNurseâ€™s Stories\\n\\n\\n\\nThere are not many places that I find it more agreeable to revisit when I am in an idle mood, than some places to which I have never been.  For, my acquaintance with those spots is of such long standing, and has ripened into an intimacy of so affectionate a nature, that I take a particular interest in assuring myself that they are unchanged.\\n\\nI never was in Robinson Crusoeâ€™s Island, yet I frequently return there.  The colony he established on it soon faded away, and it is uninhabited by any descendants of the grave and courteous Spaniards, or of Will Atkins and the other mutineers, and has relapsed into its original condition.  Not a twig of its wicker houses remains, its goats have long run wild again, its screaming parrots would darken the sun with a cloud of many flaming colours if a gun were fired there, no face is ever reflected in the waters of the little creek which Friday swam across when pursued by his two brother cannibals with sharpened stomachs.  After comparing notes with other travellers who have similarly revisited the Island and conscientiously inspected it, I have satisfied myself that it contains no vestige of Mr. Atkinsâ€™s domesticity or theology, though his track on the memorable evening of his landing to set his captain ashore, when he was decoyed about and round about until it was dark, and his boat was stove, and his strength and spirits failed him, is yet plainly to be traced.  So is the hill-top on which Robinson was struck dumb with joy when the reinstated captain pointed to the ship, riding within half a mile of the shore, that was to bear him away, in the nine-and-twentieth year of his seclusion in that lonely place.  So is the sandy beach on which the memorable footstep was impressed, and where the savages hauled up their canoes when they came ashore for those dreadful public dinners, which led to a dancing worse than speech-making.  So is the cave where the flaring eyes of the old goat made such a goblin appearance in the dark.  So is the site of the hut where Robinson lived with the dog and the parrot and the cat, and where he endured those first agonies of solitude, whichâ€”strange to sayâ€”never involved any ghostly fancies; a circumstance so very remarkable, that perhaps he left out something in writing his record?  Round hundreds of such objects, hidden in the dense tropical foliage, the tropical sea breaks evermore; and over them the tropical sky, saving in the short rainy season, shines bright and cloudless.\\n\\nNeither, was I ever belated among wolves, on the borders of France and Spain; nor, did I ever, when night was closing in and the ground was covered with snow, draw up my little company among some felled trees which served as a breastwork, and there fire a train of gunpowder so dexterously that suddenly we had three or four score blazing wolves illuminating the darkness around us.  Nevertheless, I occasionally go back to that dismal region and perform the feat again; when indeed to smell the singeing and the frying of the wolves afire, and to see them setting one another alight as they rush and tumble, and to behold them rolling in the snow vainly attempting to put themselves out, and to hear their howlings taken up by all the echoes as well as by all the unseen wolves within the woods, makes me tremble.\\n\\nI was never in the robbersâ€™ cave, where Gil Blas lived, but I often go back there and find the trap-door just as heavy to raise as it used to be, while that wicked old disabled Black lies everlastingly cursing in bed.  I was never in Don Quixoteâ€™s study, where he read his books of chivalry until he rose and hacked at imaginary giants, and then refreshed himself with great draughts of water, yet you couldnâ€™t move a book in it without my knowledge, or with my consent.  I was never (thank Heaven) in company with the little old woman who hobbled out of the chest and told the merchant Abudah to go in search of the Talisman of Oromanes, yet I make it my business to know that she is well preserved and as intolerable as ever.  I was never at the school where the boy Horatio Nelson got out of bed to steal the pears: not because he wanted any, but because every other boy was afraid: yet I have several times been back to this Academy, to see him let down out of window with a sheet.  So with Damascus, and Bagdad, and Brobingnag (which has the curious fate of being usually misspelt when written), and Lilliput, and Laputa, and the Nile, and Abyssinia, and the Ganges, and the North Pole, and many hundreds of placesâ€”I was never at them, yet it is an affair of my life to keep them intact, and I am always going back to them.\\nBut, when I was in Dullborough one day, revisiting the associations of my childhood as recorded in previous pages of these notes, my experience in this wise was made quite inconsiderable and of no account, by the quantity of places and peopleâ€”utterly impossible places and people, but none th...\n",
       "5125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tele Lyrics[Intro: Kelvyn Boy]\\nAnother (It's Kel P vibes)\\n\\n[Verse 1: Kelvyn Boy]\\nWanna give you everything oh bebe\\nMake you no dey take me dey play\\nOh my girl I go dey there for you make you skele my body bebe\\nWhen I pull up all night you know bebe\\nI dey sing song for you girl\\nOh my girl I go dey there for you make you ginger my body bebe (yeah, yeah, yeah)\\n[Chorus: Kelvyn Boy]\\nGirl I'm stuck on you\\nI get e love for you\\nWhen I tele my love for you, shey you go love me too? Girl\\nGirl, girl I'm stuck on you\\nI get e love for you\\nWhen I tele my love for you, shey you go love me too?\\n\\n[Verse 2: Kelvyn Boy]\\nI get your matter for my head, I go tell you now\\nEven if I suffer girl, I go make you know\\nGirl make I confess, I dey love you oh, I dey love you oh\\nI dey love you oh, oh-oh, oh\\nE be me and you, if another man try I go kill am oh\\nOh baby oh woah, if you don't know, I go kill am oh, I go kill am oh\\nGirl I no go kiss and tell\\nFor the party, baby owanbe\\nI like the way you ring my bell, so my baby I go dey loyal\\nGirl I no go kiss and tell\\nFor the party, baby owanbe\\nLike the way you ring my bell, so my baby I go dey loyal (yeah, yeah, yeah)\\n\\n[Chorus: Kelvyn Boy]\\nGirl I'm stuck on you, I get e love for you\\nWhen I tele my love for you, shey you go love me too?\\nGirl I'm stuck on you, I feel love for you\\nWhen I tele my love for you, shey you go love me too?\\n[Verse 3: Crayon]\\nEh eh\\nI dey for Ghana, I dey chill pelu Kelvyn boy\\nGhana girl I need your love eh eh\\nOmo tena show me something eh\\nShow me, se oma pongela? On a 9 to 5\\nWhine that waist oh my Lucifer\\nI dey pray make I cast and bind\\nGirl I no go kiss and tell\\nFor the party, baby owanbe (omo say na owanbe)\\nI like the way you ring my bell, so my baby I go dey loyal\\nGirl I no go kiss and tell\\nFor the party, baby owanbe (say na owanbe)\\nLike the way you ring my bell, so my baby I go dey loyal\\n\\n[Chorus: Kelvyn Boy]\\nGirl I'm stuck on you, I get e love for you\\nWhen I tele my love for you, shey you go love me too?\\nGirl I'm stuck on you, I feel love for you\\nWhen I tele my love for you,uh, shey you go love me too?Embed\n",
       "15805                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Home Lyrics[Verse 1]\\nI woke up this mornin'\\nAnd now I understand\\nWhat it means to give your life\\nTo just one man\\nAfraid of feelin' nothing\\nNo bees or butterflies\\nMy head is full of voices\\nAnd my house is full of lies\\n\\n[Chorus]\\nThis is home, home\\nAnd this is home, home\\nThis is home\\n[Verse 2]\\nI found you stand there\\nWhen I was seventeen\\nNow I'm thirty-two\\nAnd I can't remember what I'd seen in you\\nAnd I made a promise\\nSaid it every day\\nNow I'm reading romance novels\\nAnd I'm dreamin' of yesterday\\n\\n[Chorus]\\nThis is home, home\\nAnd this is home, and this is home\\nThis is home\\n\\n[Bridge]\\nI'd like to see the Riviera\\nAnd slow dance underneath the stars\\nI'd like to watch the sun come up\\nIn a stranger's arms\\n\\n[Chorus]\\nAnd this is home, oh\\nAnd this is home, home, home, home\\n\\n[Verse 3]\\nI'm going crazy\\nA little every day\\nAnd everything I wanted\\nIs now driving me away\\nI woke this morning\\nTo the sound of breaking hearts\\nMine is full of questions\\nAnd it's tearing yours apart\\nTearing yours apart\\n[Outro]\\nAnd it's tearing yours apart\\nTearing yours apart\\nAnd it's tearing us apart1Embed\n",
       "2952                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    RÃ¤ven LyricsRÃ¶da lÃ¤ppar dÃ¶ljer dina tÃ¤nder\\nOch din tunga Ã¤r sÃ¥ strÃ¤v\\nEld och rimfrost Ã¤r i dina Ã¶gon\\nÃ„r du kvinna eller rÃ¤v?\\nJagar slugt och vilt i natten\\nLÃ¥nga Ã¤rmar dÃ¶ljer dina klor\\nLeker lystet med ditt byte\\nMunnen din Ã¤r rÃ¶d av blod\\n\\nNouse kirki kiimalle\\nPerÃ¤hyÃ¶ry hÃ¶ngylle\\nNouse ilman nojumata\\nKihko ilman kirromata\\nVild och farligt vacker Ã¤r du\\nKjolen dÃ¶ljer knappt din svans\\nLockar lÃ¤ngre ut i mossen\\nMed din galna rovdjursdans\\nJust nÃ¤r dina klÃ¤der fallit\\nOch din nakna kropp jag sett\\nSkrattar du och Ã¶ppnar kÃ¤ften\\nGer mig djupa kÃ¤rleksbett\\n\\nNouse kirki kiimalle\\nPerÃ¤hyÃ¶ry hÃ¶ngylle\\nNouse ilman nojumata\\nKihko ilman kirromata\\n\\nStiger lust till livligt lystnad\\nRusar sav i stammen upp\\nStiger sejdens hjÃ¤lp fÃ¶rutan\\nRusar utan galder raktEmbed\n",
       "Name: Lyrics, Length: 16500, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_text_length(text_data):\n",
    "    return_data = []\n",
    "    for text in text_data:\n",
    "        new_text = text.lower()\n",
    "        new_text = text.replace('\\n',' ')\n",
    "        new_text = text.replace('  ',' ')\n",
    "        new_text = new_text.split()\n",
    "        return_data.append(new_text)\n",
    "    return(return_data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lyrics = avg_text_length(df_train['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.9347878787878\n",
      "16500\n",
      "['Movinâ€™', 'On', 'Up', 'Lyrics[Verse', '1]', 'I', 'was', 'blind,', 'now', 'I', 'can', 'see', 'You', 'made', 'a', 'believer', 'out', 'of', 'me', 'I', 'was', 'blind,', 'now', 'I', 'can', 'see', 'You', 'made', 'a', 'believer', 'out', 'of', 'me', '[Chorus]', \"I'm\", \"movin'\", 'on', 'up', 'now', 'Getting', 'out', 'of', 'the', 'darkness', 'My', 'light', 'shines', 'on', 'My', 'light', 'shines', 'on', 'My', 'light', 'shines', 'on', '[Verse', '2]', 'I', 'was', 'lost,', 'now', \"I'm\", 'found', 'I', 'believe', 'in', 'you,', \"I've\", 'got', 'no', 'bounds', 'I', 'was', 'lost,', 'now', \"I'm\", 'found', 'I', 'believe', 'in', 'you,', \"I've\", 'got', 'no', 'bounds', '[Chorus]', \"I'm\", \"movin'\", 'on', 'up', 'now', 'Getting', 'out', 'of', 'the', 'darkness', 'My', 'light', 'shines', 'on', 'My', 'light', 'shines', 'on', 'My', 'light', 'shines', 'on', '(My', 'light', 'shines', 'on)', 'My', 'light', 'shines', 'on', '(My', 'light', 'shines', 'on)', 'My', 'light', 'shines', 'on', '[Bridge]', '(My', 'light', 'shines', 'on)', '(My', 'light', 'shines', 'on)', '(My', 'light', 'shines', 'on)', '(My', 'light', 'shines', 'on)', '[Outro]', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness', 'My', 'light', 'shines', 'on', \"I'm\", 'getting', 'out', 'of', 'darkness2Embed']\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "total_len = 0\n",
    "iter = 0\n",
    "old_len = 0\n",
    "\n",
    "\n",
    "for lyric in new_lyrics:\n",
    "    total_len += len(lyric)\n",
    "    len_lyric = len(lyric)\n",
    "    iter += 1\n",
    "print(total_len/len(new_lyrics))\n",
    "print(len(new_lyrics))\n",
    "\n",
    "print(new_lyrics[14001])\n",
    "print(len(new_lyrics[14001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'Iteration', 'Lyrics[Verse', '1]', 'I', 'fell', 'for', 'some', 'Pseudo-sophisticated', 'Poet', 'laureate-posing', 'Young', 'white', 'savior', 'He', 'sang', 'to', 'me', 'A', 'blue', 'collar', 'emulation', 'An', 'accent', 'so', 'affected', 'So', 'midwestern', '[Pre-Chorus', '1]', '\"Shots', 'in', 'the', 'dark', 'Kalashnokov', \"You'll\", 'take', 'her', 'eye', 'out\"', 'Too', 'cool', 'to', 'be', 'trite', 'His', 'cunning', 'remarks', 'And', 'then', 'he', 'did', 'it', 'again,', 'did', 'it', 'again,', 'did', 'it', 'again', 'Did', 'it', 'again,', 'did', 'it', 'again,', 'did', 'it', 'again', '[Chorus]', 'An', 'iteration', 'An', 'iteration', 'An', 'iteration', '[Verse', '2]', 'He', 'said', 'to', 'me', '\"You\\'ve', 'got', 'your', 'own', 'reflection\"', \"It's\", 'extra', 'underwhelming', 'It', 'lets', 'me', 'down', '[Pre-Chorus', '2]', \"Stroh's\", 'from', 'the', 'tap,', 'working', 'class', 'porn', 'And', \"daddy's\", 'money', 'Too', 'cool', 'to', 'excite', \"He's\", 'up', 'there', 'so', 'bored', 'And', 'then', 'he', 'did', 'it', 'again,', 'did', 'it', 'again,', 'did', 'it', 'again', 'Did', 'it', 'again,', 'did', 'it', 'again,', 'did', 'it', 'again', '[Chorus]', 'An', 'iteration', '(Did', 'it', 'again,', 'did', 'it', 'again,', 'did', 'it', 'again)', 'An', 'iteration', '(Did', 'it', 'again,', 'did', 'it', 'again,', 'did', 'it', 'again)', 'An', 'iteration', '(Did', 'it', 'again,', 'did', 'it', 'again,', 'did', 'it', 'again)Embed']\n"
     ]
    }
   ],
   "source": [
    "#new_lyrics.sort(key = len, reverse = True)\n",
    "#print(new_lyrics[1][0:100])\n",
    "\n",
    "#for lyric in new_lyrics[0:1500]:\n",
    " #   print(len(lyric))\n",
    "    \n",
    "print(new_lyrics[13556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43981, 43981, 43981, 1465, 43981, 34782, 35357, 43981, 36633, 33259, 9511, 43981, 19666, 6, 7347, 30496, 10161, 3073, 11992, 43981, 22793, 43981, 22793, 23973, 43700, 28117, 27498, 43981, 1465, 43981, 39418, 21386, 588, 43981, 4110, 28117, 43981, 14193, 43981, 3231, 28117, 10490, 43981, 27452, 43981, 16915, 36945, 43981, 16915, 14193, 43981, 43981, 36945, 38967, 30212, 28117, 43981, 14193, 43981, 12728, 19334, 40473, 41394, 9059, 36645, 14193, 43981, 12728, 32865, 3821, 43981, 43981, 43981, 37663, 19870, 12369, 43981, 16834, 19870, 7033, 6071, 37060, 25687, 43981, 16834, 19870, 7033, 6071, 43981, 43981, 1465, 43981, 43981, 37060, 17131, 43981, 43981, 23844, 12292, 43981, 24652, 12139, 43817, 36633, 12139, 24344, 12917, 43069, 20514, 20354, 16915, 36945, 43981, 37008, 43817, 40367, 18991, 14446, 43981, 12139, 12289, 43981, 6071, 20514, 19334, 34707, 43981, 1465, 43981, 39418, 21386, 588, 43981, 4110, 28117, 43981, 14193, 43981, 3231, 28117, 10490, 43981, 27452, 43981, 16915, 36945, 43981, 16915, 14193, 43981, 43981, 36945, 38967, 30212, 28117, 43981, 14193, 43981, 12728, 19334, 40473, 41394, 9059, 36645, 14193, 43981, 12728, 32865, 3821, 43981, 43981, 43981, 37663, 19870, 12369, 43981, 16834, 19870, 7033, 6071, 37060, 25687, 43981, 16834, 19870, 7033, 6071, 43981, 588, 43981, 12727, 37008, 11571, 19334, 43981, 37060, 18991, 42591, 3134, 12727, 37008, 11571, 19334, 43981, 12139, 43817, 16834, 19870, 19778, 43981, 1465, 43981, 39418, 21386, 588, 43981, 4110, 28117, 43981, 14193, 43981, 3231, 28117, 10490, 43981, 27452, 43981, 16915, 36945, 43981, 16915, 14193, 43981, 43981, 36945, 38967, 30212, 28117, 43981, 14193, 43981, 12728, 19334, 40473, 41394, 9059, 36645, 14193, 43981, 12728, 32865, 3821, 43981, 43981, 43981, 37663, 19870, 12369, 43981, 16834, 19870, 7033, 6071, 37060, 25687, 43981, 16834, 19870, 7033, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, 43981, -0.015113344529992166, 1.117323295948467, 0.9093831641582428, -0.6950564266536715, -0.133309719907472, -0.42840243858826327, -0.44899379357452424, -0.9597219314312615, -0.21174926951545908]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens_prebuilt[11200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-dfa16fc7af83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lyrics'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lyrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "df_train['Lyrics'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length = df_train['Lyrics'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11293.51103030303\n"
     ]
    }
   ],
   "source": [
    "print(mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_train['Lyrics'].str.len().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([14097,  2299,  3159,  3189,  3365, 15600, 18570,  2474,  1366,\n",
      "             2968,\n",
      "            ...\n",
      "             1896, 13649,  2574,  9360,  5683,   854,  1299,  2947,  3709,\n",
      "            10720],\n",
      "           dtype='int64', length=16500)\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reindexed = df_train.reindex(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reindexed_subset = df_train_reindexed.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reindexed_subset.to_excel('reindex_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reindexed_subset_2 = df_train_reindexed[df_train_reindexed['Artist Name'] == 'AJ Tracey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reindexed_subset_3 = df_train_reindexed[df_train_reindexed['Artist Name'] == 'Kanye West']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reindexed_subset_2.to_excel('reindex_test_aj.xlsx')\n",
    "df_train_reindexed_subset_3.to_excel('reindex_test_kanye.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
