{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47218b40",
   "metadata": {},
   "source": [
    "# 1. Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0327700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,recall_score\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import matplotlib.pyplot as plt\n",
    "#import shap\n",
    "\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07bcff",
   "metadata": {},
   "source": [
    "# 2. Read in Train/Val/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c5e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('Train_Test_Data/genre_sub_genre_train.pkl','rb'))[['danceability', 'energy',\n",
    "                                                                                 'loudness', \n",
    "                                                                                 'speechiness', 'acousticness',\n",
    "                                                                                 'instrumentalness', 'liveness', \n",
    "                                                                                 'valence', 'tempo', \n",
    "                                                                                 'duration_ms',\n",
    "                                                                                 'Lyrics','Major Genre']]\n",
    "\n",
    "val = pickle.load(open('Train_Test_Data/genre_sub_genre_test.pkl','rb'))[['danceability', 'energy',\n",
    "                                                                                 'loudness',\n",
    "                                                                                 'speechiness', 'acousticness',\n",
    "                                                                                 'instrumentalness', 'liveness', \n",
    "                                                                                 'valence', 'tempo',\n",
    "                                                                                 'duration_ms', \n",
    "                                                                                 'Lyrics','Major Genre']].iloc[:1437]\n",
    "test = pickle.load(open('Train_Test_Data/genre_sub_genre_test.pkl','rb'))[['danceability', 'energy',\n",
    "                                                                                 'loudness', \n",
    "                                                                                 'speechiness', 'acousticness',\n",
    "                                                                                 'instrumentalness', 'liveness', \n",
    "                                                                                 'valence', 'tempo',\n",
    "                                                                                 'duration_ms', \n",
    "                                                                                 'Lyrics','Major Genre']].iloc[1437:]\n",
    "\n",
    "np.random.seed(50)\n",
    "all_data = pd.concat([train,val,test],ignore_index=True)\n",
    "#all_data['Major Genre'] = all_data['Major Genre'].apply(lambda x:'Rock' if x == 'Metal' else x)\n",
    "all_data = all_data.iloc[np.random.choice(all_data.index,len(all_data))]\n",
    "train = all_data.iloc[:len(train)]\n",
    "val = all_data.iloc[len(train):len(train) + len(val)]\n",
    "test = all_data.iloc[len(train) + len(val):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e109956",
   "metadata": {},
   "source": [
    "# 3. Clean Lyric Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6956d",
   "metadata": {
    "id": "e2d6956d"
   },
   "source": [
    "#### Genius API Generated Text at the beginning of lyrics, typically of the form Track Name or Album Name + ' ' + Lyrics, remove from input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025e9703",
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1658335703748,
     "user": {
      "displayName": "Amit Gattadahalli",
      "userId": "12177286346576981509"
     },
     "user_tz": 240
    },
    "id": "025e9703"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/_wnqfx9n7674x_jkw4jm41xw0000gn/T/ipykernel_2877/217011968.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['Lyrics'] = train['Lyrics'].apply(lambda x: ' '.join(x.split(' Lyrics')[1:]).lower())\n",
      "/var/folders/w2/_wnqfx9n7674x_jkw4jm41xw0000gn/T/ipykernel_2877/217011968.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val['Lyrics'] = val['Lyrics'].apply(lambda x: ' '.join(x.split(' Lyrics')[1:]).lower())\n",
      "/var/folders/w2/_wnqfx9n7674x_jkw4jm41xw0000gn/T/ipykernel_2877/217011968.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Lyrics'] = test['Lyrics'].apply(lambda x: ' '.join(x.split(' Lyrics')[1:]).lower())\n"
     ]
    }
   ],
   "source": [
    "train['Lyrics'] = train['Lyrics'].apply(lambda x: ' '.join(x.split(' Lyrics')[1:]).lower())\n",
    "val['Lyrics'] = val['Lyrics'].apply(lambda x: ' '.join(x.split(' Lyrics')[1:]).lower())\n",
    "test['Lyrics'] = test['Lyrics'].apply(lambda x: ' '.join(x.split(' Lyrics')[1:]).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7f4d3",
   "metadata": {
    "id": "2bd7f4d3"
   },
   "source": [
    "#### Drop examples with over 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724a610a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1658335704874,
     "user": {
      "displayName": "Amit Gattadahalli",
      "userId": "12177286346576981509"
     },
     "user_tz": 240
    },
    "id": "724a610a"
   },
   "outputs": [],
   "source": [
    "token_thresh = 1000\n",
    "\n",
    "train_bool = train['Lyrics'].apply(lambda x:True if len(str(x).split()) <= token_thresh else False)\n",
    "train = train[train_bool]\n",
    "\n",
    "val_bool = val['Lyrics'].apply(lambda x:True if len(str(x).split()) <= token_thresh else False)\n",
    "val = val[val_bool]\n",
    "\n",
    "test_bool = test['Lyrics'].apply(lambda x:True if len(str(x).split()) <= token_thresh else False)\n",
    "test = test[test_bool]\n",
    "\n",
    "train.index = np.arange(0,len(train))\n",
    "val.index = np.arange(0,len(val))\n",
    "test.index = np.arange(0,len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cd70f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_bool,val_bool,test_bool\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00f93a",
   "metadata": {
    "id": "9b00f93a"
   },
   "source": [
    "#### Get Language Indicators Marking Natural Split Points Between Songs, shown by genius API with language between brackets to serve as potential text splitting criteria later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a2b504",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1658335704875,
     "user": {
      "displayName": "Amit Gattadahalli",
      "userId": "12177286346576981509"
     },
     "user_tz": 240
    },
    "id": "d9a2b504"
   },
   "outputs": [],
   "source": [
    "def split_text_into_regions(text):\n",
    "    string = text\n",
    "    \n",
    "    #mark line breaks\n",
    "    string = string.replace('\\n','[]')\n",
    "    string = string.replace('embed','')\n",
    "    #find language indicators of song sections\n",
    "    splits = re.findall('\\[.*?\\]',string)\n",
    "    #find ad libs to remove\n",
    "    ad_libs = re.findall('\\(.*?\\)',string)\n",
    "    \n",
    "    #remove ad libs\n",
    "    if len(ad_libs) > 0:\n",
    "        for ad_lib in ad_libs:\n",
    "            string = string.replace(ad_lib,'')\n",
    "        string = string.replace('  ',' ')\n",
    "    \n",
    "    #If there is no splitting criteria, single string is entire song without any additional groupings\n",
    "    if len(splits) == 0:\n",
    "        string = [string]\n",
    "    else:\n",
    "        #replace split criteria with makers for splitting\n",
    "        for delim in splits:\n",
    "            string = string.replace(delim,'[]')\n",
    "        string = string.split('[]')\n",
    "    \n",
    "    #Identify sections of song, made up of groups of lyrics\n",
    "    sections = []\n",
    "    section = []\n",
    "    last_part = ''\n",
    "    for part in string:\n",
    "        if part == '' and last_part != '':\n",
    "            sections.append(section)\n",
    "            section = []\n",
    "        elif part != '':\n",
    "            section.append(part)\n",
    "        \n",
    "        last_part = part\n",
    "    \n",
    "    try:\n",
    "        if section != sections[-1]:\n",
    "            sections.append(section)\n",
    "    except:\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be688571",
   "metadata": {
    "id": "be688571"
   },
   "source": [
    "#### Cleaner version of lyrics by joining the lyric groups above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0610488",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1658335704875,
     "user": {
      "displayName": "Amit Gattadahalli",
      "userId": "12177286346576981509"
     },
     "user_tz": 240
    },
    "id": "c0610488"
   },
   "outputs": [],
   "source": [
    "def single_text_lyrics(group_of_lyrics):\n",
    "    lyrics = ''\n",
    "    for group in group_of_lyrics:\n",
    "        lyrics = lyrics + ' ' + ' '.join(group)\n",
    "    return lyrics.strip() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abee49",
   "metadata": {
    "id": "99abee49"
   },
   "source": [
    "#### Create Modified Versions of Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff11e2d",
   "metadata": {
    "executionInfo": {
     "elapsed": 2835,
     "status": "ok",
     "timestamp": 1658335707707,
     "user": {
      "displayName": "Amit Gattadahalli",
      "userId": "12177286346576981509"
     },
     "user_tz": 240
    },
    "id": "9ff11e2d"
   },
   "outputs": [],
   "source": [
    "# Lyric Groups\n",
    "train['Lyric Group'] = train['Lyrics'].apply(lambda x:split_text_into_regions(x))\n",
    "val['Lyric Group'] = val['Lyrics'].apply(lambda x:split_text_into_regions(x))\n",
    "test['Lyric Group'] = test['Lyrics'].apply(lambda x:split_text_into_regions(x))\n",
    "\n",
    "# Cleaner Lyrics\n",
    "train['Cleaner Lyrics'] = train['Lyric Group'].apply(lambda x:single_text_lyrics(x))\n",
    "val['Cleaner Lyrics'] = val['Lyric Group'].apply(lambda x:single_text_lyrics(x))\n",
    "test['Cleaner Lyrics'] = test['Lyric Group'].apply(lambda x:single_text_lyrics(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37653a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['Cleaner Lyrics'] != '']\n",
    "val = val[val['Cleaner Lyrics'] != '']\n",
    "test = test[test['Cleaner Lyrics'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0dc5de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock           0.321368\n",
       "Indie          0.157749\n",
       "Pop            0.142096\n",
       "Metal          0.120842\n",
       "Hip Hop        0.100600\n",
       "Alternative    0.096822\n",
       "Blues          0.060522\n",
       "Name: Major Genre, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Rock           0.316679\n",
       "Indie          0.164489\n",
       "Pop            0.146810\n",
       "Metal          0.126826\n",
       "Hip Hop        0.094543\n",
       "Alternative    0.086856\n",
       "Blues          0.063797\n",
       "Name: Major Genre, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Rock           0.310185\n",
       "Indie          0.172068\n",
       "Pop            0.153549\n",
       "Alternative    0.105710\n",
       "Metal          0.105710\n",
       "Hip Hop        0.095679\n",
       "Blues          0.057099\n",
       "Name: Major Genre, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train['Major Genre'].value_counts()/len(train))\n",
    "display(val['Major Genre'].value_counts()/len(val))\n",
    "display(test['Major Genre'].value_counts()/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2d7bb",
   "metadata": {},
   "source": [
    "# 4. Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339cd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_weights = train['Major Genre'].value_counts().max()/train['Major Genre'].value_counts()\n",
    "class_weights = {}\n",
    "label_mapping = {}\n",
    "weights = {}\n",
    "\n",
    "for num in range(len(label_weights)):\n",
    "    class_weights[label_weights.index[num]] = label_weights.iloc[num]\n",
    "    label_mapping[label_weights.index[num]] = num\n",
    "    weights[num] = label_weights.iloc[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab129bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rock': 1.0,\n",
       " 'Indie': 2.0372112917023095,\n",
       " 'Pop': 2.261633428300095,\n",
       " 'Metal': 2.6594081518704633,\n",
       " 'Hip Hop': 3.194500335345406,\n",
       " 'Alternative': 3.3191637630662023,\n",
       " 'Blues': 5.309921962095875}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Rock': 0,\n",
       " 'Indie': 1,\n",
       " 'Pop': 2,\n",
       " 'Metal': 3,\n",
       " 'Hip Hop': 4,\n",
       " 'Alternative': 5,\n",
       " 'Blues': 6}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{0: 1.0,\n",
       " 1: 2.0372112917023095,\n",
       " 2: 2.261633428300095,\n",
       " 3: 2.6594081518704633,\n",
       " 4: 3.194500335345406,\n",
       " 5: 3.3191637630662023,\n",
       " 6: 5.309921962095875}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(class_weights)\n",
    "display(label_mapping)\n",
    "display(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece443c7",
   "metadata": {},
   "source": [
    "# 5. Separate Audio and Lyric Features, labels, Standardize Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769ae78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train.iloc[:,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556321e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler,open('audio_scaler.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c160183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Audio + Lyrics\n",
    "train_audio = scaler.transform(train.iloc[:,:10])\n",
    "train_lyrics = train.iloc[:,-1]\n",
    "\n",
    "# Val Audio + Lyrics\n",
    "val_audio = scaler.transform(val.iloc[:,:10])\n",
    "val_lyrics = val.iloc[:,-1]\n",
    "\n",
    "# Test Audio + Lyrics\n",
    "test_audio = scaler.transform(test.iloc[:,:10])\n",
    "test_lyrics = test.iloc[:,-1]\n",
    "\n",
    "#Train/Val/Test Labels\n",
    "train_labels = train.iloc[:,-3].map(label_mapping)\n",
    "val_labels = val.iloc[:,-3].map(label_mapping)\n",
    "test_labels = test.iloc[:,-3].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc26c66",
   "metadata": {},
   "source": [
    "# 6. Feed Forward Network w/ Audio Features Only to Serve as Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce62fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_recall(y_true,y_pred):\n",
    "    #true labels\n",
    "    true = y_true.numpy()\n",
    "    #predicted prob of each class for each sample\n",
    "    pred = y_pred.numpy()\n",
    "    #prob to class based off max predicted prob\n",
    "    pred = np.array([x.argmax() for x in pred])\n",
    "    #confusion matrix\n",
    "    confuse = confusion_matrix(true,pred)\n",
    "    confuse_sum = confuse.sum(axis=1)\n",
    "    score = 0\n",
    "    for num in range(len(confuse_sum)):\n",
    "        if confuse_sum[num]!=0:\n",
    "            score = score + confuse[num][num]/confuse_sum[num]\n",
    "    \n",
    "    return score/len(confuse_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8b46329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ff(hidden_layers = [100,100],hidden_layer_activation = 'relu',dropout_rate = 0.3,shape=(10,),\n",
    "             output_layer_size = 7, output_layer_activation = 'softmax',learning_rate = 0.001,epochs = 10):\n",
    "    \n",
    "    #input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=shape)\n",
    "    \n",
    "    x = input_layer\n",
    "    for layer in hidden_layers:\n",
    "        #hidden layer\n",
    "        hidden = tf.keras.layers.Dense(layer,activation=hidden_layer_activation)(x)\n",
    "        dropout = tf.keras.layers.Dropout(rate=dropout_rate)(hidden)\n",
    "        x = dropout\n",
    "    \n",
    "    #classification\n",
    "    classification = tf.keras.layers.Dense(output_layer_size,activation= output_layer_activation)(x)\n",
    "    \n",
    "    #model\n",
    "    model = tf.keras.models.Model(inputs = [input_layer], outputs = [classification])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,decay=learning_rate/epochs),\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                            metrics=['accuracy',class_recall],\n",
    "                 run_eagerly=True)\n",
    "    \n",
    "    display(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab1459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               1100      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,907\n",
      "Trainable params: 11,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 4.3500 - accuracy: 0.2428 - class_recall: 0.1930 - val_loss: 1.7971 - val_accuracy: 0.3036 - val_class_recall: 0.3265\n",
      "Epoch 2/60\n",
      "232/232 [==============================] - 11s 46ms/step - loss: 3.9395 - accuracy: 0.2903 - class_recall: 0.3229 - val_loss: 1.6915 - val_accuracy: 0.3313 - val_class_recall: 0.3800\n",
      "Epoch 3/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.6795 - accuracy: 0.3212 - class_recall: 0.3743 - val_loss: 1.6149 - val_accuracy: 0.3513 - val_class_recall: 0.3915\n",
      "Epoch 4/60\n",
      "232/232 [==============================] - 12s 50ms/step - loss: 3.5538 - accuracy: 0.3469 - class_recall: 0.3960 - val_loss: 1.5795 - val_accuracy: 0.3605 - val_class_recall: 0.3941\n",
      "Epoch 5/60\n",
      "232/232 [==============================] - 10s 43ms/step - loss: 3.4852 - accuracy: 0.3531 - class_recall: 0.4007 - val_loss: 1.5587 - val_accuracy: 0.3728 - val_class_recall: 0.4031\n",
      "Epoch 6/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.4451 - accuracy: 0.3587 - class_recall: 0.4085 - val_loss: 1.5486 - val_accuracy: 0.3751 - val_class_recall: 0.4089\n",
      "Epoch 7/60\n",
      "232/232 [==============================] - 10s 45ms/step - loss: 3.4177 - accuracy: 0.3630 - class_recall: 0.4127 - val_loss: 1.5327 - val_accuracy: 0.3836 - val_class_recall: 0.4143\n",
      "Epoch 8/60\n",
      "232/232 [==============================] - 9s 41ms/step - loss: 3.4051 - accuracy: 0.3654 - class_recall: 0.4131 - val_loss: 1.5197 - val_accuracy: 0.3958 - val_class_recall: 0.4225\n",
      "Epoch 9/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.3900 - accuracy: 0.3736 - class_recall: 0.4198 - val_loss: 1.5245 - val_accuracy: 0.3843 - val_class_recall: 0.4167\n",
      "Epoch 10/60\n",
      "232/232 [==============================] - 10s 41ms/step - loss: 3.3641 - accuracy: 0.3726 - class_recall: 0.4169 - val_loss: 1.5162 - val_accuracy: 0.3912 - val_class_recall: 0.4216\n",
      "Epoch 11/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.3462 - accuracy: 0.3787 - class_recall: 0.4258 - val_loss: 1.5099 - val_accuracy: 0.3974 - val_class_recall: 0.4259\n",
      "Epoch 12/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.3511 - accuracy: 0.3779 - class_recall: 0.4246 - val_loss: 1.5133 - val_accuracy: 0.3905 - val_class_recall: 0.4241\n",
      "Epoch 13/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.3375 - accuracy: 0.3726 - class_recall: 0.4244 - val_loss: 1.5093 - val_accuracy: 0.3943 - val_class_recall: 0.4261\n",
      "Epoch 14/60\n",
      "232/232 [==============================] - 10s 41ms/step - loss: 3.3277 - accuracy: 0.3750 - class_recall: 0.4299 - val_loss: 1.5069 - val_accuracy: 0.3943 - val_class_recall: 0.4285\n",
      "Epoch 15/60\n",
      "232/232 [==============================] - 11s 49ms/step - loss: 3.3174 - accuracy: 0.3768 - class_recall: 0.4297 - val_loss: 1.5069 - val_accuracy: 0.3928 - val_class_recall: 0.4261\n",
      "Epoch 16/60\n",
      "232/232 [==============================] - 13s 55ms/step - loss: 3.3083 - accuracy: 0.3789 - class_recall: 0.4246 - val_loss: 1.5025 - val_accuracy: 0.3920 - val_class_recall: 0.4244\n",
      "Epoch 17/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.2847 - accuracy: 0.3861 - class_recall: 0.4367 - val_loss: 1.5027 - val_accuracy: 0.3943 - val_class_recall: 0.4292\n",
      "Epoch 18/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.2836 - accuracy: 0.3855 - class_recall: 0.4322 - val_loss: 1.5046 - val_accuracy: 0.3897 - val_class_recall: 0.4278\n",
      "Epoch 19/60\n",
      "232/232 [==============================] - 9s 41ms/step - loss: 3.2891 - accuracy: 0.3861 - class_recall: 0.4357 - val_loss: 1.4972 - val_accuracy: 0.3958 - val_class_recall: 0.4286\n",
      "Epoch 20/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.2844 - accuracy: 0.3874 - class_recall: 0.4342 - val_loss: 1.4995 - val_accuracy: 0.3943 - val_class_recall: 0.4323\n",
      "Epoch 21/60\n",
      "232/232 [==============================] - 10s 43ms/step - loss: 3.2855 - accuracy: 0.3852 - class_recall: 0.4324 - val_loss: 1.4972 - val_accuracy: 0.3943 - val_class_recall: 0.4314\n",
      "Epoch 22/60\n",
      "232/232 [==============================] - 10s 44ms/step - loss: 3.2666 - accuracy: 0.3870 - class_recall: 0.4385 - val_loss: 1.4949 - val_accuracy: 0.3966 - val_class_recall: 0.4333\n",
      "Epoch 23/60\n",
      "232/232 [==============================] - 9s 41ms/step - loss: 3.2673 - accuracy: 0.3896 - class_recall: 0.4428 - val_loss: 1.4901 - val_accuracy: 0.3966 - val_class_recall: 0.4316\n",
      "Epoch 24/60\n",
      "232/232 [==============================] - 9s 41ms/step - loss: 3.2569 - accuracy: 0.3970 - class_recall: 0.4432 - val_loss: 1.4920 - val_accuracy: 0.3958 - val_class_recall: 0.4342\n",
      "Epoch 25/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.2567 - accuracy: 0.3911 - class_recall: 0.4397 - val_loss: 1.4926 - val_accuracy: 0.3958 - val_class_recall: 0.4341\n",
      "Epoch 26/60\n",
      "232/232 [==============================] - 10s 41ms/step - loss: 3.2596 - accuracy: 0.3869 - class_recall: 0.4361 - val_loss: 1.4923 - val_accuracy: 0.3958 - val_class_recall: 0.4366\n",
      "Epoch 27/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.2508 - accuracy: 0.3924 - class_recall: 0.4407 - val_loss: 1.4911 - val_accuracy: 0.3928 - val_class_recall: 0.4329\n",
      "Epoch 28/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.2383 - accuracy: 0.3895 - class_recall: 0.4400 - val_loss: 1.4875 - val_accuracy: 0.3997 - val_class_recall: 0.4368\n",
      "Epoch 29/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.2403 - accuracy: 0.3926 - class_recall: 0.4442 - val_loss: 1.4888 - val_accuracy: 0.3958 - val_class_recall: 0.4353\n",
      "Epoch 30/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.2427 - accuracy: 0.3979 - class_recall: 0.4460 - val_loss: 1.4899 - val_accuracy: 0.3943 - val_class_recall: 0.4357\n",
      "Epoch 31/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.2383 - accuracy: 0.3928 - class_recall: 0.4452 - val_loss: 1.4876 - val_accuracy: 0.3989 - val_class_recall: 0.4395\n",
      "Epoch 32/60\n",
      "232/232 [==============================] - 10s 41ms/step - loss: 3.2455 - accuracy: 0.3955 - class_recall: 0.4435 - val_loss: 1.4885 - val_accuracy: 0.3958 - val_class_recall: 0.4349\n",
      "Epoch 33/60\n",
      "232/232 [==============================] - 10s 42ms/step - loss: 3.2359 - accuracy: 0.3928 - class_recall: 0.4497 - val_loss: 1.4841 - val_accuracy: 0.4020 - val_class_recall: 0.4377\n",
      "Epoch 34/60\n",
      "232/232 [==============================] - 9s 40ms/step - loss: 3.2237 - accuracy: 0.3975 - class_recall: 0.4444 - val_loss: 1.4879 - val_accuracy: 0.3966 - val_class_recall: 0.4359\n",
      "Epoch 34: early stopping\n"
     ]
    }
   ],
   "source": [
    "def audio_ff():\n",
    "    epochs = 60\n",
    "    model = create_ff(shape=(train_audio.shape[1],),epochs = epochs)\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('genre_classification_audio_ff.h5',monitor = 'val_class_recall',\n",
    "                                                save_best_only = True,mode='max')\n",
    "    model.fit(np.array(train_audio),np.array(train_labels),\n",
    "             validation_data=(np.array(val_audio),np.array(val_labels)),\n",
    "             epochs = epochs,\n",
    "             batch_size=64,\n",
    "             class_weight = weights,\n",
    "             shuffle=True,\n",
    "             callbacks = [stoppage,checkpoint])\n",
    "audio_ff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9462123",
   "metadata": {},
   "source": [
    "# 7. Standardized Term Density Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c570f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train['Cleaner Lyrics'])\n",
    "pickle.dump(vectorizer,open('vectorizer.pkl','wb'))\n",
    "\n",
    "train_lyrics = pd.DataFrame(vectorizer.transform(train['Cleaner Lyrics']).todense(),columns = vectorizer.get_feature_names())\n",
    "train_lyrics = (train_lyrics/np.array(train_lyrics).sum(axis = 1).repeat(len(vectorizer.get_feature_names())).reshape(train_lyrics.shape)).astype('float32')\n",
    "\n",
    "val_lyrics = pd.DataFrame(vectorizer.transform(val['Cleaner Lyrics']).todense(),columns = vectorizer.get_feature_names())\n",
    "val_lyrics = (val_lyrics/np.array(val_lyrics).sum(axis = 1).repeat(len(vectorizer.get_feature_names())).reshape(val_lyrics.shape)).astype('float32')\n",
    "\n",
    "test_lyrics = pd.DataFrame(vectorizer.transform(test['Cleaner Lyrics']).todense(),columns = vectorizer.get_feature_names())\n",
    "test_lyrics = (test_lyrics/np.array(test_lyrics).sum(axis = 1).repeat(len(vectorizer.get_feature_names())).reshape(test_lyrics.shape)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d4752e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lyrics.fillna(0,inplace=True)\n",
    "val_lyrics.fillna(0,inplace=True)\n",
    "test_lyrics.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62bd5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Term Density Features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_lyrics)\n",
    "pickle.dump(scaler,open('lyric_term_density_scaler.pkl','wb'))\n",
    "\n",
    "train_lyrics = scaler.transform(train_lyrics)\n",
    "val_lyrics = scaler.transform(val_lyrics)\n",
    "test_lyrics = scaler.transform(test_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a367e",
   "metadata": {},
   "source": [
    "# 8. Term Density Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0e38daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 83372)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               8337300   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,348,107\n",
      "Trainable params: 8,348,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "232/232 [==============================] - 19s 81ms/step - loss: 13.0249 - accuracy: 0.2039 - class_recall: 0.1999 - val_loss: 3.2988 - val_accuracy: 0.3574 - val_class_recall: 0.3616\n",
      "Epoch 2/60\n",
      "232/232 [==============================] - 18s 78ms/step - loss: 8.0462 - accuracy: 0.4003 - class_recall: 0.4149 - val_loss: 2.5421 - val_accuracy: 0.4827 - val_class_recall: 0.4923\n",
      "Epoch 3/60\n",
      "232/232 [==============================] - 18s 78ms/step - loss: 6.0503 - accuracy: 0.5342 - class_recall: 0.5568 - val_loss: 2.2305 - val_accuracy: 0.5465 - val_class_recall: 0.5587\n",
      "Epoch 4/60\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 4.4585 - accuracy: 0.6265 - class_recall: 0.6538 - val_loss: 2.1431 - val_accuracy: 0.5934 - val_class_recall: 0.6012\n",
      "Epoch 5/60\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 3.3778 - accuracy: 0.6998 - class_recall: 0.7264 - val_loss: 2.1986 - val_accuracy: 0.6065 - val_class_recall: 0.6329\n",
      "Epoch 6/60\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 2.7529 - accuracy: 0.7516 - class_recall: 0.7728 - val_loss: 2.1914 - val_accuracy: 0.6111 - val_class_recall: 0.6295\n",
      "Epoch 7/60\n",
      "232/232 [==============================] - 19s 82ms/step - loss: 2.3198 - accuracy: 0.7813 - class_recall: 0.8025 - val_loss: 2.2307 - val_accuracy: 0.6195 - val_class_recall: 0.6438\n",
      "Epoch 8/60\n",
      "232/232 [==============================] - 19s 80ms/step - loss: 1.9327 - accuracy: 0.8007 - class_recall: 0.8234 - val_loss: 2.2992 - val_accuracy: 0.6234 - val_class_recall: 0.6453\n",
      "Epoch 9/60\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 1.9597 - accuracy: 0.8194 - class_recall: 0.8384 - val_loss: 2.3597 - val_accuracy: 0.6226 - val_class_recall: 0.6352\n",
      "Epoch 10/60\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 1.4743 - accuracy: 0.8332 - class_recall: 0.8539 - val_loss: 2.4342 - val_accuracy: 0.6226 - val_class_recall: 0.6337\n",
      "Epoch 11/60\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 1.6225 - accuracy: 0.8402 - class_recall: 0.8567 - val_loss: 2.4805 - val_accuracy: 0.6218 - val_class_recall: 0.6321\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "def term_density_ff():\n",
    "    epochs = 60\n",
    "    model = create_ff(shape=(train_lyrics.shape[1]),dropout_rate=0.3,hidden_layers=[100,100],epochs = epochs)\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('genre_classification_td_ff.h5',monitor = 'val_class_recall',\n",
    "                                                    save_best_only = True,mode='max')\n",
    "    model.fit(train_lyrics,np.array(train_labels),\n",
    "             validation_data=(val_lyrics,np.array(val_labels)),\n",
    "             epochs = epochs,\n",
    "             batch_size=64,\n",
    "             class_weight = weights,\n",
    "             shuffle=True,\n",
    "             callbacks=[stoppage,checkpoint])\n",
    "    \n",
    "term_density_ff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8efef",
   "metadata": {},
   "source": [
    "# 9. Term Density + Audio Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2b9d801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 83382)]           0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               8338300   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,349,107\n",
      "Trainable params: 8,349,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "232/232 [==============================] - 20s 86ms/step - loss: 12.9866 - accuracy: 0.2287 - class_recall: 0.2389 - val_loss: 3.1627 - val_accuracy: 0.3613 - val_class_recall: 0.4051\n",
      "Epoch 2/30\n",
      "232/232 [==============================] - 19s 80ms/step - loss: 9.0145 - accuracy: 0.3660 - class_recall: 0.4147 - val_loss: 2.6493 - val_accuracy: 0.4443 - val_class_recall: 0.5017\n",
      "Epoch 3/30\n",
      "232/232 [==============================] - 19s 80ms/step - loss: 6.6568 - accuracy: 0.4988 - class_recall: 0.5556 - val_loss: 2.1947 - val_accuracy: 0.5396 - val_class_recall: 0.5754\n",
      "Epoch 4/30\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 4.9479 - accuracy: 0.5960 - class_recall: 0.6439 - val_loss: 2.3152 - val_accuracy: 0.5688 - val_class_recall: 0.6001\n",
      "Epoch 5/30\n",
      "232/232 [==============================] - 19s 81ms/step - loss: 3.8954 - accuracy: 0.6628 - class_recall: 0.7069 - val_loss: 2.2585 - val_accuracy: 0.5988 - val_class_recall: 0.6146\n",
      "Epoch 6/30\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 2.9901 - accuracy: 0.7215 - class_recall: 0.7574 - val_loss: 2.2584 - val_accuracy: 0.6188 - val_class_recall: 0.6333\n",
      "Epoch 7/30\n",
      "232/232 [==============================] - 19s 80ms/step - loss: 2.7698 - accuracy: 0.7600 - class_recall: 0.7886 - val_loss: 2.2698 - val_accuracy: 0.6280 - val_class_recall: 0.6434\n",
      "Epoch 8/30\n",
      "232/232 [==============================] - 19s 80ms/step - loss: 2.2454 - accuracy: 0.7893 - class_recall: 0.8117 - val_loss: 2.2343 - val_accuracy: 0.6303 - val_class_recall: 0.6376\n",
      "Epoch 9/30\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 1.9782 - accuracy: 0.8097 - class_recall: 0.8346 - val_loss: 2.2778 - val_accuracy: 0.6303 - val_class_recall: 0.6439\n",
      "Epoch 10/30\n",
      "232/232 [==============================] - 19s 81ms/step - loss: 1.5972 - accuracy: 0.8201 - class_recall: 0.8426 - val_loss: 2.3390 - val_accuracy: 0.6357 - val_class_recall: 0.6464\n",
      "Epoch 11/30\n",
      "232/232 [==============================] - 18s 80ms/step - loss: 1.4287 - accuracy: 0.8304 - class_recall: 0.8519 - val_loss: 2.3856 - val_accuracy: 0.6318 - val_class_recall: 0.6455\n",
      "Epoch 12/30\n",
      "232/232 [==============================] - 18s 79ms/step - loss: 1.4395 - accuracy: 0.8425 - class_recall: 0.8675 - val_loss: 2.4212 - val_accuracy: 0.6326 - val_class_recall: 0.6400\n",
      "Epoch 13/30\n",
      "232/232 [==============================] - 19s 81ms/step - loss: 1.3074 - accuracy: 0.8445 - class_recall: 0.8666 - val_loss: 2.4442 - val_accuracy: 0.6280 - val_class_recall: 0.6414\n",
      "Epoch 13: early stopping\n"
     ]
    }
   ],
   "source": [
    "def td_audio_ff():\n",
    "    epochs = 30\n",
    "    model = create_ff(shape=(train_lyrics.shape[1] + train_audio.shape[1]),epochs = epochs)\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('genre_classification_td_audio_ff.h5',monitor = 'val_class_recall',\n",
    "                                                    save_best_only = True,mode='max')\n",
    "    model.fit(np.hstack((train_lyrics,train_audio)),np.array(train_labels),\n",
    "             validation_data=(np.hstack((val_lyrics,val_audio)),np.array(val_labels)),\n",
    "             epochs = epochs,\n",
    "             batch_size=64,\n",
    "             class_weight = weights,\n",
    "             shuffle=True,\n",
    "             callbacks=[stoppage,checkpoint])\n",
    "\n",
    "td_audio_ff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767172b",
   "metadata": {},
   "source": [
    "# 10. Term Density + Audio + DAN Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f7fa341",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = pickle.load(open('../Topic_Density_Classification/WANs/word2vec_embeddings.pkl','rb'))\n",
    "token_map = pickle.load(open('../Topic_Density_Classification/word2vec_embedding_token_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7aaff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(text_data,mapping,max_size):\n",
    "    return_data = []\n",
    "    for text in text_data:\n",
    "        new_text = text.lower()\n",
    "        new_text = text.replace('\\n',' ')\n",
    "        new_text = text.replace('  ',' ')\n",
    "        new_text = new_text.split()\n",
    "        mapped_text = []\n",
    "        for token in new_text:\n",
    "            try:\n",
    "                mapped_text.append(mapping[token])\n",
    "            except:\n",
    "                mapped_text.append(len(mapping))\n",
    "\n",
    "        if len(mapped_text) > max_size:\n",
    "            mapped_text = mapped_text[:max_size]\n",
    "        else:\n",
    "            while len(mapped_text) < max_size:\n",
    "                mapped_text.append(len(mapping))\n",
    "\n",
    "        return_data.append(mapped_text)\n",
    "\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55379f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = text_to_index(train['Cleaner Lyrics'],token_map,1000)\n",
    "val_tokens = text_to_index(val['Cleaner Lyrics'],token_map,1000)\n",
    "test_tokens = text_to_index(test['Cleaner Lyrics'],token_map,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4d1a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_dan(hidden_layer1_size = [300,300],hidden_layer1_activation = 'relu', layer1_input_shape = (10,),\n",
    "                          embedding_matrix = embedding_matrix, max_length = 1000, retrain_embeddings = True,\n",
    "                          hidden_layers = [100,100],hidden_layer_activation = 'relu',dropout_rate = 0.3,\n",
    "                          output_layer_size = 7, output_layer_activation = 'softmax',learning_rate = 0.001,\n",
    "                          epochs = 10):\n",
    "\n",
    "    #### TERM DENSITY AND AUDIO LAYERS ####\n",
    "    input_size = layer1_input_shape[0] + max_length\n",
    "    input_shape = (input_size,)\n",
    "    \n",
    "    input_layer = tf.keras.layers.Input(shape = input_shape)\n",
    "    x = tf.keras.layers.Lambda(lambda x:x[:,:layer1_input_shape[0]])(input_layer)\n",
    "    count = 1\n",
    "    for num in hidden_layer1_size:\n",
    "        l1_hidden = tf.keras.layers.Dense(num,activation=hidden_layer1_activation, name = 'td_audio_hidden_' + str(count))(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate, name = 'td_audio_dropout_' + str(count))(l1_hidden)\n",
    "        count = count + 1\n",
    "    td_audio_output = x\n",
    "    \n",
    "    #### DAN LAYERS ####\n",
    "    embedding_input = tf.keras.layers.Lambda(lambda x:x[:,-max_length:])(input_layer)\n",
    "    embedding_layer = tf.keras.layers.Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                  name = 'embedding_layer')(embedding_input)\n",
    "    avg_layer = tf.keras.layers.Lambda(lambda x:K.mean(x,axis=1),name='averaging_layer')(embedding_layer)\n",
    "    \n",
    "    final_layer_input = tf.keras.layers.Concatenate(name='concat_layer')([td_audio_output,avg_layer])\n",
    "    x = final_layer_input\n",
    "    count = 1\n",
    "    for num in hidden_layers:\n",
    "        hidden = tf.keras.layers.Dense(num,activation=hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate,name='dropout_'+str(count))(hidden)\n",
    "        count = count + 1\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(output_layer_size,output_layer_activation,name='classification')(x)\n",
    "    \n",
    "    classification_model = tf.keras.models.Model(inputs = [input_layer],outputs = [classification])\n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,decay=learning_rate/epochs),\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                            metrics=['accuracy',class_recall],\n",
    "                 run_eagerly=True)\n",
    "    \n",
    "    display(classification_model.summary())\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "03f4b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 84382)]      0           []                               \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)              (None, 83382)        0           ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_hidden_1 (Dense)      (None, 300)          25014900    ['lambda_6[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_dropout_1 (Dropout)   (None, 300)          0           ['td_audio_hidden_1[0][0]']      \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 1000)         0           ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_hidden_2 (Dense)      (None, 300)          90300       ['td_audio_dropout_1[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 1000, 300)    13194600    ['lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_dropout_2 (Dropout)   (None, 300)          0           ['td_audio_hidden_2[0][0]']      \n",
      "                                                                                                  \n",
      " averaging_layer (Lambda)       (None, 300)          0           ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, 600)          0           ['td_audio_dropout_2[0][0]',     \n",
      "                                                                  'averaging_layer[0][0]']        \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 100)          60100       ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100)          0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 100)          10100       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " classification (Dense)         (None, 7)            707         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,370,707\n",
      "Trainable params: 38,370,707\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "232/232 [==============================] - 86s 366ms/step - loss: 5.5583 - accuracy: 0.1924 - class_recall: 0.1934 - val_loss: 1.8952 - val_accuracy: 0.2798 - val_class_recall: 0.3333\n",
      "Epoch 2/30\n",
      "232/232 [==============================] - 83s 356ms/step - loss: 4.6437 - accuracy: 0.2691 - class_recall: 0.2832 - val_loss: 1.7821 - val_accuracy: 0.3467 - val_class_recall: 0.4204\n",
      "Epoch 3/30\n",
      "232/232 [==============================] - 83s 358ms/step - loss: 4.0381 - accuracy: 0.3523 - class_recall: 0.4004 - val_loss: 1.6000 - val_accuracy: 0.4358 - val_class_recall: 0.4939\n",
      "Epoch 4/30\n",
      "232/232 [==============================] - 82s 351ms/step - loss: 3.4645 - accuracy: 0.4366 - class_recall: 0.4934 - val_loss: 1.3945 - val_accuracy: 0.5165 - val_class_recall: 0.5502\n",
      "Epoch 5/30\n",
      "232/232 [==============================] - 82s 354ms/step - loss: 2.8163 - accuracy: 0.5378 - class_recall: 0.5907 - val_loss: 1.2430 - val_accuracy: 0.5826 - val_class_recall: 0.5971\n",
      "Epoch 6/30\n",
      "232/232 [==============================] - 84s 361ms/step - loss: 2.3204 - accuracy: 0.6354 - class_recall: 0.6789 - val_loss: 1.1946 - val_accuracy: 0.6149 - val_class_recall: 0.6254\n",
      "Epoch 7/30\n",
      "232/232 [==============================] - 82s 354ms/step - loss: 1.9100 - accuracy: 0.7116 - class_recall: 0.7445 - val_loss: 1.1805 - val_accuracy: 0.6372 - val_class_recall: 0.6418\n",
      "Epoch 8/30\n",
      "232/232 [==============================] - 82s 354ms/step - loss: 1.5953 - accuracy: 0.7603 - class_recall: 0.7872 - val_loss: 1.2193 - val_accuracy: 0.6457 - val_class_recall: 0.6518\n",
      "Epoch 9/30\n",
      "232/232 [==============================] - 82s 354ms/step - loss: 1.3767 - accuracy: 0.7976 - class_recall: 0.8206 - val_loss: 1.2808 - val_accuracy: 0.6441 - val_class_recall: 0.6413\n",
      "Epoch 10/30\n",
      "232/232 [==============================] - 82s 353ms/step - loss: 1.2171 - accuracy: 0.8194 - class_recall: 0.8397 - val_loss: 1.3463 - val_accuracy: 0.6457 - val_class_recall: 0.6506\n",
      "Epoch 11/30\n",
      "232/232 [==============================] - 82s 354ms/step - loss: 1.1560 - accuracy: 0.8306 - class_recall: 0.8502 - val_loss: 1.3727 - val_accuracy: 0.6426 - val_class_recall: 0.6479\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "def td_audio_dan300():\n",
    "    epochs = 30\n",
    "    model = create_multimodal_dan(embedding_matrix=embedding_matrix,retrain_embeddings=True,\n",
    "                                  layer1_input_shape=(train_lyrics.shape[1] + train_audio.shape[1],),epochs=epochs)\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('genre_classification_td_audio_dan300.h5',monitor = 'val_class_recall',\n",
    "                                                save_best_only = True,mode='max')\n",
    "    model.fit(np.hstack((train_lyrics,train_audio,train_tokens)),np.array(train_labels),\n",
    "             validation_data=(np.hstack((val_lyrics,val_audio,val_tokens)),np.array(val_labels)),\n",
    "             epochs = epochs,\n",
    "             batch_size=64,\n",
    "             class_weight = weights,\n",
    "             shuffle=True,\n",
    "             callbacks = [stoppage,checkpoint])\n",
    "td_audio_dan300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e215f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 84382)]      0           []                               \n",
      "                                                                                                  \n",
      " lambda_8 (Lambda)              (None, 83382)        0           ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_hidden_1 (Dense)      (None, 100)          8338300     ['lambda_8[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_dropout_1 (Dropout)   (None, 100)          0           ['td_audio_hidden_1[0][0]']      \n",
      "                                                                                                  \n",
      " lambda_9 (Lambda)              (None, 1000)         0           ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_hidden_2 (Dense)      (None, 100)          10100       ['td_audio_dropout_1[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 1000, 300)    13194600    ['lambda_9[0][0]']               \n",
      "                                                                                                  \n",
      " td_audio_dropout_2 (Dropout)   (None, 100)          0           ['td_audio_hidden_2[0][0]']      \n",
      "                                                                                                  \n",
      " averaging_layer (Lambda)       (None, 300)          0           ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, 400)          0           ['td_audio_dropout_2[0][0]',     \n",
      "                                                                  'averaging_layer[0][0]']        \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 100)          40100       ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100)          0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 100)          10100       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " classification (Dense)         (None, 7)            707         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,593,907\n",
      "Trainable params: 21,593,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "232/232 [==============================] - 77s 330ms/step - loss: 5.8761 - accuracy: 0.1528 - class_recall: 0.1777 - val_loss: 2.1497 - val_accuracy: 0.1929 - val_class_recall: 0.2487\n",
      "Epoch 2/30\n",
      "232/232 [==============================] - 77s 330ms/step - loss: 5.5337 - accuracy: 0.1967 - class_recall: 0.2364 - val_loss: 1.9889 - val_accuracy: 0.2636 - val_class_recall: 0.3459\n",
      "Epoch 3/30\n",
      "232/232 [==============================] - 74s 319ms/step - loss: 5.0714 - accuracy: 0.2339 - class_recall: 0.2729 - val_loss: 1.8860 - val_accuracy: 0.3344 - val_class_recall: 0.3906\n",
      "Epoch 4/30\n",
      "232/232 [==============================] - 74s 318ms/step - loss: 4.5518 - accuracy: 0.2915 - class_recall: 0.3386 - val_loss: 1.7474 - val_accuracy: 0.3720 - val_class_recall: 0.4451\n",
      "Epoch 5/30\n",
      "232/232 [==============================] - 74s 319ms/step - loss: 4.2006 - accuracy: 0.3400 - class_recall: 0.3851 - val_loss: 1.6361 - val_accuracy: 0.4189 - val_class_recall: 0.4875\n",
      "Epoch 6/30\n",
      "232/232 [==============================] - 74s 318ms/step - loss: 3.8033 - accuracy: 0.3990 - class_recall: 0.4545 - val_loss: 1.4936 - val_accuracy: 0.4635 - val_class_recall: 0.5387\n",
      "Epoch 7/30\n",
      "232/232 [==============================] - 74s 318ms/step - loss: 3.3479 - accuracy: 0.4530 - class_recall: 0.5112 - val_loss: 1.3692 - val_accuracy: 0.5150 - val_class_recall: 0.5622\n",
      "Epoch 8/30\n",
      "232/232 [==============================] - 74s 319ms/step - loss: 2.9790 - accuracy: 0.5125 - class_recall: 0.5694 - val_loss: 1.3161 - val_accuracy: 0.5342 - val_class_recall: 0.5718\n",
      "Epoch 9/30\n",
      "232/232 [==============================] - 75s 323ms/step - loss: 2.6602 - accuracy: 0.5652 - class_recall: 0.6076 - val_loss: 1.2543 - val_accuracy: 0.5772 - val_class_recall: 0.5998\n",
      "Epoch 10/30\n",
      "232/232 [==============================] - 74s 319ms/step - loss: 2.3852 - accuracy: 0.6136 - class_recall: 0.6526 - val_loss: 1.2014 - val_accuracy: 0.6042 - val_class_recall: 0.6264\n",
      "Epoch 11/30\n",
      "232/232 [==============================] - 74s 318ms/step - loss: 2.1997 - accuracy: 0.6448 - class_recall: 0.6730 - val_loss: 1.1837 - val_accuracy: 0.6026 - val_class_recall: 0.6246\n",
      "Epoch 12/30\n",
      "232/232 [==============================] - 74s 317ms/step - loss: 2.0627 - accuracy: 0.6722 - class_recall: 0.6974 - val_loss: 1.1810 - val_accuracy: 0.6057 - val_class_recall: 0.6233\n",
      "Epoch 13/30\n",
      "232/232 [==============================] - 74s 318ms/step - loss: 1.8381 - accuracy: 0.6996 - class_recall: 0.7247 - val_loss: 1.1927 - val_accuracy: 0.6272 - val_class_recall: 0.6314\n",
      "Epoch 14/30\n",
      "232/232 [==============================] - 78s 338ms/step - loss: 1.7150 - accuracy: 0.7211 - class_recall: 0.7402 - val_loss: 1.2358 - val_accuracy: 0.6211 - val_class_recall: 0.6345\n",
      "Epoch 15/30\n",
      "232/232 [==============================] - 78s 334ms/step - loss: 1.6357 - accuracy: 0.7313 - class_recall: 0.7513 - val_loss: 1.2509 - val_accuracy: 0.6287 - val_class_recall: 0.6408\n",
      "Epoch 16/30\n",
      "232/232 [==============================] - 72s 309ms/step - loss: 1.5653 - accuracy: 0.7566 - class_recall: 0.7760 - val_loss: 1.2615 - val_accuracy: 0.6295 - val_class_recall: 0.6447\n",
      "Epoch 17/30\n",
      "232/232 [==============================] - 72s 312ms/step - loss: 1.3909 - accuracy: 0.7678 - class_recall: 0.7883 - val_loss: 1.2795 - val_accuracy: 0.6264 - val_class_recall: 0.6370\n",
      "Epoch 18/30\n",
      "232/232 [==============================] - 73s 316ms/step - loss: 1.3950 - accuracy: 0.7771 - class_recall: 0.7999 - val_loss: 1.2878 - val_accuracy: 0.6372 - val_class_recall: 0.6536\n",
      "Epoch 19/30\n",
      "232/232 [==============================] - 74s 320ms/step - loss: 1.3490 - accuracy: 0.7911 - class_recall: 0.8090 - val_loss: 1.3267 - val_accuracy: 0.6226 - val_class_recall: 0.6328\n",
      "Epoch 20/30\n",
      "232/232 [==============================] - 74s 317ms/step - loss: 1.2528 - accuracy: 0.8025 - class_recall: 0.8219 - val_loss: 1.3548 - val_accuracy: 0.6249 - val_class_recall: 0.6309\n",
      "Epoch 21/30\n",
      "232/232 [==============================] - 74s 318ms/step - loss: 1.2072 - accuracy: 0.8128 - class_recall: 0.8300 - val_loss: 1.3819 - val_accuracy: 0.6272 - val_class_recall: 0.6395\n",
      "Epoch 21: early stopping\n"
     ]
    }
   ],
   "source": [
    "def td_audio_dan100():\n",
    "    epochs = 30\n",
    "    model = create_multimodal_dan(embedding_matrix=embedding_matrix,retrain_embeddings=True,hidden_layer1_size=[100,100],\n",
    "                                  layer1_input_shape=(train_lyrics.shape[1] + train_audio.shape[1],),epochs=epochs)\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('genre_classification_td_audio_dan100.h5',monitor = 'val_class_recall',\n",
    "                                                save_best_only = True,mode='max')\n",
    "    model.fit(np.hstack((train_lyrics,train_audio,train_tokens)),np.array(train_labels),\n",
    "             validation_data=(np.hstack((val_lyrics,val_audio,val_tokens)),np.array(val_labels)),\n",
    "             epochs = epochs,\n",
    "             batch_size=64,\n",
    "             class_weight = weights,\n",
    "             shuffle=True,\n",
    "             callbacks = [stoppage,checkpoint])\n",
    "td_audio_dan100()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d127f",
   "metadata": {},
   "source": [
    "# 11. Term Density + Audio + WAN Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e5545b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_wan(hidden_layer1_size = [300,300],hidden_layer1_activation = 'relu', layer1_input_shape = (10,),\n",
    "                          embedding_matrix = embedding_matrix, max_length = 1000, retrain_embeddings = True, num_attention = 10,\n",
    "                          hidden_layers = [100,100],hidden_layer_activation = 'relu',dropout_rate = 0.3,\n",
    "                          output_layer_size = 7, output_layer_activation = 'softmax',learning_rate = 0.001,\n",
    "                          epochs = 10):\n",
    "\n",
    "    #### TERM DENSITY AND AUDIO LAYERS ####\n",
    "    input_size = layer1_input_shape[0] + max_length\n",
    "    input_shape = (input_size,)\n",
    "    \n",
    "    input_layer = tf.keras.layers.Input(shape = input_shape)\n",
    "    x = tf.keras.layers.Lambda(lambda x:x[:,:layer1_input_shape[0]])(input_layer)\n",
    "    count = 1\n",
    "    for num in hidden_layer1_size:\n",
    "        l1_hidden = tf.keras.layers.Dense(num,activation=hidden_layer1_activation, name = 'td_audio_hidden_' + str(count))(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate, name = 'td_audio_dropout_' + str(count))(l1_hidden)\n",
    "        count = count + 1\n",
    "    td_audio_output = x\n",
    "    \n",
    "    #### DAN LAYERS ####\n",
    "    embedding_input = tf.keras.layers.Lambda(lambda x:x[:,-max_length:])(input_layer)\n",
    "    embedding_layer = tf.keras.layers.Embedding(embedding_matrix.shape[0],\n",
    "                                  embedding_matrix.shape[1],\n",
    "                                  weights = [embedding_matrix],\n",
    "                                  input_length=max_length,\n",
    "                                  trainable=retrain_embeddings,\n",
    "                                  name = 'embedding_layer')(embedding_input)\n",
    "    \n",
    "    \n",
    "    wan_embeddings = embedding_layer\n",
    "    \n",
    "    if num_attention > 0: #If attention is applied to embeddings to learn how to weight into single representation\n",
    "        if num_attention > 1:\n",
    "            #Create attention based single vector representations of words according to alternative query vectors\n",
    "            attention_embeddings = []\n",
    "            for num in range(num_attention):\n",
    "                #Apply Query Vector to words in embeddings, returning a max_sequence_length x 1 tensor\n",
    "                l1_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query' + str(num+1))(wan_embeddings)\n",
    "                #reshape to 1 x max_sequence_length\n",
    "                l1_reshape_query = tf.keras.layers.Reshape((1,max_length))(l1_query)\n",
    "                #Softmax over query * key (words) to obtain weights\n",
    "                l1_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                                    name='attention_weights' + str(num+1))(l1_reshape_query)\n",
    "                #weight embeddings according to weights\n",
    "                l1_attention = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((wan_embeddings,l1_weights)))\n",
    "                attention_embeddings.append(l1_attention)\n",
    "\n",
    "            concat_attention = tf.keras.layers.Concatenate()(attention_embeddings)\n",
    "            concat_attention = tf.keras.layers.Reshape((num_attention,embedding_matrix.shape[1]))(concat_attention)\n",
    "        else:\n",
    "            concat_attention = wan_embeddings\n",
    "            num_attention = max_length\n",
    "    \n",
    "        #Apply Query Vector to attention based representations, returning a num_attention x 1 tensor\n",
    "        wan_query = tf.keras.layers.Dense(1,activation='linear',use_bias=False,name='attention_query')(concat_attention)\n",
    "        #reshape to 1 x num_attention\n",
    "        reshaped_query = tf.keras.layers.Reshape((1,num_attention))(wan_query)\n",
    "        #Softmax over query * key (words) to obtain weights\n",
    "        wan_weights = tf.keras.layers.Lambda(lambda x:tf.keras.activations.softmax(x),\n",
    "                                            name='attention_weights')(reshaped_query)\n",
    "        #weight attention embeddings according to weights, learning how to balance attention based vector representations \n",
    "        #from prior layer\n",
    "        embedding = tf.keras.layers.Flatten()(tf.keras.layers.Dot((1,2))((concat_attention,wan_weights)))\n",
    "    else: #Default to DAN Treatment of Embeddings if num_attention = 0\n",
    "        embedding = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1), name='averaging')(wan_embeddings)\n",
    "    \n",
    "    final_layer_input = tf.keras.layers.Concatenate(name='concat_layer')([td_audio_output,embedding])\n",
    "    x = final_layer_input\n",
    "    count = 1\n",
    "    for num in hidden_layers:\n",
    "        hidden = tf.keras.layers.Dense(num,activation=hidden_layer_activation,name='hidden_' + str(count))(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate,name='dropout_'+str(count))(hidden)\n",
    "        count = count + 1\n",
    "    \n",
    "    classification = tf.keras.layers.Dense(output_layer_size,output_layer_activation,name='classification')(x)\n",
    "    \n",
    "    classification_model = tf.keras.models.Model(inputs = [input_layer],outputs = [classification])\n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,decay=learning_rate/epochs),\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                            metrics=['accuracy',class_recall],\n",
    "                 run_eagerly=True)\n",
    "    \n",
    "    display(classification_model.summary())\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e20d8c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 84382)]      0           []                               \n",
      "                                                                                                  \n",
      " lambda_13 (Lambda)             (None, 1000)         0           ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 1000, 300)    13194600    ['lambda_13[0][0]']              \n",
      "                                                                                                  \n",
      " attention_query1 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query2 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query3 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query4 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query5 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query6 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query7 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query8 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query9 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query10 (Dense)      (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1000)      0           ['attention_query1[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1000)      0           ['attention_query2[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 1000)      0           ['attention_query3[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 1000)      0           ['attention_query4[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 1000)      0           ['attention_query5[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 1000)      0           ['attention_query6[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 1000)      0           ['attention_query7[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)            (None, 1, 1000)      0           ['attention_query8[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_8 (Reshape)            (None, 1, 1000)      0           ['attention_query9[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 1, 1000)      0           ['attention_query10[0][0]']      \n",
      "                                                                                                  \n",
      " attention_weights1 (Lambda)    (None, 1, 1000)      0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " attention_weights2 (Lambda)    (None, 1, 1000)      0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights3 (Lambda)    (None, 1, 1000)      0           ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights4 (Lambda)    (None, 1, 1000)      0           ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights5 (Lambda)    (None, 1, 1000)      0           ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights6 (Lambda)    (None, 1, 1000)      0           ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights7 (Lambda)    (None, 1, 1000)      0           ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights8 (Lambda)    (None, 1, 1000)      0           ['reshape_7[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights9 (Lambda)    (None, 1, 1000)      0           ['reshape_8[0][0]']              \n",
      "                                                                                                  \n",
      " attention_weights10 (Lambda)   (None, 1, 1000)      0           ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights1[0][0]']     \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights2[0][0]']     \n",
      "                                                                                                  \n",
      " dot_2 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights3[0][0]']     \n",
      "                                                                                                  \n",
      " dot_3 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights4[0][0]']     \n",
      "                                                                                                  \n",
      " dot_4 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights5[0][0]']     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dot_5 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights6[0][0]']     \n",
      "                                                                                                  \n",
      " dot_6 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights7[0][0]']     \n",
      "                                                                                                  \n",
      " dot_7 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights8[0][0]']     \n",
      "                                                                                                  \n",
      " dot_8 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights9[0][0]']     \n",
      "                                                                                                  \n",
      " dot_9 (Dot)                    (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights10[0][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 300)          0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 300)          0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 300)          0           ['dot_2[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 300)          0           ['dot_3[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 300)          0           ['dot_4[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 300)          0           ['dot_5[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 300)          0           ['dot_6[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 300)          0           ['dot_7[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 300)          0           ['dot_8[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 300)          0           ['dot_9[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3000)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_5[0][0]',              \n",
      "                                                                  'flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]',              \n",
      "                                                                  'flatten_8[0][0]',              \n",
      "                                                                  'flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_10 (Reshape)           (None, 10, 300)      0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_12 (Lambda)             (None, 83382)        0           ['input_23[0][0]']               \n",
      "                                                                                                  \n",
      " attention_query (Dense)        (None, 10, 1)        300         ['reshape_10[0][0]']             \n",
      "                                                                                                  \n",
      " td_audio_hidden_1 (Dense)      (None, 300)          25014900    ['lambda_12[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_11 (Reshape)           (None, 1, 10)        0           ['attention_query[0][0]']        \n",
      "                                                                                                  \n",
      " td_audio_dropout_1 (Dropout)   (None, 300)          0           ['td_audio_hidden_1[0][0]']      \n",
      "                                                                                                  \n",
      " attention_weights (Lambda)     (None, 1, 10)        0           ['reshape_11[0][0]']             \n",
      "                                                                                                  \n",
      " td_audio_hidden_2 (Dense)      (None, 300)          90300       ['td_audio_dropout_1[0][0]']     \n",
      "                                                                                                  \n",
      " dot_10 (Dot)                   (None, 300, 1)       0           ['reshape_10[0][0]',             \n",
      "                                                                  'attention_weights[0][0]']      \n",
      "                                                                                                  \n",
      " td_audio_dropout_2 (Dropout)   (None, 300)          0           ['td_audio_hidden_2[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 300)          0           ['dot_10[0][0]']                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, 600)          0           ['td_audio_dropout_2[0][0]',     \n",
      "                                                                  'flatten_10[0][0]']             \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 100)          60100       ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100)          0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 100)          10100       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " classification (Dense)         (None, 7)            707         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Total params: 38,374,007\n",
      "Trainable params: 38,374,007\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "232/232 [==============================] - 355s 2s/step - loss: 5.9779 - accuracy: 0.1530 - class_recall: 0.1881 - val_loss: 1.9038 - val_accuracy: 0.1929 - val_class_recall: 0.2654\n",
      "Epoch 2/30\n",
      "232/232 [==============================] - 378s 2s/step - loss: 4.7779 - accuracy: 0.2243 - class_recall: 0.2668 - val_loss: 1.7758 - val_accuracy: 0.3251 - val_class_recall: 0.3928\n",
      "Epoch 3/30\n",
      "232/232 [==============================] - 357s 2s/step - loss: 4.2091 - accuracy: 0.3009 - class_recall: 0.3554 - val_loss: 1.6433 - val_accuracy: 0.3636 - val_class_recall: 0.4594\n",
      "Epoch 4/30\n",
      "232/232 [==============================] - 356s 2s/step - loss: 3.5827 - accuracy: 0.3802 - class_recall: 0.4591 - val_loss: 1.4847 - val_accuracy: 0.4466 - val_class_recall: 0.5473\n",
      "Epoch 5/30\n",
      "232/232 [==============================] - 357s 2s/step - loss: 2.9808 - accuracy: 0.4743 - class_recall: 0.5619 - val_loss: 1.3457 - val_accuracy: 0.5127 - val_class_recall: 0.5930\n",
      "Epoch 6/30\n",
      "232/232 [==============================] - 351s 2s/step - loss: 2.5092 - accuracy: 0.5609 - class_recall: 0.6377 - val_loss: 1.2647 - val_accuracy: 0.5565 - val_class_recall: 0.6112\n",
      "Epoch 7/30\n",
      "232/232 [==============================] - 344s 1s/step - loss: 2.0931 - accuracy: 0.6456 - class_recall: 0.7080 - val_loss: 1.2539 - val_accuracy: 0.5911 - val_class_recall: 0.6270\n",
      "Epoch 8/30\n",
      "232/232 [==============================] - 347s 1s/step - loss: 1.7562 - accuracy: 0.7034 - class_recall: 0.7497 - val_loss: 1.2597 - val_accuracy: 0.6164 - val_class_recall: 0.6459\n",
      "Epoch 9/30\n",
      "232/232 [==============================] - 351s 2s/step - loss: 1.4964 - accuracy: 0.7526 - class_recall: 0.7912 - val_loss: 1.2565 - val_accuracy: 0.6272 - val_class_recall: 0.6473\n",
      "Epoch 10/30\n",
      "232/232 [==============================] - 351s 2s/step - loss: 1.3679 - accuracy: 0.7833 - class_recall: 0.8157 - val_loss: 1.2835 - val_accuracy: 0.6364 - val_class_recall: 0.6541\n",
      "Epoch 11/30\n",
      "232/232 [==============================] - 346s 1s/step - loss: 1.2380 - accuracy: 0.8078 - class_recall: 0.8323 - val_loss: 1.3111 - val_accuracy: 0.6487 - val_class_recall: 0.6586\n",
      "Epoch 12/30\n",
      "232/232 [==============================] - 348s 1s/step - loss: 1.1353 - accuracy: 0.8261 - class_recall: 0.8484 - val_loss: 1.3663 - val_accuracy: 0.6418 - val_class_recall: 0.6590\n",
      "Epoch 13/30\n",
      "232/232 [==============================] - 350s 2s/step - loss: 1.0597 - accuracy: 0.8354 - class_recall: 0.8557 - val_loss: 1.3604 - val_accuracy: 0.6387 - val_class_recall: 0.6527\n",
      "Epoch 14/30\n",
      "232/232 [==============================] - 350s 2s/step - loss: 0.9766 - accuracy: 0.8485 - class_recall: 0.8691 - val_loss: 1.3877 - val_accuracy: 0.6418 - val_class_recall: 0.6579\n",
      "Epoch 15/30\n",
      "232/232 [==============================] - 348s 2s/step - loss: 0.9270 - accuracy: 0.8537 - class_recall: 0.8741 - val_loss: 1.4466 - val_accuracy: 0.6410 - val_class_recall: 0.6515\n",
      "Epoch 15: early stopping\n"
     ]
    }
   ],
   "source": [
    "def td_audio_wan300():\n",
    "    epochs = 30\n",
    "    model = create_multimodal_wan(embedding_matrix=embedding_matrix,retrain_embeddings=True,hidden_layer1_size=[300,300],\n",
    "                                  layer1_input_shape=(train_lyrics.shape[1] + train_audio.shape[1],),epochs=epochs,num_attention=10)\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('genre_classification_td_audio_wan300.h5',monitor = 'val_class_recall',\n",
    "                                                save_best_only = True,mode='max')\n",
    "    model.fit(np.hstack((train_lyrics,train_audio,train_tokens)),np.array(train_labels),\n",
    "             validation_data=(np.hstack((val_lyrics,val_audio,val_tokens)),np.array(val_labels)),\n",
    "             epochs = epochs,\n",
    "             batch_size=64,\n",
    "             class_weight = weights,\n",
    "             shuffle=True,\n",
    "             callbacks = [stoppage,checkpoint])\n",
    "td_audio_wan300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d71a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_24 (InputLayer)          [(None, 84382)]      0           []                               \n",
      "                                                                                                  \n",
      " lambda_15 (Lambda)             (None, 1000)         0           ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_layer (Embedding)    (None, 1000, 300)    13194600    ['lambda_15[0][0]']              \n",
      "                                                                                                  \n",
      " attention_query1 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query2 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query3 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query4 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query5 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query6 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query7 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query8 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query9 (Dense)       (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " attention_query10 (Dense)      (None, 1000, 1)      300         ['embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " reshape_12 (Reshape)           (None, 1, 1000)      0           ['attention_query1[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_13 (Reshape)           (None, 1, 1000)      0           ['attention_query2[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_14 (Reshape)           (None, 1, 1000)      0           ['attention_query3[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_15 (Reshape)           (None, 1, 1000)      0           ['attention_query4[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_16 (Reshape)           (None, 1, 1000)      0           ['attention_query5[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_17 (Reshape)           (None, 1, 1000)      0           ['attention_query6[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_18 (Reshape)           (None, 1, 1000)      0           ['attention_query7[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_19 (Reshape)           (None, 1, 1000)      0           ['attention_query8[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_20 (Reshape)           (None, 1, 1000)      0           ['attention_query9[0][0]']       \n",
      "                                                                                                  \n",
      " reshape_21 (Reshape)           (None, 1, 1000)      0           ['attention_query10[0][0]']      \n",
      "                                                                                                  \n",
      " attention_weights1 (Lambda)    (None, 1, 1000)      0           ['reshape_12[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights2 (Lambda)    (None, 1, 1000)      0           ['reshape_13[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights3 (Lambda)    (None, 1, 1000)      0           ['reshape_14[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights4 (Lambda)    (None, 1, 1000)      0           ['reshape_15[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights5 (Lambda)    (None, 1, 1000)      0           ['reshape_16[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights6 (Lambda)    (None, 1, 1000)      0           ['reshape_17[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights7 (Lambda)    (None, 1, 1000)      0           ['reshape_18[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights8 (Lambda)    (None, 1, 1000)      0           ['reshape_19[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights9 (Lambda)    (None, 1, 1000)      0           ['reshape_20[0][0]']             \n",
      "                                                                                                  \n",
      " attention_weights10 (Lambda)   (None, 1, 1000)      0           ['reshape_21[0][0]']             \n",
      "                                                                                                  \n",
      " dot_11 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights1[0][0]']     \n",
      "                                                                                                  \n",
      " dot_12 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights2[0][0]']     \n",
      "                                                                                                  \n",
      " dot_13 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights3[0][0]']     \n",
      "                                                                                                  \n",
      " dot_14 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights4[0][0]']     \n",
      "                                                                                                  \n",
      " dot_15 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights5[0][0]']     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dot_16 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights6[0][0]']     \n",
      "                                                                                                  \n",
      " dot_17 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights7[0][0]']     \n",
      "                                                                                                  \n",
      " dot_18 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights8[0][0]']     \n",
      "                                                                                                  \n",
      " dot_19 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights9[0][0]']     \n",
      "                                                                                                  \n",
      " dot_20 (Dot)                   (None, 300, 1)       0           ['embedding_layer[0][0]',        \n",
      "                                                                  'attention_weights10[0][0]']    \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 300)          0           ['dot_11[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)           (None, 300)          0           ['dot_12[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 300)          0           ['dot_13[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)           (None, 300)          0           ['dot_14[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_15 (Flatten)           (None, 300)          0           ['dot_15[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_16 (Flatten)           (None, 300)          0           ['dot_16[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_17 (Flatten)           (None, 300)          0           ['dot_17[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_18 (Flatten)           (None, 300)          0           ['dot_18[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_19 (Flatten)           (None, 300)          0           ['dot_19[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_20 (Flatten)           (None, 300)          0           ['dot_20[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 3000)         0           ['flatten_11[0][0]',             \n",
      "                                                                  'flatten_12[0][0]',             \n",
      "                                                                  'flatten_13[0][0]',             \n",
      "                                                                  'flatten_14[0][0]',             \n",
      "                                                                  'flatten_15[0][0]',             \n",
      "                                                                  'flatten_16[0][0]',             \n",
      "                                                                  'flatten_17[0][0]',             \n",
      "                                                                  'flatten_18[0][0]',             \n",
      "                                                                  'flatten_19[0][0]',             \n",
      "                                                                  'flatten_20[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_22 (Reshape)           (None, 10, 300)      0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_14 (Lambda)             (None, 83382)        0           ['input_24[0][0]']               \n",
      "                                                                                                  \n",
      " attention_query (Dense)        (None, 10, 1)        300         ['reshape_22[0][0]']             \n",
      "                                                                                                  \n",
      " td_audio_hidden_1 (Dense)      (None, 100)          8338300     ['lambda_14[0][0]']              \n",
      "                                                                                                  \n",
      " reshape_23 (Reshape)           (None, 1, 10)        0           ['attention_query[0][0]']        \n",
      "                                                                                                  \n",
      " td_audio_dropout_1 (Dropout)   (None, 100)          0           ['td_audio_hidden_1[0][0]']      \n",
      "                                                                                                  \n",
      " attention_weights (Lambda)     (None, 1, 10)        0           ['reshape_23[0][0]']             \n",
      "                                                                                                  \n",
      " td_audio_hidden_2 (Dense)      (None, 100)          10100       ['td_audio_dropout_1[0][0]']     \n",
      "                                                                                                  \n",
      " dot_21 (Dot)                   (None, 300, 1)       0           ['reshape_22[0][0]',             \n",
      "                                                                  'attention_weights[0][0]']      \n",
      "                                                                                                  \n",
      " td_audio_dropout_2 (Dropout)   (None, 100)          0           ['td_audio_hidden_2[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_21 (Flatten)           (None, 300)          0           ['dot_21[0][0]']                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, 400)          0           ['td_audio_dropout_2[0][0]',     \n",
      "                                                                  'flatten_21[0][0]']             \n",
      "                                                                                                  \n",
      " hidden_1 (Dense)               (None, 100)          40100       ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100)          0           ['hidden_1[0][0]']               \n",
      "                                                                                                  \n",
      " hidden_2 (Dense)               (None, 100)          10100       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100)          0           ['hidden_2[0][0]']               \n",
      "                                                                                                  \n",
      " classification (Dense)         (None, 7)            707         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Total params: 21,597,207\n",
      "Trainable params: 21,597,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "232/232 [==============================] - 349s 1s/step - loss: 5.2257 - accuracy: 0.1922 - class_recall: 0.1919 - val_loss: 1.9549 - val_accuracy: 0.2175 - val_class_recall: 0.2502\n",
      "Epoch 2/30\n",
      "232/232 [==============================] - 344s 1s/step - loss: 4.9153 - accuracy: 0.2254 - class_recall: 0.2432 - val_loss: 1.8287 - val_accuracy: 0.3205 - val_class_recall: 0.3268\n",
      "Epoch 3/30\n",
      "232/232 [==============================] - 349s 2s/step - loss: 4.6627 - accuracy: 0.2762 - class_recall: 0.2911 - val_loss: 1.7262 - val_accuracy: 0.4020 - val_class_recall: 0.4116\n",
      "Epoch 4/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 4.3393 - accuracy: 0.3181 - class_recall: 0.3530 - val_loss: 1.6312 - val_accuracy: 0.4181 - val_class_recall: 0.4669\n",
      "Epoch 5/30\n",
      "232/232 [==============================] - 344s 1s/step - loss: 3.9107 - accuracy: 0.3662 - class_recall: 0.4094 - val_loss: 1.5238 - val_accuracy: 0.4520 - val_class_recall: 0.4948\n",
      "Epoch 6/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 3.4690 - accuracy: 0.4321 - class_recall: 0.4776 - val_loss: 1.4164 - val_accuracy: 0.4689 - val_class_recall: 0.5290\n",
      "Epoch 7/30\n",
      "232/232 [==============================] - 342s 1s/step - loss: 3.2276 - accuracy: 0.4793 - class_recall: 0.5181 - val_loss: 1.3278 - val_accuracy: 0.5188 - val_class_recall: 0.5567\n",
      "Epoch 8/30\n",
      "232/232 [==============================] - 341s 1s/step - loss: 2.8070 - accuracy: 0.5364 - class_recall: 0.5769 - val_loss: 1.2568 - val_accuracy: 0.5719 - val_class_recall: 0.5833\n",
      "Epoch 9/30\n",
      "232/232 [==============================] - 342s 1s/step - loss: 2.5519 - accuracy: 0.5985 - class_recall: 0.6365 - val_loss: 1.2375 - val_accuracy: 0.6003 - val_class_recall: 0.6041\n",
      "Epoch 10/30\n",
      "232/232 [==============================] - 342s 1s/step - loss: 2.2806 - accuracy: 0.6428 - class_recall: 0.6799 - val_loss: 1.2181 - val_accuracy: 0.6103 - val_class_recall: 0.6136\n",
      "Epoch 11/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 2.0275 - accuracy: 0.6800 - class_recall: 0.7093 - val_loss: 1.2092 - val_accuracy: 0.6249 - val_class_recall: 0.6273\n",
      "Epoch 12/30\n",
      "232/232 [==============================] - 342s 1s/step - loss: 1.8587 - accuracy: 0.7134 - class_recall: 0.7433 - val_loss: 1.2210 - val_accuracy: 0.6234 - val_class_recall: 0.6171\n",
      "Epoch 13/30\n",
      "232/232 [==============================] - 342s 1s/step - loss: 1.6949 - accuracy: 0.7445 - class_recall: 0.7683 - val_loss: 1.2280 - val_accuracy: 0.6295 - val_class_recall: 0.6335\n",
      "Epoch 14/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 1.5298 - accuracy: 0.7647 - class_recall: 0.7858 - val_loss: 1.2858 - val_accuracy: 0.6287 - val_class_recall: 0.6310\n",
      "Epoch 15/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 1.4570 - accuracy: 0.7825 - class_recall: 0.8032 - val_loss: 1.2977 - val_accuracy: 0.6403 - val_class_recall: 0.6371\n",
      "Epoch 16/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 1.3985 - accuracy: 0.7933 - class_recall: 0.8106 - val_loss: 1.3263 - val_accuracy: 0.6341 - val_class_recall: 0.6309\n",
      "Epoch 17/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 1.3147 - accuracy: 0.8077 - class_recall: 0.8286 - val_loss: 1.3479 - val_accuracy: 0.6318 - val_class_recall: 0.6252\n",
      "Epoch 18/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 1.2145 - accuracy: 0.8167 - class_recall: 0.8339 - val_loss: 1.4025 - val_accuracy: 0.6357 - val_class_recall: 0.6406\n",
      "Epoch 19/30\n",
      "232/232 [==============================] - 342s 1s/step - loss: 1.1827 - accuracy: 0.8264 - class_recall: 0.8485 - val_loss: 1.4081 - val_accuracy: 0.6318 - val_class_recall: 0.6287\n",
      "Epoch 20/30\n",
      "232/232 [==============================] - 342s 1s/step - loss: 1.0593 - accuracy: 0.8326 - class_recall: 0.8508 - val_loss: 1.4534 - val_accuracy: 0.6318 - val_class_recall: 0.6285\n",
      "Epoch 21/30\n",
      "232/232 [==============================] - 343s 1s/step - loss: 1.0670 - accuracy: 0.8378 - class_recall: 0.8569 - val_loss: 1.4885 - val_accuracy: 0.6357 - val_class_recall: 0.6399\n",
      "Epoch 21: early stopping\n"
     ]
    }
   ],
   "source": [
    "def td_audio_wan100():\n",
    "    epochs = 30\n",
    "    model = create_multimodal_wan(embedding_matrix=embedding_matrix,retrain_embeddings=True,hidden_layer1_size=[100,100],\n",
    "                                  layer1_input_shape=(train_lyrics.shape[1] + train_audio.shape[1],),epochs=epochs,num_attention=10)\n",
    "    stoppage = keras.callbacks.EarlyStopping(monitor = 'val_class_recall',verbose=1,patience=3,mode='max')\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint('genre_classification_td_audio_wan100.h5',monitor = 'val_class_recall',\n",
    "                                                save_best_only = True,mode='max')\n",
    "    model.fit(np.hstack((train_lyrics,train_audio,train_tokens)),np.array(train_labels),\n",
    "             validation_data=(np.hstack((val_lyrics,val_audio,val_tokens)),np.array(val_labels)),\n",
    "             epochs = epochs,\n",
    "             batch_size=64,\n",
    "             class_weight = weights,\n",
    "             shuffle=True,\n",
    "             callbacks = [stoppage,checkpoint])\n",
    "td_audio_wan100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302881f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
